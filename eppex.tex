% In this chapter:
% - introduction of eppex
% -- design goals
% -- distinction from legacy phrase-extract tools
% - implementation details
% -- Murmur hash, boost pools, std c++11 hash tables, indexed storages, gzipped I/O

\chapter{Eppex}
\label{chap:eppex}

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{Your program can always have a twice as much time to run, but not always have a~twice as much memory.}{--- RNDr. Rudolf Kryl, "Programming II" course}

\Eppex{} is phrase pairs extraction and scoring tool capable of obtaining approximate
frequency counts of extracted phrase pairs by using Lossy Counting algorithm
(thus the name \eppex{}, an acronym for \emph{epochal phrase pairs extraction}).
It is designed to be an alternative to standard phrase extraction and scoring tools that
are shipped with Moses, implementing most of the functionality of steps 5 and 6 of
\emph{train-model.perl} script.
\Eppex{} input and output interface is fully compatible with the replaced tools
and \eppex{} in fact is intended to be invoked from within the Moses training script itself
by adding a few specific parameters.

\Eppex{} differs from its core counterparts in one important aspect: during its runtime,
only physical memory is utilized, no temporary files are stored on disk as with \emph{extract}
and \emph{score} tools.
The goal is to make \eppex{} a faster alternative, aiming at environments with plenty of RAM.
Benchmarking of time/memory trade-offs was the fundamental part of this work and the results are
thoroughly examined in \Cref{chap:results}.

\Eppex{} memory demands may be limited by setting more restrictive support and error thresholds
for Lossy Counting, but aggressive pruning may lead to loss of phrase table quality --
experimentally evaluated trade-offs are also discussed in \Cref{chap:results}.

This chapter is fully devoted to technical aspects of implementation and it expects the reader
to be familiar with the basics of programming and the C++ programming language, including
a basic knowledge of Standard Template Library (STL).
Instructions on how to install \eppex{} on Linux-based operating system are given in
\Aref{chap:installation}, usage instructions are given in \Aref{chap:usage}.

\section{Design goals and philosophy}

% What and why is a target platform?
\Eppex{} is implemented as a command-line program and, as indicated above, it is written in C++.
From the development perspective, \eppex{} adheres to the philosophy of Moses, what is mainly
reflected by the selection of development platform and third party libraries:
\begin{enumerate}
  \item The primary development platform is Linux --
  \eppex{} has been developed and tested on desktop version of Ubuntu 12.04 LTS with GCC~4.6.3 and
  server version of Ubuntu 10.04 LTS with GCC~4.4.3 installed.\footnote{GCC stands for the GNU Compiler Collection: \url{http://gcc.gnu.org/}}
  \item \Eppex{} relies on Boost C++ library\footurl{http://www.boost.org/} -- it is used to provide
  some essential functionality, so \eppex{} cannot be compiled without access to some of Boost
  headers and packages.\footnote{See \Aref{chap:installation} for details.}
\end{enumerate}
Nevertheless, a considerable effort has been taken to make the implementation as cross-platform
compatible as possible.

% 64-bit or 32-bit?
\Eppex{} is intended to be run on 64-bit machines, but this is rather a matter of fact than
a requirement: to process a parallel corpus of reasonable size, \eppex{} will in typical
setting require much more memory than 32-bit environments can provide\footnote{In 32-bit
environment the virtual address space holds no more than $2^{32}$ addresses, so at maximum
4,294,967,296 bytes (4~GB) of memory are utilizable. Workarounds exist, but they might be
considered unfeasible nowadays, as 64-bit architecture is well established.} (see
\Sref{sec:eppex-memory-demands} for some experimentally established figures).
Therefore, although not tested, \eppex{} should run as smoothly on 32-bit machine as it runs
on 64-bit, but the amount of input data that it will manage to process will have to be rather
small for practical MT or aggressive pruning will have to be undertaken.

\section{Effective C++}

% Why C++?
C++ is a programming language that offers a wide range of optimization techniques to
tune up both the execution speed and memory requirements of a program.
Both aspects are important to us, but memory usage optimization is our primary concern,
since the execution speed, when compared to legacy tools, is implicitly boosted by
the Lossy Counting algorithm and the fact that we keep all temporary data in the computer
memory instead of disk.
Besides, as pointed out in epigraph on the beginning of this chapter, memory, unlike time,
is a limited resource.
In this section we present several universal rules and techniques for effective C++
programming, that we followed and applied during implementation of \emph{eppex}.

\subsection{Data alignment and structure packing}

The issue behind \emph{data alignment} is best explained using example
-- let us define a dummy structure \texttt{LossyCounterItem}:

\begin{verbatim}
struct LossyCounterItem {
  unsigned char maximum_error; // sizeof(unsigned char) == 1
  unsigned int frequency; // sizeof(unsigned int) == 4
  unsigned short item_id; // sizeof(unsigned short) == 2
};
\end{verbatim}

Given the definition above and with respect to the byte sizes of individual members,
one could expect that \texttt{sizeof(LossyCounterItem)} equals to 7, but this guess
will be wrong in most of the cases. The most likely result is 12.

This unexpected behavior stems from the fact that many machines require that
objects of a certain type are aligned on architecture-dependent boundaries,
for example integers are often allocated on word\footnote{\emph{Word} in computer
architecture terminology denotes a fixed-sized group of bits that are handled
as a unit by the instruction  set and/or hardware of the processor.} boundaries
\citep[Chapter 5]{stroustrup:cplusplus}.

The easiest way how to minimize effects of data alignment is to order members
by their size with the largest members first.
This way our \texttt{LossyCounterItem} structure can be shrunk to 8 bytes,
but still there will be one byte of blank padding at the end of the structure.
With some compilers this padding can be trimmed by setting on \emph{structure packing}
using \texttt{\#pragma} preprocessor directive, but doing this will slow data access
on most processors that read memory by words, because misaligned structures will span
multiple words and in consequence require more reads.

This being said, the best solution in terms of code portability and effectiveness
is to design data structures in such a way that all their members properly align:
this ensures their effective handling by processing unit without spurious memory padding.

\subsection{C-string vs. std::string}
\label{sec:strings}
% Why had we used C-string instead of std::string.

Unlike Java, C and C++ have no fundamental data type for strings, instead they have
to be stored as an array of \texttt{chars}.
Such string handling is obviously too low-level for most modern applications,
which are mainly about string processing, so STL creators came to help and designed
several \emph{string} classes that mimic behavior of ordinary data types
\citep[Chapter 11]{josuttis:stl}:
STL strings may be copied, assigned and compared like any other fundamental type,
without the programmer being worried about the internal memory allocations and deallocations.
Because both approaches are very often used side by side, there exists well-established
terminology to distinguish between them:
the arrays of characters are called \emph{C-like-strings} (or shortly \emph{C-strings}) and
STL string classes are just \emph{strings} (sometimes labeled \emph{std::string} after the
most commonly used class).

From the memory management standpoint it turns out, that any std::string
implementation will be more memory demanding than plain C-string.
This is expectable: the STL string class has to provide more flexible interface
and does all the internal memory management by itself.
Depending on particular implementation, it might store the size of the string,
store the capacity of allocated memory, do the reference counting and more
\citep[Item 15]{meyers:effectivestl}.
The reference counting feature seems to be useful for our phrase-counting problem,
but it would imply to rely on proprietary implementation that is not enforced by
standard, which is a bad design choice for any open-source software.

Therefore, whenever the memory footprint of strings storage is more important than
the convenience and flexibility of their handling, C-string is a better choice
than any STL string implementation.

\subsection{Objects unification}
\label{sec:objects-unification}

When processing large stream of items that have the property of being distinguishable
either as different or equal to each other, it is often efficient to keep only a single
copy of each \emph{unique} item in computer memory and use some type of lightweight
reference to this copy in all the program parts that has to access item data.
For example, when counting $n$-grams in a text corpus, instead of keeping $n$ strings
within each $n$-gram object, it might be more efficient to just keep $n$ references to
unique, separately stored copies of such strings.\footnote{An observant reader may realize
that the task of items counting itself implicitly requires objects unification. In fact,
the example was deliberately picked to demonstrate that there are two aspects to objects
unification: it is a procedure inherent to some tasks, such as items counting, but also
a technique to achieve some memory savings in general.}
Obviously, this technique of \emph{objects unification} is not applicable universally and,
in general, its efficiency depends on the following factors:
\begin{enumerate}
  \item The input redundancy of items -- the greater the redundancy, the greater
  the possible memory savings. If there are no redundant items, this technique is useless,
  because it is cheaper to just store the item without an additional reference to it.
  \item The processing redundancy of items -- the more the items have to be
  referenced within the program, the greater the possible memory savings.
  \item The ratio between average in-memory size of item and reference to it -- the greater
  the ratio, the more memory is saved by each reference.
  Clearly, this ratio has to be greater than one.
\end{enumerate}

Typically, a programmer cannot affect the first factor, but should carefully consider it.
For example, in case of words pulled from natural language corpus it is generally hard to
presume whether their unification in program memory will be efficient:
Zipf's law \citep[Chapter 1]{manning:stat-nlp} suggests that the efficiency of words unification
will largely depend on the overhead imposed by the referencing scheme and if the references are
too heavy, the memory spared thanks to the most frequent words can be easily depleted by
the massive amount of words occurring only once.

The second factor might be tricky to consider in programs with non-linear work flow,
for example when the amount of items within program scope depends on some external
factor (time of day etc.), but in case of streamline input processing this value is
most often linear with respect to the input size.

The third factor is the only one that is completely in hands of the programmer.
With C++ we have two basic options, how to reference an object:
\begin{enumerate}
  \item by using object's address within memory
  \item by using a numeric index that uniquely identifies an object within a large storage
  (either a random access container or an array).
\end{enumerate}

We may note that both options are fundamentally the same:
the pointer is basically a numeric index into the container of a maximum size (entire memory)
and the numeric index is just a pointer into restrained part of memory (container).
The pointer is easier to dereference, but comes in single size only -- the size of
\emph{processor unit word}, which on 64-bit architecture is 8~bytes long.
On the other hand, to dereference the numeric index, a corresponding container must be
accessed to determine the address of index start.
However, a numeric index data type can be adjusted to the size of domain of unified items,
if it is known in advance or can be assessed by some upper bound.
Being able to reference 4~billions of unique objects is enough in many situations and
in such cases only 4~bytes long integer type will suffice, reducing referencing overhead
to half of that of the pointer.

As a final note, we shall point out that objects unification comes with a cost of
additional processing overhead: especially insertion of new object is more time-consuming,
as every new object have to be first checked for an existing copy.

\subsection{Simple segregated storage}
\label{sec:simple-segregated-storage}

Almost every non-trivial computer program uses dynamic memory allocation.
C++ provides two interfaces to ask for a new memory during program runtime:
one can either use legacy C API function \emph{malloc} or pure C++ API operator \emph{new}.
Both APIs have to provide means to satisfy any type of memory request -- and for any size.
Hence, usually the size of an allocated memory block (and any other bookkeeping information) is
kept in the memory chunk right before the block itself \citep[Chapter 10]{meyers:effective-cpp}.
This way, for deallocation either via \emph{free} function or the operator \emph{delete},
the memory allocator only needs the pointer to the memory block that should be deallocated.
It can easily determine, how much memory should be actually freed, by inspecting the
bookkeeping information.

The obvious overhead in memory consumption incurred by this approach is usually negligible,
but may become significant in situations, when a load of small-sized memory blocks is allocated.
To avoid the overhead in such situations, a frequently used approach is to allocate a big pool of
fixed-size memory blocks once a time and serve these blocks sequentially as new memory requests
come in, allocating a new pool whenever all memory blocks in existing pools are exhausted.
This technique is generally called \emph{memory pooling}, but when details of
memory pool allocation and deallocation matters, usually more specific names are used to
distinguish between various implementations --  a \emph{simple segregated storage} from Boost
library is an example of one such specific
implementation.\footurl{http://www.boost.org/doc/libs/1_54_0/libs/pool/doc/html/boost_pool/pool/pooling.html}

% Unions - in current version of Eppex they don't play significant role.

\subsection{Unordered sets}

The recent C++ standard,\footnote{ISO/IEC 14882:2011} marked as C++11, standardizes
a new type of containers, that were already part of almost every implementation of STL,
but due non-existing standardization they differ slightly in their interfaces across
various implementations.
The \emph{unordered associative containers} are well known to almost every programmer,
although usually they are referred to under a different name: \emph{hash tables}.

The C++11 standard defines four types of hash table containers that correspond to existing
binary-search-tree-based containers (with an \emph{unordered_} prefix):
\emph{unordered_set}, \emph{unordered_multiset}, \emph{unordered_map} and \emph{unordered_multimap}.
The key differences between hash tables and binary-search trees are:
\begin{itemize}
  \item Binary-search trees require the "less-than" comparison operator to be defined for stored items,
  whereas hash tables require the "equal-to" comparison operator and also a hash function to be
  defined for them.
  \item When iterating over a binary-search tree, the items are returned in the order provided by
  the "less-than" comparison operator, whereas the iteration over hash table returns the items in arbitrary
  order.
  \item Lookup, insertion and deletion operations with hash tables have (amortized) constant
  average cost per operation (one call to the hash function plus one or more equality comparisons),
  whereas with binary-search tree these operations have a cost dependent on the number of items
  in the tree ($N$): each operation requires $O\big(log_2(N)\big)$ comparisons.
\end{itemize}

In the real life applications hash tables often perform much better than binary-search
trees, although their worst case complexity is linear with respect to number of items
(the C++ standard requires that hash tables are implemented using
\emph{closed addressing}\footurl{http://en.wikipedia.org/wiki/Hash_tables\#Separate_chaining}).
The effectiveness of hash tables stands and falls with the hash function:
a good hash function has to be fast to compute and possess the quality of distributing
the data as evenly as possible over its output range.

\section{Implementation analysis and description}

During the program runtime, the input data are processed in order to establish frequency counts
necessary to compute phrase pair scores and print the complete phrase translation table.
This requires the phrase pairs to be extracted and kept in memory along with the estimated
frequency and maximum error for Lossy Counting -- essentially, a set of triples $(e, f, \Delta)$,
as described in \Cref{chap:lossy-counting}, is to be maintained.
These data account for most of the memory required during runtime, therefore their effective
representation in memory is mission-critical and was object of the major programming effort.

\subsection{Size of integer types}

Both frequency count and maximum error are plain unsigned integers, so at first it might seem
there is nothing that can be possibly done about their optimization.
However, it will be unwise to just declare them both as \texttt{unsigned int}.
First, the size of integer data types is implementation dependent.
Despite that \texttt{int} is typically 4 bytes long, this is not guaranteed \citep[Chapter 4.6]{stroustrup:cplusplus}.
Second, it is important to realize that by processing real life data we may never need as much
space as some integer types provide and saving even few bytes per item can lead to gigabytes
saved in total.

To ensure cross-platform compatibility, we base the definitions of estimated frequency count and
maximum error (and other crucial numeric types) on definitions of fixed-size integer types from
Boost.Integer library.\footurl{http://www.boost.org/doc/libs/1_53_0/libs/integer/doc/html/index.html}
For frequency counts we dedicate 4~bytes, a space capable of storing all unsigned values up to
$2^{32} \approx 4.3 \times 10^9$. This is definitely enough for our task as there is no
parallel corpus yet with billions of parallel sentences, therefore it is safe to expect no phrase
pair will occur billion times.
For maximum error values it is sufficient to dedicate 1~byte only (unsigned values up to 255).
A maximum error of any item during Lossy Counting is always less than the ID of current epoch and
from our initial experiments, we had learned that lossy counted phrase extraction with more than just
a few epochs leads to pruning that already hurts too much.\footnote{As results in \Cref{chap:results}
demonstrate, with more data it is possible to prune more (with more epochs) with no significant
loss of quality, but still the 0-255 range displays a tremendous reserve.}

\subsection{The phrase pair}

% Types of objects stored:
% - words, phrases, alignments

In contrast to counters, a phrase pair is quite a complex structure: it consist of the source phrase,
the target phrase and also the information about alignment between particular words of both phrases.
Both source and target phrases can be further decomposed into sequence of words, while alignment
information can be further decomposed into sequence of pairs of alignment points.
What makes the design of effective in-memory representation of such a structure even more complicated,
is the necessity to account for the flexible lengths of these components: they not only differ between
distinct phrase pairs, but even a particular phrase pair can have the source phrase of length $S$,
the target phrase of length $T$ and the alignment information of length $L$, such that $S \neq T \neq L$.
On the other hand, there is a natural limit to these lengths incurred by the maximum sentence length and,
in practice, even a more restrictive limits are usually enforced to secure the feasibility of the
extracted translation model.

From the perspective of effective implementation, it is useful to consider the phrase pair structure as
the composition of three different components: words, phrases and alignments.
In the following text, we are first going to analyze the properties of each of these components and
then conclude with the description of how to put them all together to represent the phrase pair.

\subsubsection*{Words}

Words, or more generally translation model input factors, are technically a string types.
In \Sref{sec:strings}, we discussed the advantage of C-strings over STL strings when it comes 
to memory requirements of both.
Later, in \Sref{sec:objects-unification} we presented a technique of lowering program memory
footprint by keeping all unique copies of some type of objects in a single place and keeping
only lightweight references to such copies in data structures that have to access them.
With both these options in mind, it is easy to come up with an idea to store all words as
C-strings in a separate storage and keep only pointers to them in each phrase pair structure.
The only problem with this idea is that a pointer type in 64-bits environments occupies
8~bytes, what is simply excessive for representation of a single word (a lot of words could
be fit in 8 bytes directly).

Instead, as already suggested in \Sref{sec:objects-unification}, it is more effective to
use numeric index as a reference.
A data type of such numeric index has to be capable of representing the whole domain of
input factor and arguably the natural language words themselves present the input factor with
the largest possible variability.
Despite that it is impossible to count the number of words in any language, a reasonable estimate
for English mentions the order of hundreds of thousands.\footurl{http://en.wikipedia.org/wiki/Number_of_words_in_English}
This said, the \texttt{boost::uint32_t} data type from Boost.Integer library, capable of representing
more than 4.2~billions of distinct values, should be enough for any reasonable input factor.

During the lossy counted extraction, the end-of-epoch pruning can remove all the phrase pairs that
reference a certain word, and in such a case the unique copy of this word could be deleted from memory.
However, we decided to not implement this functionality, because even with a million of words stored,
their memory footprint will be still orders of magnitude below the memory footprint of hundreds of
millions of the phrase pairs and, intuitively, it seems more viable to spare the processing time
overhead necessary for detection and removal of such words than try to save a rather insignificant
amount of the memory.

\subsubsection*{Phrases}

With the words represented as numeric indexes, it is straight-forward to represent phrase as
a sequence of such indexes and this is in fact the solution we sticked to.

An alternative implementation could employ objects unification, but the positive effect
of this technique is doubtful on this level, because there are far more distinct phrase forms
than there are word forms.
As mentioned in \Sref{sec:significance-pruning}, a typical phrase table contains a lot of
1-1-1 phrase pairs.\footnote{In our experiments, the amount of 1-1-1 phrase pairs account for
27\% and 40\% of the phrase tables, see \Cref{chap:results}.}
The source and target phrases in such phrase pairs are not only diminishing the efficiency of
objects unification, as they occur only once, but the phrase pair as such is a very likely
candidate for pruning.
In contrast to the pruning of words data as discussed earlier, the pruning of phrase data can
have a very significant impact on the program memory footprint: if these data were stored as
a unique copies only, either some reference counting scheme (additional memory overhead) or
a full scan over the copies and the references (additional processing overhead) would be required.

\subsubsection*{Alignments}

% Properties of phrase pair alignment.
The phrase pair alignment marks words from source and target side that were aligned to
each other in the word alignment of the sentence from which the phrase pair was extracted.
The phrase extraction algorithm ensures that each extracted phrase pair is based on
at least one alignment pair.
However, because there is no dependency between word-alignments of sentences,
a particular phrase pair can be encountered with multiple distinct alignments and only
the most frequent one should be included in the phrase table.\footnote{This is what Moses \emph{phrase-extract} toolkit does.}
Consequently, during the extraction of phrase pairs, it is necessary not only to track all
distinct alignments for each phrase pair, but also the number of times they occurred with
that particular pair.

For the purpose of in-memory representation, we decided to think of phrase pair alignments
as of words and to use the same strategy to store them (i.e. keep unique copies in a single,
separate storage referenced by a numeric indexes).

The main motivation for such decision was the finding that phrase pair alignments tend to
have a Zipfian-like frequency distribution, as \Fref{fig:alignment-dist} presents.
To obtain the plot, we extracted all (41.3~M) of phrase pairs of length up to~7 from the Czech-English
part of Europarl v6 parallel corpus and for each alignment counted the number of phrase pairs
it occurred in.
The extracted data contained more than 506~K unique alignments and the most frequent alignment
"\texttt{0-0 1-1}" belonged to more than 1.5~M phrase pairs.
The number of unique phrase pairs suggests that we should use at least 3 bytes
for the numeric index, because 2~bytes provide space for 65536 indexes only.
Interestingly, the phrase pair with the most alignments had only 9 of them.

\begin{figure}[ht]
  \input{alignments-distribution}
  \caption{
    A plot of phrase pair alignment frequency in a set of 41.3~M phrase pairs with
    506~K unique alignments extracted from the Czech-English part of Europarl v6 corpus.
    On the x axis is the rank of alignment based on the frequency, on y axis is the frequency.
  }
  \label{fig:alignment-dist}
\end{figure}

\subsubsection*{Phrase pairs}

The use of references to phrase pair alignments has one more advantage: all the main
components of a phrase pair can be stacked in a single array of 4-bytes-wide fields.
The source and target phrases are just sequences of 4-byte-wide word indexes and
we empirically estimated that 3~bytes are enough for a numeric reference in the case
of alignments.

The alignment frequency counter can also be represented with a 4~bytes unsigned integer,
because we demonstrated the same type to be capable of holding the estimated frequency
counts of phrase pairs in the Lossy Counter implementation.
In fact, a smaller data type will not suffice, as there might be a phrase pair with only
a single alignment.

Besides the components data, we also have to keep their dimensions,
to be able to tell where each component data starts and where it ends.
We already gave reasons why to safely expect these values to be rather small and,
with a bit of static type casting, we can put this information in the first field
of the array, so its layout will be as follows:
\begin{itemize}
  \item the first field contains a header-like structure with the count
    of distinct alignments $A$ (2~bytes),
    the source phrase length $S$ (1~byte) and the target phrase length $T$ (1~byte)
  \item the next $S$ fields contain word indexes of the source phrase
  \item the next $T$ fields contain word indexes of the target phrase
  \item the last $2 \times A$ fields contains a sequence of $A$ pairs with
    the alignment index and related frequency counter
\end{itemize}

We experimented with an implementation that used memory pools to allocate the memory
for the array, but it had run terribly slow, presumably because of the frequent deallocation
of the phrase pair data.\footnote{A phrase pair data are deallocated not only when a particular
phrase pair is pruned, but also when a new alignment is encountered and to accommodate it,
the data has to be moved to a bigger array.}
Quite possible, we did not employ the best memory pool implementation for such a purpose,
nevertheless, the current implementation of \eppex{} allocates the memory via the operator
\texttt{new}.

\subsection{Indexed storage}
% Indexed storage with memory pool

In the previous text, we just briefly mentioned that words and alignments are represented
by a numeric indexes that references the unique copy of a particular word or alignment,
without any further details. Let us now describe this \emph{indexed storage}.

The implementation of \emph{indexed storage} internally employs three container types to perform its function.

The actual string or sequence of alignment points is stored in a memory pool managed by
\texttt{boost::pool} class from the Boost Pool Library.\footurl{http://www.boost.org/doc/libs/1_54_0/libs/pool/doc/html/index.html}

The pointer to the stored data is kept in a \texttt{vector} container: the position of
the pointer within the container defines the numeric index used as a (word/alignment) reference
and the container itself serves as a lookup table.

To be able to determine, whether a particular string or alignment is present in the storage,
without performing a full-scan search over the lookup table, an additional index-like
structure over the numeric indexes is maintained.
This index is implemented with an \texttt{unordered_set} container and a hash function computed
over the data of an object, therefore the existence of an already stored copy of an object can be
determined in (amortized) constant time.\footnote{The default compilation
option is to use std::set instead of std::unordered_set, because of the latter recency.}

... % TODO: Indexed storage is write-only (no deletes)

\subsection{Lossy Counter}

% TODO: Finish.
The Lossy Counter algorithm is implemented by a template class ...

\subsection{Compressed I/O}

\Eppex{} can read/write directly from/to gzipped files, the same way legacy \emph{phrase-extract}
tools does.
This option allows to save a significant amount of disk space, as a typical phrase table will be
several times smaller when compressed.\footnote{In case of the phrase tables produced in our experiments
the compression ratio of the gzipped files varied between 14.0\% and 22.9\%.}
In addition, in environments when disks are under heavy load (shared computation servers are often
the case), compression may even speed up the whole I/O process.

Our implementation simply reuses the respective library shipped with Moses source code,
as it is cleanly designed and easy to include.
