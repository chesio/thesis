% In this chapter:
% - introduction of eppex
% -- design goals
% -- distinction from legacy phrase-extract tools
% - implementation details
% -- Murmur hash, boost pools, std c++11 hash tables, indexed storages, gzipped I/O

\chapter{Eppex}
\label{chap:eppex}

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{Your program can always have a twice as much time to run, but not always have a~twice as much memory.}{--- Rudolf Kryl, Introduction to Programming course}

\Eppex{} is phrase pairs extraction and scoring tool capable of obtaining approximate
frequency counts of extracted phrase pairs by using Lossy Counting algorithm
(thus the name \eppex{}, an acronym for \emph{epochal phrase pairs extraction}).
It is designed to be an alternative to standard phrase extraction and scoring tools that
are shipped with Moses, implementing most of the functionality of steps 5 and 6 of
\emph{train-model.perl} script.
\Eppex{} input and output interface is fully compatible with those of the replaced tools
and \eppex{} in fact is intended to be invoked from within the Moses training script itself
by passing specific parameters.

\Eppex{} differs from its core counterparts in one important aspect: during its runtime
only physical memory is utilized, no temporary files are stored on disk as with \emph{extract}
and \emph{score} tools.
The goal is to make \eppex{} a faster alternative, aiming at environments with plenty of RAM.
Benchmarking of time/memory trade-offs was fundamental part of this work and the results are
thoroughly examined in \Cref{chap:results}.

\Eppex{} memory demands may be limited by setting more restrictive support and error thresholds
for Lossy Counting, but aggressive pruning may lead to loss of phrase table quality --
experimentally evaluated trade-offs are also discussed in \Cref{chap:results}.

This chapter is fully devoted to technical aspects of implementation and it expects the reader
to be familiar with the basics of programming and the C++ programming language, including
a basic knowledge of Standard Template Library (STL).
Instructions on how to install \eppex{} on Linux-based operating system are given in
\Aref{chap:installation}, usage instructions are given in \Aref{chap:usage}.

\section{Design goals and philosophy}

% What and why is a target platform?
\Eppex{} is implemented as a command-line program and, as indicated above, it is written in C++.
From the development perspective, \eppex{} adheres to the philosophy of Moses, which is mainly
reflected by the selection of development platform and third party libraries:
\begin{enumerate}
  \item The primary development platform is Linux --
  \eppex{} has been developed and tested on desktop version of Ubuntu 12.04 LTS with GCC 4.6.3 and
  server version of Ubuntu 10.04 LTS with GCC 4.4.3 installed.
  \item \Eppex{} relies on Boost C++ library\footurl{http://www.boost.org/} -- it is used to provide
  some essential functionality, so \eppex{} cannot be compiled without access to some of Boost
  headers and packages.\footnote{See \Aref{chap:installation} for details.}
\end{enumerate}
Nevertheless, a considerable effort has been taken to make the implementation as cross-platform
compatible as possible.

% 64-bit or 32-bit?
\Eppex{} is intended to be run on 64-bit machines, but this is rather a matter of fact than
a requirement: to process a parallel corpus of reasonable size, \eppex{} will in typical
setting require much more memory than 32-bit environments can provide\footnote{In 32-bit
environment the virtual address space holds no more than $2^{32}$ addresses, so at maximum
4,294,967,296 bytes (4~GB) of memory are utilizable. Workarounds exist, but they might be
considered unfeasible nowadays, as 64-bit architecture is well established.} (see
\Sref{sec:eppex-memory-demands} for some experimentally established figures).
Therefore, although not tested, \eppex{} should run as smoothly on 32-bit machine as it runs
on 64-bit, but the amount of input data that it will manage to process will have to be decent
or aggressive pruning will have to be undertaken.

\section{Effective C++}

% Why C++?
C++ is a programming language that offers a wide range of optimization techniques to
tune up both the execution speed and memory requirements of a program.
Both aspects are important to us, but memory usage optimization is our primary concern,
since the execution speed, when compared to legacy tools, is implicitly boosted by
the Lossy Counting algorithm and the fact that we keep all temporary data in the computer
memory instead of disk.
Besides, as pointed out in epigraph on the beginning of this chapter, memory, unlike time,
is a limited resource.
In this section we present several universal rules and techniques for effective C++
programming, that we followed and applied during implementation of \emph{eppex}.

\subsection{Data alignment and structure packing}

The issue behind \emph{data alignment} is best explained using example
-- let us define a dummy structure \verb|LossyCounterItem|:

\begin{verbatim}
struct LossyCounterItem {
  unsigned char maximum_error; // sizeof(unsigned char) == 1
  unsigned int frequency; // sizeof(unsigned int) == 4
  unsigned short item_id; // sizeof(unsigned short) == 2
};
\end{verbatim}

Given the definition above and with respect to the byte sizes of individual members,
one could expect that \verb|sizeof(LossyCountItem)| equals to 7, but this guess
will be wrong in most of the cases as the most likely result is 12.

This unexpected behavior stems from the fact that many machines require that
objects of certain type are aligned on architecture-dependent boundaries,
for example integers are often allocated on word\footnote{\emph{Word} in computer
architecture terminology denotes a fixed-sized group of bits that are handled
as a unit by the instruction  set and/or hardware of the processor.} boundaries
\citep[Chapter 5]{stroustrup:cplusplus}.

The easiest way how to minimize effects of data alignment is to order members
by their size with the largest members first.
This way our \verb|LossyCounterItem| structure can be shrunk to 8 bytes,
but still there will be one byte of blank padding at the end of the structure.
With some compilers this padding can be trimmed by setting on \emph{structure packing}
using \verb|#pragma| preprocessor directive, but doing this will slow data access
on most processors that read memory by words, because misaligned structures will span
multiple words and in consequence require more reads.

This being said, the best solution in terms of code portability and effectiveness
is to design data structures in such a way that all their members properly align:
this ensures their effective handling by processing unit without spurious memory padding.

\subsection{C-string vs. std::string}
\label{sec:strings}
% Why had we used C-string instead of std::string.

Unlike Java, C and C++ have no fundamental data type for strings, instead they have
to be stored as an array of \verb|chars|.
Such string handling is obviously too low-level for most modern applications,
which are mainly about string processing, so STL creators came to help and designed
several \emph{string} classes that mimic behavior of ordinary data types
\citep[Chapter 11]{josuttis:stl}:
STL strings may be copied, assigned and compared like any other fundamental type,
without the programmer being worried about the internal memory allocations and deallocations.
Because both approaches are very often used side by side, there exists well-established
terminology to distinguish between them:
the arrays of characters are called \emph{C-like-strings} (or shortly \emph{C-strings}) and
STL string classes are just \emph{strings} (sometimes labeled \emph{std::string} after the
most commonly used class).

From the memory management standpoint it turns out, that any std::string
implementation will be more memory demanding than plain C-string.
This is expectable: the STL string class has to provide more flexible interface
and does all the internal memory management by itself.
Depending on particular implementation, it might store the size of the string,
store the capacity of allocated memory, do the reference counting and more
\citep[Item 15]{meyers:effectivestl}.
The reference counting feature seems to be useful for our phrase-counting problem,
but it would imply to rely on proprietary implementation that is not enforced by
standard, which is a bad design choice for any open-source software.

Therefore, whenever memory footprint of strings storage is more important than
the convenience and flexibility of their handling, C-string is a better choice
than any STL string implementation.

\subsection{Objects unification}
\label{sec:objects-unification}

When processing large stream of items that have the property of being distinguishable
either as different or equal to each other, it is often efficient to keep only a single
copy of each \emph{unique} item in computer memory and use some type of lightweight
reference to this copy in all the program parts that has to access item data.
For example, when counting $n$-grams in a text corpus, instead of keeping $n$ strings
within each $n$-gram object, it might be more efficient to just keep $n$ references to
unique, separately stored copies of such strings.\footnote{An observant reader may realize
that the task of items counting itself implicitly requires objects unification. In fact,
the example was deliberately picked to demonstrate that there are two aspects to object
unification: it is inherent in case of some tasks, such as items counting, but also more
generally applicable as a technique to achieve some memory savings.}
Obviously, this technique of \emph{objects unification} is not applicable universally and,
in general, its efficiency depends on the following factors:
\begin{enumerate}
  \item The input redundancy of items -- the greater the redundancy, the greater
  the possible memory savings. If there are no redundant items, this technique is useless,
  because it is cheaper to just store the item without an additional reference to it.
  \item The processing redundancy of items -- the more the items have to be
  referenced within the program, the greater the possible memory savings.
  \item The ratio between average in-memory size of item and reference to it -- the greater
  the ratio, the more memory is saved by each reference.
  Clearly, this ratio has to be greater than one.
\end{enumerate}

Typically, a programmer cannot affect the first factor, but should carefully consider it.
For example, in case of words pulled from natural language corpus it is generally hard to
presume whether their unification in program memory will be efficient:
Zipf's law \citep[Chapter 1]{manning:stat-nlp} suggests that the efficiency of words unification
will largely depend on the overhead imposed by the referencing scheme and if the references are
too heavy, the memory spared thanks to the most frequent words can be easily depleted by
the massive amount of words occurring only once.

The second factor might be tricky to consider in programs with non-linear work flow,
for example when the amount of items within program scope depends on some external
factor (time of day etc.), but in case of streamline input processing this value is
most often linear with respect to the input size.

The third factor is the only one that is completely in hands of the programmer.
With C++ we have two basic options, how to reference an object:
\begin{enumerate}
  \item by using object's address within memory
  \item by using a numeric index that uniquely identifies an object within a large storage
  (either a random access container or array).
\end{enumerate}

We may note that both options are fundamentally the same:
the pointer is basically a numeric index into the container of a maximum size (entire memory)
and the numeric index is just a pointer into restrained part of memory (container).
The pointer is easier to dereference, but comes in single size only -- the size of
\emph{processor unit word}, which on 64-bit architecture is 8~bytes long.
On the other hand, to dereference the numeric index, a corresponding container must be
accessed to determine the address of index start.
However, a numeric index data type can be adjusted to the size of domain of unified items,
if it is known in advance or can be assessed by some upper bound.
Being able to reference 4~billions of unique objects is enough in many situations and
in such cases only 4~bytes long integer type will suffice, reducing referencing overhead
to half of that of the pointer.

As a final note, we shall point out that objects unification comes with a cost of
additional processing overhead: especially insertion of new object is more time-consuming,
as every new object have to be first checked for an existing copy.

\subsection{Simple segregated storage}

Almost every non-trivial computer program uses dynamic memory allocation.
C++ provides two interfaces to ask for a new memory during program runtime:
one can either use legacy C API function \emph{malloc} or pure C++ API operator \emph{new}.
Both APIs has to provide means to satisfy any type of memory request -- and for any size.
Hence, usually the size of allocated memory block (and any other bookkeeping information) is
kept in memory chunk right before the block itself \citep[Chapter 10]{meyers:effective-cpp}.
This way, for deallocation either via \emph{free} function or operator \emph{delete}
memory allocator only needs the pointer to memory block that should be deallocated and
can easy determine, how much memory should be actually freed.

The obvious overhead in memory consumption incurred by this approach is usually negligible,
but may become significant in situations, when a load of small-sized memory blocks is allocated.
To avoid the overhead in such situations, a frequently used approach is to allocate big pool of
fixed-size memory blocks once a time and serve these blocks sequentially as new memory requests
come in, allocating a new pool whenever all memory blocks in existing pools are exhausted.
This technique is generally called \emph{memory pooling}, but when details of
memory pool allocation and deallocation matters, usually more specific names are used to
distinguish between various implementations --  a \emph{simple segregated storage} from Boost
library is an example of one such specific
implementation.\footurl{http://www.boost.org/doc/libs/1_53_0/libs/pool/doc/html/boost_pool/pool/pooling.html}

% Unions - in current version of Eppex they don't play significant role.

\subsection{Unordered sets}

The recent C++ standard,\footnote{ISO/IEC 14882:2011} marked as C++11, standardizes
a new type of containers, that were already part of almost every implementation of STL,
but due non-existing standardization they differ slightly in their interfaces across
various implementations.
The \emph{unordered associative containers} are well known to almost every programmer,
although usually they are referred to under a different name: \emph{hash tables}.

The C++11 standard defines four types of hash table containers that correspond to existing
binary-search-tree-based containers (with an \emph{unordered_} prefix):
\emph{unordered_set}, \emph{unordered_multiset}, \emph{unordered_map} and \emph{unordered_multimap}.
The key differences between hash tables and binary-search trees are:
\begin{itemize}
  \item Binary-search trees require less-than comparison operator to be defined for stored items,
  whereas hash tables require equal-to comparison operator and also a hash function to be
  defined for them.
  \item When iterating over binary-search tree the items are returned in order provided by
  less-than comparison operator, whereas iteration over hash table returns items in arbitrary
  order.
  \item Lookup, insertion and deletion operations with hash tables have amortized (constant)
  average cost per operation (one call to hash function plus one or more equality comparisons),
  whereas with binary-search tree these operations have cost dependent on the number of items
  in the tree ($N$): each operation requires $log_2(N)$ comparisons.
\end{itemize}

In the real life applications hash tables often perform much better than binary-search
trees, although their worst case complexity is linear with respect to number of items
(standard requires that hash tables are implemented using
\emph{closed addressing}\footurl{http://en.wikipedia.org/wiki/Hash_tables\#Separate_chaining}).
% TODO: Cite wikipedia or find a "better" source?
The effectiveness of hash tables stands and falls with a hash function:
a good hash function has to be fast to compute and display the quality of distributing
the data as evenly as possible over its output range.

\section{Implementation analysis and description}

During the program runtime the input data are processed in order to establish frequency counts
necessary to compute the phrase pair scores and print the complete phrase translation table.
This requires the phrase pairs to be extracted and kept in the memory along with the estimated
frequency and maximum error for Lossy Counting -- essentially, a set of triples $(e, f, \Delta)$
has to be maintained.\footnote{I.e. the data structure $D$ described in \Cref{chap:lossy-counting}.}
These data account for most of the memory required during the runtime, therefore their effective
representation in memory is mission-critical and was object of the major programming effort.

\subsection{Size of integer types}

Both frequency count and maximum error are plain unsigned integers, so at first it might seem
there is nothing that can be possibly done about their optimization.
However, it will be unwise to just declare them both as \verb|unsigned int|.
First, the size of integer data types is implementation dependent.
Despite that \verb|int| is typically 4 bytes long, this is not guaranteed \citep[Chapter 4.6]{stroustrup:cplusplus}.
Second, it is important to realize that by processing real life data we may never need as much
space as some integer types provide and saving even few bytes per item can lead to gigabytes
saved in total.

To ensure cross-platform compatibility, we base the definitions of estimated frequency count and
maximum error (and other crucial numeric types) on definitions of fixed-size integer types from
Boost.Integer library.\footurl{http://www.boost.org/doc/libs/1_53_0/libs/integer/doc/html/index.html}
For frequency counts we dedicated 4~bytes, to be able to store all unsigned values up to
$2^{32} \approx 4.3 \times 10^9$ -- this is definitely enough for our task as there is no
parallel corpus yet with billions of parallel sentences, therefore it is safe to expect no phrase
pair will occur billion times.
For the values of maximum error it is sufficient to reserve only 1~byte (range from 0 to 255)
-- maximum error is always less than the number of epochs in Lossy Counting and from our initial
experiments we had learned that having more than just a few epochs leads to pruning that already
hurts too much.\footnote{As results in \Cref{chap:results} demonstrate, with more data it is
possible to prune more (with more epochs) with no significant loss of quality, but still the 0-255
range displays a tremendous reserve.}

\subsection{Words, phrases, alignments}

% Types of objects stored:
% - words, phrases, alignments

In contrast to counters, a phrase pair is quite a complex structure: it consist of source phrase,
target phrase and also the information about alignment between particular words of both phrases.
Both source and target phrases can be further decomposed into sequence of words, while alignment
information can be further decomposed into sequence of pairs of alignment points.

Let us try to sketch a possible implementation of phrase pair structure.
Given the description above and being familiar with the containers from standard
template library, we might come up with the following declaration:
\begin{verbatim}
struct PhrasePair {
  std::vector<std::string> source; // Sequence of source words.
  std::vector<std::string> target; // Sequence of target words.
  std::vector<size_t> alignment; // Sequence of alignment points.
};
\end{verbatim}

Obviously, this is very simple and very naive solution.
The flexibility of STL containers comes at a cost: each container has to keep track of its
current size, the memory it has reserved etc.
In case of our \verb|PhrasePair| structure, the overhead of 3 instances of STL \verb|vector|
container requires 72 bytes of memory.\footnote{With GCC on 64-bit machine, on 32-bit machine
this value will be usually half as big.}
Assuming a parallel corpus with hundred of millions of phrase pairs, only to account for
this overhead, several gigabytes of memory would be required.

The main point of the hypothetical example above was therefore to stress the importance of
a careful analysis of the problem and a proper usage of optimization options and techniques
of C++ that were described in previous section.

So, let us now describe the actual implementation of phrase pairs handling within \eppex{}.
As mentioned, the main building blocks of phrase pair are words, phrases and alignments.

\subsubsection*{Words}

In \Sref{sec:strings} we have already discussed the advantage of C-strings over STL strings
when it comes to memory requirements of both.
Later, in \Sref{sec:objects-unification} we presented a technique of lowering program memory
footprint by keeping all unique copies of some type of objects in a single place and keeping
only a lightweight references to such copies in data structures that have to access them.
With both these options in mind, it is easy to come up with an idea to store all words as
C-strings in a separate storage and keep only pointers to them in phrase pair structure.
The only problem with this idea is that a pointer type in 64-bits environments occupies
8~bytes, what is simply excessive for representation of a single word (many of words could
be fit in 8 bytes directly).

Instead, as already suggested in \Sref{sec:objects-unification}, it is more effective to
use numeric index as a reference.
Although such solution incurs an additional overhead, because pointers to C-strings have to
be maintained in some index-like structure anyway, the nature of phrase pairs extraction
process suggests that each word occurrence in the input corpus will be referenced from within
several phrase pairs.\footnote{Recall that the phrase extraction algorithm may, in theory,
extract as many as $N^2$ phrases from a sentence of $N$ words.}
The data type of numeric index has to be capable of representing the whole domain of input
factor forms, therefore we picked up \verb|boost::uint32_t| data type from Boost.Integer
library for this purpose with a reasonable assumption that no model will have factor with
more than 4.2~billions of distinct forms.

\subsubsection*{Phrases}

With the words represented as numeric indexes, it is straight-forward to represent phrase as
a sequence of such indexes and this is in fact the solution we sticked to.

An alternative implementation could also make use of objects unification, but the positive
effect of this technique is doubtful on this level, because there are far more distinct
phrase forms than there are word forms.
Also, because a lot of phrase pairs will be pruned out during a typical \eppex{} run,
with object unification it would be impossible to wipe out their data without some reference
counting scheme, but such scheme would bring in an additional overhead, further diminishing
the already uncertain efficiency of such solution.\footnote{Although the same reasoning applies
for words, the scale of the problem is less prominent in this case as there are fewer word forms.
Consequently, we do not attempt to detect and remove words that are no more referenced by
a phrase pair (as a result of pruning).}

\subsubsection*{Alignments}

% Properties of phrase pair alignment.
The phrase pair alignment marks words from source and target side that were aligned to
each other in word alignment of sentence from which the phrase pair was extracted.
The phrase extraction algorithm ensures that each extracted phrase pair is based on
at least one alignment pair, ...

% TODO: Alignment length.

\subsubsection*{Phrase pairs}

Now it is time to put all the phrase pairs building blocks together: ...

\subsection{Persistence of objects}

...

% Indexed strings storage (and memory pools)

\subsection{Compressed I/O}

\Eppex{} can read/write directly from/to gzipped files, the same way legacy \emph{phrase-extract}
tools does.
This option allows to save a significant amount of disk space, as a typical phrase table will be
several times smaller when compressed.\footnote{In case of phrase tables produced in our experiments
the compression ratio of gzipped phrase tables varied between 14.0\% and 22.9\%.}
Moreover, in environments when disks are under heavy load (shared computation servers are often
the case), it may even speed up the whole I/O process.

Our implementation simply reuses the respective library shipped with Moses source code,
as it is cleanly designed and easy to include.

% "Copying" phrases between extraction and scoring
