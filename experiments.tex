\chapter{Experiments}
\label{chap:experiments}

We conducted a series of experimental phrase table extractions in order to
assess \eppex{} memory demands, determine the ratio of speed up of the whole
process obtainable with various pruning levels and evaluate the impact of both
lossy counting and significance filtering on the quality of pruned phrase tables.
In this chapter we are going to describe the parameters of our experiments,
the employed data sets and the means of runtime benchmarking and quality
evaluation, while the actual results and comments on them are presented in
\Cref{chap:results}.

\section{Setups}

\subsection{Baseline}

Our baseline is naturally determined by the phrase table creation process
that is implemented within 5th and 6th step of Moses training pipeline.
In the brief description of this process in section \ref{sec:phrase-table-creation-in-moses}
we have mentioned that both steps can be partially run in parallel.
This option helps to speed up the whole process whenever training is
attempted on a machine with multiple processing units (cores or threads).
Besides the parallelization, the training script also supports a more subtle
runtime optimization: the sorting process, that is invoked to sort both halves
of phrase table in the phrase extraction step and later to sort the inverse
half before final consolidation of both halves, may be enabled to use a bigger
memory buffer, order to compress the output or to run in parallel mode.
\Tref{baseline-optimization-parameters} sums up all the parameters of Moses
training script that can be set in order to optimize the process of phrase
table creation and briefly explains their role.

\begin{table}[ht]
\centering
\begin{tabular}{ r p{86mm} }
parameter & description \\
\hline
\hline
\verb|--parallel|               & run scoring of both phrase table halves in parallel \\
\verb|--cores <num>|            & run extraction of phrases and scoring of each table half using \verb|num| threads (eg. 4) \\
\verb|--sort-buffer <memsize>|  & allow sorting process to use internal buffer of \verb|memsize| RAM (eg. 12G)\\
\verb|--sort-compress <cmd>|    & order sorting process to compress the output using \verb|cmd| program (eg. gzip) \\
\verb|--sort-parallel <num>|    & allow sorting process to run using \verb|num| threads (eg. 4, sort program must support it)\\
\hline
\hline
\end{tabular}
\caption{\label{baseline-optimization-parameters}
List of parameters to Moses training script that allows to speed up
the process of phrase table creation.}
\end{table}

To give a better picture of the performance of the baseline system with respect
to various optimization options and to get some insight into their importance,
we decided to run experiments with four different sets of optimization parameters
each being either proper subset or superset of the others (the short name in
parentheses is designated for use in result tables):
\begin{itemize}
  \item default (\emph{def-base}) -- no optimization parameters are set
  \item multi-core (\emph{multi-base}) -- multiple cores are used
  \item compressed (\emph{comp-base}) -- multiple cores are used and
    sorting process is ordered to compress its output
  \item optimized (\emph{opt-base}) -- multiple cores are used, sorting process
    is ordered to compress its output and to use more than default amount of memory
\end{itemize}

This selection by our own experience should represent the most popular ways of
invoking phrase table creation process via Moses training script and is designed to
reflect phrase table creation benchmarks that earlier popped out on Moses mailing
list.\footnote{Our selection is particularly inspired by the most recent benchmark
we are aware of: \tt{http://article.gmane.org/gmane.comp.nlp.moses.user/6749}.}
Also, by exploring the impact of available optimization options we aim to establish
as challenging baseline as possible given the computation resources of the machines
at our disposal.

% On number of cores...
In the non-default baselines we favored to turn on parallel processing using \verb|--cores|
option as this setting applies to both steps of phrase table construction, whereas
\verb|--parallel| is applicable only to scoring step.
Despite that all machines employed in our experiments have 8 or more cores (or threads),
in most of the experiments we decided to utilize 4 cores only as this is configuration
affordable not only on computational servers equipped with multiple CPUs, but also on most
of the modern laptops and we expect such results to be more apt for the Moses community.
All the time, however, we intentionally left at least one core idle to make sure our
benchmarking script or any administrative process of software managing the computational
cluster will not interfere with benchmarking.

% On sort options...
Regarding the optimization parameters for \emph{sort}, we use \emph{gzip} as compressing
program and set sort buffer size to either use the whole memory available on machine
(with some reasonable reserve for the remaining processes) or use approximately the same
amount of memory as consumed by \eppex{} when creating the full phrase table from
the same data (ie. without pruning -- the most "hungry" setting).
We did not set parallelization on for \emph{sort} -- simply because we were using older
version of \emph{sort} program that did not yet implement this feature.

% Moses 1.0 only.
For clarity and reproducibility of results we sticked to Moses release 1.0 in all our 
experiments.\footnote{Precisely the Github commit 1530ae4f5ff59cfd30f6f933e4dc05ea89cca8fc.}

\subsection{Significance filtering}

% Post-filtering baseline.
Post-filtering the phrase table with \emph{significance filter} can be considered as yet
another baseline, sensible mainly for quality benchmarking, because time requirements
will be by design above those of baseline phrase extraction.
Nevertheless, we attempted both performance and quality benchmarking of the \emph{sigfilter}
tool introduced in \Sref{sec:significance-filtering} to give the whole picture.

% Parameters of sigfiltering.
The degree of pruning done by \emph{sigfilter} is determined by the pruning
threshold for p-values (set via \verb|-l| option), by cutoff limit for forward
(direct) translation probabilities (set via \verb|-n| option) or by both.
A particularly interesting values for pruning threshold are \verb|a+e| and
\verb|a-e|: the former implies $\alpha + \epsilon$ threshold and results
in removal of all 1-1-1 phrase pairs, the latter implies $\alpha - \epsilon$
threshold and result in all 1-1-1 phrase pairs being preserved.
As for cutoff limit, in \citep{johnson:sigfilter} a limit of 30 is applied to
all experiments in order to control the width of beam search in decoding.
In Moses, the width of beam is controlled directly by the decoder, but
in the manual section on significance filtering the cutoff limit of 30
in combination with pruning threshold $\alpha + \epsilon$ is recommended as
"a good setting". % TODO: Footnote the section again?

% TODO: A sum up table of various experiments and their parameters?

% Preprocessing steps.
Before \emph{sigfilter} can be run, some preprocessing is required.
The counts $C(\bar{s})$, $C(\bar{t})$ and $C(\bar{s},\bar{t})$ necessary for
population of contingency tables are established on the fly during the
pruning process by querying a suffix-array index built over the parallel
corpus used for phrase table creation.
This suffix-array index has to be produced by indexer that is part of SALM
toolkit,\footurl{http://projectile.sv.cmu.edu/research/public/tools/salm/salm.htm}
and because SALM indexer cannot process sentences longer than 254 words,\footnote{Although SALM
indexer complains about sentences being longer than 256 words, the actual limit is 254 words.}
such sentences have to be trimmed in advance.

\subsection{Eppex}

The runtime performance of \eppex{} and the degree of pruning is determined
by the parameters of Lossy Counting.
For each phrase pair length a separate Lossy Counter instance can be set.

% all-in setup
The simplest case is to use no pruning at all. All phrase pairs will be
output and the phrase table will be equal to the one created by baseline
extraction.\footnote{Equal does not necessarily mean identical: some minor
differences in scores with floating point values might occur due to rounding
errors.}

% sigfilter-like setups
Another setup interesting to consider is the approximation of significance
filtering with $\alpha \pm \epsilon$ thresholds.
By setting \emph{support} $s$ to value such that $sN < 1$, all of the 1-1-1
phrase pairs will be preserved (like in $\alpha - \epsilon$), while setting
\emph{support} $s$ and \emph{error} $\epsilon$ to satisfy $(s - \epsilon)N > 1$
will result in their complete removal (like in $\alpha + \epsilon$).

% frequency limits setup
To set \emph{support} and \emph{error} parameters properly, the value of $N$,
that is the number of all extracted phrase pairs, have to be known in advance.
% TODO: Finish.

\section{Data}
% Data we used in experiments.

%\subsection*{Cs-En}
% Czech-English
We used CzEng 1.0 parallel corpus \citep{czeng10:lrec2012} to train Czech-English
translation model...
% TODO: Update with more precise info when Obo reply.

%\subsection*{Fr-En}
% French-English
To confront \eppex{} with even more challenging input data (in terms of size),
we also prepared French-English translation model using data available for
Shared Machine Translation Task for 2013 Workshop on Statistical Machine
Translation (WMT~13 in short).\footurl{http://www.statmt.org/wmt13/translation-task.html}
These data consist of the following parallel corpora:
\begin{itemize}
  \item Europarl v7 (Fr-En data)\footurl{http://www.statmt.org/europarl/}
  \item Common Crawl corpus
  \item United Nations corpus\footurl{http://www.euromatrixplus.net/multi-un/}
  \item News Commentary corpus
  \item $10^9$ French-English corpus
\end{itemize}
The size of our joint French-English training data is (after preprocessing described
below) 39,143,703 sentences with 1,173,485,756 French and 1,000,538,764 English tokens.
% TODO: Add information about monolingual data used.

%\subsection*{cu-bojar}
% Czech-English (cu-bojar)
Finally, we were curious to get some insight in the rate of performance improvement
of both \eppex{} and \emph{phrase-extract} tools since their older versions that have
been evaluated in a similar manner by \citet{przywara:eppex}.
Therefore, as a third dataset we picked up the same data that have been used in their
work -- see \citet{marecek:twostep} for the exact setup of the system \emph{cu-bojar}.
This parallel corpus is the smallest from our three configurations: it contains
approximately 8.4~M sentence pairs with 93.2~M Czech and 107.2~M English tokens.
Since we are mainly interested in comparison of the runtime benchmarking figures,
we did not attempt the quality benchmarking with this dataset.
Also, we omit the significance filtering setup, as \emph{sigfilter} did not change
since the version tested in 2011.

\subsection{Preprocessing}

Input data preprocessing is essential to almost any natural language processing task
and model training in SMT is no exception.

We treated our input data with following preprocessing steps:
\begin{enumerate}
  \item Cleaning of tokenization format:
  \begin{itemize}
    \item spaces at the beginning and end of the lines are removed
    \item all sequences of white space characters are replaced by a single space only
    \item all line endings are converted to Unix-style line endings
  \end{itemize}
  \item Normalization of punctuation encoding
  \item Tokenization
  \item Cleaning of any superfluous white space from tokenization -- ensures that the output contains only spaces and line-feed characters
  \item Special characters escaping:
  \begin{itemize}
    \item pipes ("\textbar") are replaced by "\&pipe;"
    \item less-than characters ("<") are replaced by "\&lt;"
    \item greater-than characters (">") are replaced by "\&gt;"
    \item ampersands ("\&") are replaced by "\&amp;"
  \end{itemize}
\end{enumerate}

We used our own tools for cleaning of tokenization format,
post-tokenization cleaning and escaping of special characters.
For normalization of punctuation encoding we used script provided by organizers of
WMT 2011 Translation Task,\footurl{http://www.statmt.org/wmt11/normalize-punctuation.perl}
while for tokenization we used \emph{tokenizer} shipped with
Moses.\footnote{Tokenization script may be found in \texttt{scripts/tokenizer/tokenizer.perl}}

The alignment tool, GIZA++, further requires that too long sentences are removed from parallel corpus.
No hard-coded limit exists, but as a rule of thumb the limit of 80 words is
recommended.\footnote{The 80 words limit is for example mentioned in "Corpus Preparation" section of Moses tutorial:
\url{http://www.statmt.org/moses/?n=moses.baseline}}
During sentence-length filtering both sides of parallel corpus must be processed simultaneously,
because the overlong sentence must be removed along with its counterpart from the other part of corpus
to keep the remaining sentences properly aligned.
Usually during this step one wants to remove also any empty lines as the same principle applies to them.

\section{Runtime benchmarking}

% What has been benchmarked?
We benchmarked phrase table construction by measuring CPU time, wall clock time,
virtual memory usage peak and disk usage peak both of \eppex{} and standard Moses
pipeline.

% Environment.
For all experiments we used servers that are part of Sun Grid Engine cluster.
Although these machines were standard nodes in a cluster, we kept jobs of other
users away by ordering SGE to reserve all memory of the machine for our job only.
All the input and output files of benchmarked processes were read from and written
to a locally mounted hard disk, therefore network load had no impact on
benchmarking results.

% Software and hardware configuration.
All the servers had identical software configuration:
they were running 64-bit version of Ubuntu 10.04 server edition.
For each dataset we use a server or set of servers with hardware configuration
that satisfies memory and disk demands incurred by the size of particular dataset:
\begin{itemize}
  \item "cu-bojar" configuration was run on a machine with two Quad-Core
  AMD Opteron\texttrademark{} 2387 processors (each with 4~cores with clock speed
  of 2.8~GHz; 8~threads in total),
  32~GB of RAM and approximately 429~GB of hard disk space. % andromeda7
  \item "cs-en" configuration was run on several machines with identical hardware
  configuration: two Intel\textregistered{} Xeon\textregistered{} E5620
  processors\footurl{http://ark.intel.com/products/47925/Intel-Xeon-Processor-E5620}
  (each with 4 hyper-threaded cores with clock speed of 2.4~GHz; 16 threads in total),
  128~GB of RAM and approximately 558~GB of hard disk space. % lucifer[6-9]
  \item "fr-en" configuration was run on a machine with two Intel\textregistered{}
  Xeon\textregistered{} E7520
  processors\footurl{http://ark.intel.com/products/46490/Intel-Xeon-Processor-E7520}
  (each with 4 hyper-threaded cores with clock speed of 1.866~GHz; 16 threads in total),
  512~GB of RAM and approximately 1.7~TB of hard disk space. % iridium
\end{itemize}

% How the benchmarking was implemented?
% TODO: Make script available online.

% On wall clock, CPU time and VM peak benchmarking.
The baseline phrase table creation managed by the training script involves
running multitude of working subprocesses to perform various intermediate
tasks like phrase pairs extraction, phrase pairs scoring, phrase table
consolidation, gzipping, wrapping the parallel execution etc.
To measure the overall runtime demands of this dynamically changing tree
of processes, we crafted a special Python script:
it periodically gathers list consisting of the main observed
process\footnote{The main training script \texttt{train-model.perl} in our case.}
and all its subprocesses, captures the values of their CPU time used so far
and currently occupied virtual memory and updates the total CPU time counter
and virtual memory peak.
To retrieve a list of child processes a call to Unix \verb|ps| utility is
performed, CPU time and virtual memory usage of each process are read from
\verb|stat| and \verb|status| files from the respective \verb|/proc/[pid]/|
directory.
A period of one second gives a reasonable precision of benchmark, while
not damaging the performance of running processes.
The script provides a full log with benchmarking information of all detected
subprocesses, but we are mainly interested in the final summary that presents
the total wall clock time, aggregated CPU time consumed by all subprocesses
and their joint virtual memory peak.

% On disk usage peak measurement.
The disk usage have been measured by a separate bash script that invoked Unix
\texttt{du} command with summarizing option on the working directory with
a period of 1 second and updated relevant log file with the full output of
\texttt{du} whenever new peak was reached.
In case of \eppex{} experiments the peak disk usage is always equal to the
size of phrase table file as there are no temporary files produced and we
ignored the disk usage incurred by input files.

\section{Phrase table quality benchmarking}

% BLEU score

...
