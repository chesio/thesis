\chapter{Experiments}
\label{chap:experiments}

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{Hang on. I'm trying to see how long it takes a 500-kilowatt oxygen iodine laser to heat up my Cup o' Noodles.}{--- Leslie Winkle, "The Big Bang Theory" TV series}

We conducted a series of experimental phrase table extractions in order to
assess \eppex{} memory demands, determine the speed up ratio of the whole
process obtainable with various pruning levels and evaluate the impact of both
lossy counting and significance filtering on the quality of pruned phrase tables.
In this chapter, we are going to describe the parameters of our experiments,
the employed data sets and the means of runtime benchmarking and quality
evaluation, while the actual results and comments on them are presented in
\Cref{chap:results}.

\section{Scenarios}

We use the term scenario to denote a particular toolkit (or phrase table construction process)
and the set of its configurations that we aim to evaluate.

\subsection{Baseline}
\label{sec:baseline-experiments}

Our baseline is naturally determined by the phrase table creation process
that is implemented within the 5th and 6th step of the Moses training pipeline.
In the detailed description of this process in section \ref{sec:phrase-table-creation-in-moses}
we have mentioned that both steps can be partially run in parallel.
This option helps to speed up the whole process whenever training is
attempted on a machine with multiple processing units (cores or threads).
Besides this parallelization, the training script also supports a more subtle
runtime optimization: the sorting process, that is invoked to sort both halves
of phrase table in the phrase extraction step and later to sort the inverse
half before final consolidation of both halves may be configured to use a bigger
memory buffer, to compress its temporary data stored on disk or to run in parallel mode.
\Tref{baseline-optimization-parameters} sums up all the parameters of the Moses
training script that can be set in order to optimize the process of phrase
table creation and briefly explains their role.

\begin{table}[ht]
\centering
\begin{tabular}{ r p{86mm} }
parameter & description \\
\hline
\hline
\verb|--parallel|               & run the scoring of both phrase table halves in parallel \\
\verb|--cores <num>|            & run extraction of phrases and the scoring of each table half using \verb|num| threads (eg. 4) \\
\verb|--sort-buffer <memsize>|  & allow the sorting process to use internal buffer of \verb|memsize| RAM (eg. 12G)\\
\verb|--sort-compress <cmd>|    & order the sorting process to compress the output using a \verb|cmd| program (eg. gzip) \\
\verb|--sort-parallel <num>|    & allow the sorting process to run using \verb|num| threads (eg. 4, the sort program must support it)\\
\hline
\hline
\end{tabular}
\caption{\label{baseline-optimization-parameters}
List of parameters of the Moses training script that allow a speeding up of
the process of phrase table creation.}
\end{table}

To give a better picture of the performance of the baseline system with respect
to various optimization options and to get some insight into their importance,
we decided to run experiments with four different sets of optimization parameters
each being either a proper subset or superset of the others (the short name in
parentheses is designated for use in result tables):
\begin{itemize}
  \item default (\emph{def-base}) -- no optimization parameters are set
  \item multi-core (\emph{multi-base}) -- multiple cores are used
  \item compressed (\emph{comp-base}) -- multiple cores are used and
    the sorting process is ordered to compress its output
  \item optimized (\emph{opt-base}) -- multiple cores are used, the sorting process
    is ordered to compress its output and to use more than the default amount of memory
\end{itemize}

This selection in our own experience should represent the most popular ways of
invoking a phrase table creation process via the Moses training script and is designed to
reflect phrase table creation benchmarks that earlier stood out on the Moses mailing
list.\footnote{Our selection is particularly inspired by the most recent benchmark
we are aware of: \url{http://article.gmane.org/gmane.comp.nlp.moses.user/6749}}
Also, by exploring the impact of available optimization options we aim to establish
as challenging a baseline as possible given the computational resources of the machines
at our disposal.

% On number of cores.
In the non-default baselines, we favored turning on parallel processing using the \verb|--cores|
option as this setting applies to both steps of phrase table construction, whereas
\verb|--parallel| is applicable only to scoring step.
Despite the fact that all the machines employed in our experiments have 8 or more cores (or threads),
in most of the experiments we decided to utilize 4 cores only as this is configuration
affordable not only on computational servers equipped with multiple CPUs, but also on most
of the modern laptops and we expect such results to be more suited to the Moses community.
At all times, however, we intentionally left at least one core idle to make sure our
benchmarking script or any administrative process of software managing the computational
cluster will not interfere with the benchmarking.

% On sort options.
Regarding the optimization parameters for \emph{sort}, we use \emph{gzip} as a compression
program and set the sort buffer size to either use the whole memory available on the machine
(with some reasonable reserve for the remaining processes) or to use approximately the same
amount of memory as consumed by \eppex{} when creating the full phrase table from
the same data (ie. without pruning -- the most "hungry" setting).
We did not set parallelization on for \emph{sort} -- simply because we were using an older
version of the \emph{sort} program that did not yet implement this feature.

% Moses 1.0 only.
For clarity and reproducibility of results we stuck to Moses release 1.0 in all our 
experiments.\footnote{Precisely the Github commit 1530ae4f5ff59cfd30f6f933e4dc05ea89cca8fc.}

\subsection{Significance filtering}

% Post-filtering baseline.
Post-filtering the phrase table with a \emph{significance filter} can be considered as yet
another baseline, mainly applicable to quality benchmarking, because the time requirements
of phrase table construction with significance filtering will be by design above those of
the baseline phrase extraction.
Nevertheless, we attempted both performance and quality benchmarking of the \emph{sigfilter}
tool introduced in \Sref{sec:significance-pruning} to give the whole picture.

% Parameters of sigfiltering.
The degree of pruning done by \emph{sigfilter} is determined by the pruning
threshold for p-values (set via \verb|-l| option), by the cutoff limit for forward
(direct) translation probabilities (set via \verb|-n| option) or by both.
Particularly interesting values for the pruning threshold are \verb|a+e| and
\verb|a-e|: the former serves to set threshold value to $\alpha + \epsilon$ and
results in removal of all 1-1-1 phrase pairs, the latter serves to set the threshold
value to $\alpha - \epsilon$ and results in all 1-1-1 phrase pairs being preserved.
As for the cutoff limit, \citet{johnson:sigfilter} applied a limit of 30 in all
experiments in order to control the width of beam search in decoding.
In Moses, the width of beam is controlled directly by the decoder, but
in the manual section dealing with significance filtering a cutoff limit of 30
in combination with a pruning threshold $\alpha + \epsilon$ is recommended as
"a good setting".

% Preprocessing steps.
Before \emph{sigfilter} can be run, some preprocessing is required.
The counts $C(\bar{s})$, $C(\bar{t})$ and $C(\bar{s},\bar{t})$ necessary for
the population of contingency tables are established on the fly during the
pruning process by querying a suffix-array index built over the parallel
corpus used for phrase table creation.
This suffix-array index has to be produced by the indexer that is part of SALM
toolkit,\footurl{http://projectile.sv.cmu.edu/research/public/tools/salm/salm.htm}
and because the SALM indexer cannot process sentences longer than 254 words,\footnote{Although
our version of the SALM indexer complained about sentences being longer than 256 words,
the actual limit was 254 words.} such sentences have to be trimmed in advance.

\subsection{Relative entropy filtering}

We attempted to post-filter the phrase tables with the \emph{relent-filter} toolkit as well,
but it turned out that the forced decoding of all phrases in the phrase table required by this
method makes it extremely time consuming when dealing with phrase tables as enormous as ours:
in the case of the Cs-En setup described below, it took more than 30~hours to calculate the pruning
scores for 336~K of phrase pairs (approx. 0.1\% of the whole phrase table).
Despite the fact that the filtration process can be scaled by running multiple
instances of the scoring script in parallel, the applicability of such an approach is in practice
limited by the considerable memory demands of forced decoding: in our case, the parallel running of
two \emph{relent-filter} instances fed with 25~K phrases to be decoded at a time required
45~GB of memory.
Such excessive time requirements made it impossible to fit the proper evaluation of
a \emph{relent-filter} into our experiment schedule, therefore we had to abandon it.

\subsection{Eppex}

The runtime performance of \eppex{} and the degree of pruning heavily depends
on the parameters of Lossy Counting: more intensive pruning results in less phrase pairs
being extracted and then scored, thus more pruning leads to faster \eppex{} runs.

The earlier experiments with \eppex{} \citep{przywara:eppex} suggested that lossy-counted
pruning should not be very intensive: especially the removal of short phrase pairs can be
damaging to the quality of translation model.
These findings were also the main reason behind the option to instantiate a separate
Lossy Counter instance for each phrase pair length (or range of them) in \eppex{}.

% all-in setup
However, the simplest scenario to run is to use no pruning at all and let \eppex{} output
a phrase table equal to the one that would be created by a baseline extraction.\footnote{Equal
does not necessarily mean identical: some minor differences in scores with floating point
values might occur due to rounding errors.}
Obviously, this scenario is also the most memory consuming.

% hypothetical p=n+1 setup
Perhaps not as obviously, the scenario with \emph{positive and negative limits} set at
$p = n + 1$ is as memory consuming as the one with no pruning.
The same explanation applies in both the cases, although here it might be not as striking:
when the Lossy Counting algorithm is given no space for error, it will make none,
but will consequently reach the space complexity upper bound.
Therefore, despite the fact that this scenario is theoretically interesting, it is not very
useful in practice, unless one has plenty of RAM available and wants to perform perfect
count-based pruning on the fly.

% sigfilter-like setups
A more practically interesting scenario to consider is the possibility of approximating
significance filtering with $\alpha \pm \epsilon$ thresholds with a proper setting of
\emph{limits}: by setting the \emph{positive limit} to~1, all of the 1-1-1 phrase pairs
will be preserved (like in $\alpha - \epsilon$), while by setting the \emph{negative limit} to~1,
all single occurring phrase pairs, so called \emph{singletons}, will be removed
(like in $\alpha + \epsilon$).
In addition, the option to set a separate Lossy Counting for each phrase pair length
allows us to come up with a mix of both approaches:
define a less intensive $\alpha - \epsilon$-like pruning for
the shorter phrase pairs and a more intensive $\alpha + \epsilon$-like pruning for the
longer phrase pairs.

% frequency-range rising limits setup
To sum up this reflection on various \eppex{} scenarios, the main reasoning that should
determine the settings of \emph{limits} is the observation that short phrase pairs are
more crucial to the overall translation quality of phrase-based models than the longer ones
and therefore short phrases have to be pruned with more caution.
This phenomenon was also observed by \citet{zens:systcomp} when examining the
properties of output produced by various pruning methods. In the case of methods that
performed well in their evaluations, they found out that the more aggressive the pruning,
the larger the percentage of short phrases.

Finally, an important note to make is that for all experiments with \eppex{} we have used
the version 2.5.1\footnote{Precisely the Github commit 7052adc50c46f74d36175632977f6b93c2e4f931.}
compiled with the \verb|--with-hashtables| option.\footnote{Compilation options are explained in
\Sref{sec:installation}.}

\section{Datasets}
% Data we used in experiments.

The thesis assignment specifically dictated that a massive parallel dataset has to be
employed in the evaluation of epochal extraction performance.
For this reason, we picked up data available as part of the Shared Machine Translation Task
for 2013 Workshop on Statistical Machine Translation (or WMT~13 in short) that presents
the de facto standard in the area of MT systems evaluation.\footurl{http://www.statmt.org/wmt13/translation-task.html}

\subsection{The Cs-En setup}
% Czech-English

The Czech-English setup was based on data and models prepared as a part of a broader
effort to deliver a competitive system for the WMT~13 Translation Task.\footnote{The author
took only minor part in this effort, but was kindly allowed to make use of the results
including the baseline system and all data not directly produced by or depending on
the tools evaluated in this work.}
%The description of this setup is therefore a rather brief as only part of the system
%has been built directly by the author.

As training data for construction of the translation model we put together a CzEng 1.0
parallel corpus\footurl{http://ufal.mff.cuni.cz/czeng/czeng10/} \citep{czeng10:lrec2012}
and the Czech-English section of the Europarl v7 corpus\footurl{http://www.statmt.org/europarl/}.
The combined data have 15,478,910 parallel sentences with 220,173,420 Czech and 253,283,160
English tokens.

The word-alignments were established on a lemmatized version of the combined corpus using
the recently published \emph{fast-align} tool \citep{dyer:fastalign}.

The translation model was created with two factors on both sides of the corpus:
\begin{enumerate}
  \item a token with case determined by a supervised truecaser
  \item a tag assigned by the state-of-art morphological tagger \emph{Morče}\footurl{http://ufal.mff.cuni.cz/morce/}
\end{enumerate}

The translation model defined only a single decoding step: two factors on the source side
were directly translated to two factors on the target side.\footnote{To say it in the language
of a Moses command line: translation factors were set to \texttt{0,1-0,1}.}

Three language models were used:
\begin{enumerate}
  \item an interpolated language model of order 6 for the first factor built from News Crawl
    articles from the years 2007-2011 treated with the supervised truecaser and with the perplexity tuned
    on the target side of the \emph{news-test2010} dataset
  \item an interpolated language model of order 8 for the second factor built from (and tuned
    on) the same data as the model for the first factor
  \item a language model of order 6 for the first factor built from the target side of CzEng 1.0
    treated with the supervised truecaser
\end{enumerate}

We did not train the lexicalized reordering model and instead relied only on the built-in
distance-based distortion model.

The weights of all the components were tuned using the Minimum Error Rate Training (MERT)
implementation that is part of Moses \citep{bertoldi:mert}.\footurl{http://www.statmt.org/moses/?n=FactoredTraining.Tuning}
The \emph{news-test2011} dataset was used as the development data for tuning.

The BLEU score was evaluated using the \texttt{evaluator} tool that is also part of
Moses\footnote{The \texttt{evaluator} tool is implemented in file \texttt{<moses>/mert/evaluator.cpp}}
with a bootstrap value of 1000.
The \emph{news-test2012} and \emph{news-test2013} datasets were used as the testing data.

\subsection{The Fr-En setup}
% French-English

To confront \eppex{} with even more challenging input data (in terms of their size),
we built and evaluated a French-English MT system, as significantly more French-English parallel
corpora were available as part of the WMT~13 Translation Task. Concretely, the training data for
the French-English setup were gathered from the following parallel corpora:
\begin{itemize}
  \item Europarl v7 (Fr-En data)\footurl{http://www.statmt.org/europarl/}
  \item Common Crawl corpus
  \item United Nations corpus\footurl{http://www.euromatrixplus.net/multi-un/}
  \item News Commentary corpus
  \item $10^9$ French-English corpus
\end{itemize}
The size of our joint French-English training data (after the preprocessing described
at the end of the section) is 39,143,703 sentences with 1,173,485,756 French and 1,000,538,764
English tokens.

GIZA++ \citep{och:giza}, the recommended word-aligner for Moses,\footurl{http://www.statmt.org/moses/?n=FactoredTraining.RunGIZA}
was used to establish the word alignments over the lowercased form of each
of the Fr-En parallel corpora, the particular alignments were then merged to
obtain a single word-alignment for the whole training data.
The lowercased form of the training data was also used as the only factor on both sides of
the translation model.
To build the language model, only the target side of parallel corpora was used.
As in the Cs-En setup, no lexicalized reordering model was built and reordering was
determined only by the built-in distortion model.

The weights tuning and the evaluation of translation quality were done by the same methods
(the MERT implementation from Moses and the \texttt{evaluator} tool) and the same datasets
(\emph{news-test2011} for the tuning, \emph{news-test2012} and \emph{news-test2013} for
the evaluation) as in case of the Cs-En setup. We did not, however, truecase the
output translations before evaluation, instead we lowercased the reference side of the test data.

\subsubsection*{Preprocessing}

Input data preprocessing is essential to almost any natural language processing task
and translation model training in phrase-based SMT is no exception.

We treated our French-English data with the following preprocessing steps:
\begin{enumerate}
  \item White space cleaning:
  \begin{itemize}
    \item spaces at the beginning and end of the lines are removed
    \item all sequences of white space characters are replaced by a single space only
    \item all line endings are converted to Unix-style line endings
  \end{itemize}
  \item Normalization of punctuation to plain ASCII quotes
  \item Tokenization -- splits the text into the atomic units of translation.
  \item Cleaning of any superfluous white space from tokenization -- ensures that the output contains only spaces and line-feed characters.
  \item Special characters escaping (necessary in the case of Moses):
  \begin{itemize}
    \item ampersands ("\&") are replaced by "\&amp;"
    \item pipes ("\textbar") are replaced by "\&pipe;"
    \item less-than characters ("<") are replaced by "\&lt;"
    \item greater-than characters (">") are replaced by "\&gt;"
  \end{itemize}
\end{enumerate}

We used our own tools for white space cleaning, post-tokenization cleaning
and the escaping of special characters.
For the normalization of punctuation encoding we used a script provided by organizers of
the WMT~11 Translation Task,\footurl{http://www.statmt.org/wmt11/normalize-punctuation.perl}
and for tokenization we used the tokenizer shipped with
Moses.\footnote{Tokenization script may be found in \texttt{<moses>/scripts/tokenizer/tokenizer.perl}}

The word-alignment tool we employed, GIZA++, further requires that overly long sentences are
removed from the parallel corpus.
No hard-coded limit exists, but as a rule of thumb the limit of 80 words is
recommended.\footnote{The 80 words limit is for example mentioned in the "Corpus Preparation"
section of the Moses tutorial: \url{http://www.statmt.org/moses/?n=moses.baseline}}
During sentence-length filtering both sides of a parallel corpus must be
processed simultaneously, because each overlong sentence must be removed along
with its counterpart from the other part of corpus to keep the remaining sentences
properly aligned.
Usually during this step one wants to remove also any empty lines as the same principle
applies to them.

\subsection{The 2011 setup}
% Czech-English (cu-bojar)
Finally, we were curious to get some insight into the rate of performance improvement
of both \eppex{} and \emph{phrase-extract} tools since their older versions that have
been evaluated in a similar manner by \citet{przywara:eppex}.
Therefore, as a third dataset we picked up the same data that had been used in their
work -- see \citet{marecek:twostep} for the exact setup of the system \emph{cu-bojar}.
This parallel corpus is the smallest of our three configurations: it contains
approximately 8.4~M sentence pairs with 93.2~M Czech and 107.2~M English tokens.

Since we are mainly interested in a comparison of the runtime benchmarking figures,
we did not attempt quality benchmarking with this dataset.
Also, we omit the significance filtering scenario, as \emph{sigfilter} has not changed
since the version tested in 2011.

\section{Benchmarking}

In computing, the act of running a computer program or a set of programs, to assess their relative
performance, is called \emph{benchmarking}.
Because the ultimate goal of this work was to implement a software tool that is an alternative
to existing programs, the benchmarking of both \eppex{} and the existing tools is an important part of
our work.

\subsection{Runtime}
\label{sec:runtime-benchmarking}

% What has been benchmarked?
We benchmarked phrase table construction by measuring CPU time, wall clock time,
virtual memory usage peak and disk usage peak.

% Environment.
For all experiments we used servers that are part of a Grid Engine (GE) cluster.\footnote{Formerly
known as the Sun Grid Engine, Grid Engine is now a continuation of the open source development of SGE:
\url{http://sourceforge.net/projects/gridscheduler/}}
Although these machines were standard nodes in a cluster, we kept jobs of other
users away by ordering the GE to reserve all the memory of the machine for our job only.
All the input and output files of benchmarked processes were read from and written
to a locally mounted hard disk, therefore network load had no impact on
benchmarking results.

% Software and hardware configuration.
All the servers had identical software configuration:
they were running the 64-bit version of the Ubuntu 10.04 server edition.
For each dataset we use a server or set of servers with hardware configuration
that satisfies memory and disk demands incurred by the size of the particular dataset:
\begin{itemize}
  \item The "cu-bojar" setup was run on a machine with 2 Quad-Core
  AMD Opteron\texttrademark{} processors\footurl{http://products.amd.com/pages/OpteronCPUDetail.aspx?id=536}
  (each with 4~cores with clock speeds of 2.8~GHz; 8~threads in total),
  32~GB of RAM and approximately 429~GB of hard disk space. % andromeda7
  This is the same machine that was employed in the 2011 experiments.
  \item The Cs-En setup was run on several machines with identical hardware
  configuration: two Intel\textregistered{} Xeon\textregistered{} E5620
  processors\footurl{http://ark.intel.com/products/47925/Intel-Xeon-Processor-E5620}
  (each with 4 hyper-threaded cores with clock speeds of 2.4~GHz; 16 threads in total),
  128~GB of RAM and approximately 558~GB of hard disk space. % lucifer[6-9]
  \item The Fr-En setup was run on a machine with two Intel\textregistered{}
  Xeon\textregistered{} E7520
  processors\footurl{http://ark.intel.com/products/46490/Intel-Xeon-Processor-E7520}
  (each with 4 hyper-threaded cores with clock speeds of 1.866~GHz; 16 threads in total),
  512~GB of RAM and approximately 1.7~TB of hard disk space. % iridium
\end{itemize}

% On wall clock, CPU time and VM peak benchmarking.
The baseline phrase table creation managed by the training script involves
running a multitude of working subprocesses to perform various intermediate
tasks like phrase pair extraction, phrase pair scoring, phrase table
consolidation, gzipping, wrapping parallel execution etc.
% How the benchmarking was implemented?
To measure the overall runtime demands of this dynamically changing tree
of processes, we crafted a special Python script:\footnote{The script is available
from: \url{https://github.com/chesio/tools/tree/master/Benchie}}
it periodically gathers a list consisting of the main observed
process\footnote{The main training script \texttt{train-model.perl} in our case.}
and all its subprocesses, captures the values of their CPU time used so far
and currently occupied virtual memory and updates the total CPU time counter
and virtual memory peak.
To retrieve a list of child processes a call to Unix \verb|ps| utility is
performed, CPU time and virtual memory usage of each process are read from
\verb|stat| and \verb|status| files from the respective \verb|/proc/[pid]/|
directory.
A period of one second gives a reasonable precision of a benchmark, while
not damaging the performance of running processes.
The script provides a full log with benchmarking information of all detected
subprocesses, but we are mainly interested in the final summary that presents
the total wall clock time, aggregated CPU time consumed by all subprocesses
and their joint virtual memory peak.

% On disk usage peak measurement.
The disk usage has been measured by a separate bash script that invoked the Unix
\texttt{du} command with the summarizing option on the working directory with
a~period of 1 second and updated relevant log file with the full output of
\texttt{du} whenever a new peak was reached.
In the case of the \eppex{} experiments the peak disk usage is always equal to the
size of phrase table file as there are no temporary files produced and we
ignored the disk usage incurred by input files.

\subsection{Memory demands}
\label{sec:memory-benchmarking}

We performed an additional series of experimental phrase table extractions with
the Cs-En and Fr-En datasets to get an insight into the impact of training data size and
the use of different pruning limits on the memory demands of \eppex{} and the amount of phrase pairs
extracted.

For both datasets we created a list of batch sizes and with each batch of size $K$ we did the following:
\begin{enumerate}
  \item From the first $K$ sentences of the whole dataset and their word alignments, we
    created a temporary training dataset.
  \item Using Moses \texttt{train-model.perl} script, we constructed lexical scores tables
    from the temporary training data.\footurl{http://www.statmt.org/moses/?n=FactoredTraining.GetLexicalTranslationTable}
  \item Finally, we used the temporary training data and lexical scores tables to perform several epochal
    extractions with some of the configurations mentioned in
    \Sref{sec:cs-en-results} (for Cs-En batches) and \Sref{sec:fr-en-results} (for Fr-En batches).
    In each extraction, we measured the virtual memory peak and recorded phrase table size.
\end{enumerate}
