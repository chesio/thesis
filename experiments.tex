\chapter{Experiments}
\label{chap:experiments}

We conducted a series of experimental phrase table extractions in order to
assess \eppex{} memory demands, determine the ratio of speed up of the whole
process and evaluate the impact of filtering on the ultimate quality of phrase
tables.
In this chapter we are going to describe the parameters of our experiments,
while the results of runtime and quality benchmarking are presented in
\Cref{chap:results}.

\section{Baseline setup}

Our baseline is naturally determined by the \emph{phrase-extract} suite that
is core part of Moses toolkit.
In \Cref{chap:phrase-based} we explained that the phrase table creation process
is implemented in two steps: phrase pairs extraction and phrase pairs scoring.
% The first of these steps is implemented within \emph{extract} program, the second
% is implemented within \emph{score} and \emph{consolidate} programs.
Both steps can be partially run in parallel and therefore we decided to run
experiments with two types of baseline: non-parallel and parallel.

% TODO: Pick only single parallel setup.
In parallel setup we run phrase table extraction by \emph{train-model.perl}
either with \verb|--parallel| option or with \verb|--cores| set to $N-1$,
where $N$ is number of cores of our machine.

% Post-filtering baseline.
Post-filtering the phrase table with significance filter can be considered as
yet another baseline, mainly useful for quality benchmarking, because
phrase table extraction with post-filtering is by design more time-consuming
than just sole phrase table extraction.

% Moses 1.0 only
For clarity of results in all our experiments we sticked to Moses release 1.0.

\section{Eppex setup}

The runtime performance of \eppex{} and the degree of pruning is determined
by the parameters of Lossy Counting.
For each phrase pair length a separate Lossy Counter instance can be set.

% all-in setup
The simplest case is to use no pruning at all. All phrase pairs will be
output and the phrase table will be equal to the one created by baseline
extraction.\footnote{Equal does not necessarily mean identical: some minor
differences in scores with floating point values might occur due to rounding
errors.}

% sigfilter-like setups
Another setup interesting to consider is the approximation of significance
filtering with $\alpha \pm \epsilon$ thresholds.
By setting \emph{support} $s$ to value such that $sN < 1$, all of the 1-1-1
phrases will be preserved (like in $\alpha - \epsilon$), while setting
\emph{support} $s$ and \emph{error} $\epsilon$ to satisfy $(s - \epsilon)N > 1$
will result in their complete removal (like in $\alpha + \epsilon$).

% frequency limits setup
To set \emph{support} and \emph{error} parameters properly, the value of $N$,
that is the number of all extracted phrase pairs, have to be known in advance.
% TODO: Finish.

\section{Data}
% Data we used in experiments.

% Czech-English
We used CzEng 1.0 parallel corpus \citep{czeng10:lrec2012} to train Czech-English
translation model.
% TODO: Update with more precise info when Obo reply.

% French-English
To confront \eppex{} with even more challenging input data (in terms of size),
we also prepared French-English translation model using data available for
Shared Machine Translation Task for 2013 Workshop on Statistical Machine
Translation (WMT~13 in short).\footurl{http://www.statmt.org/wmt13/translation-task.html}
These data consist of following parallel corpora:
\begin{itemize}
  \item Europarl v7 (Fr-En data)\footurl{http://www.statmt.org/europarl/}
  \item Common Crawl corpus
  \item United Nations corpus\footurl{http://www.euromatrixplus.net/multi-un/}
  \item News Commentary corpus
  \item $10^9$ French-English corpus
\end{itemize}
The size of our joint French-English training data is (after preprocessing described
below) 39,143,703 sentences with 1,173,485,756 French and 1,000,538,764 English tokens.
% TODO: Add information about monolingual data used.

% Czech-English (cu-bojar)
Finally, we were curious to get some insight in the rate of performance improvement
of both \eppex{} and \emph{phrase-extract} tools since their older versions that have
been evaluated in a similar manner by \citet{przywara:eppex}.
Therefore, as a third dataset we picked up the same data that have been used in their
work -- see \citet{marecek:twostep} for the exact setup of the system \emph{cu-bojar}.
This parallel corpus is the smallest from our three configurations: it contains
approximately 8.4~M sentence pairs with 93.2~M Czech and 107.2~M English tokens.
Since we are mainly interested in comparison of the runtime benchmarking figures,
we did not attempt the quality benchmarking with this dataset.

\subsection{Preprocessing}

Input data preprocessing is essential to almost any natural language processing task
and model training in SMT is no exception.

We treated our input data with following preprocessing steps:
\begin{enumerate}
  \item Cleaning of tokenization format:
  \begin{itemize}
    \item spaces at the beginning and end of the lines are removed
    \item all sequences of white space characters are replaced by a single space only
    \item all line endings are converted to Unix-style line endings
  \end{itemize}
  \item Normalization of punctuation encoding
  \item Tokenization
  \item Cleaning of any superfluous white space from tokenization -- ensures that the output contains only spaces and line-feed characters
  \item Special characters escaping:
  \begin{itemize}
    \item pipes ("\textbar") are replaced by "\&pipe;"
    \item less-than characters ("<") are replaced by "\&lt;"
    \item greater-than characters (">") are replaced by "\&gt;"
    \item ampersands ("\&") are replaced by "\&amp;"
  \end{itemize}
\end{enumerate}

We used our own tools for cleaning of tokenization format,
post-tokenization cleaning and escaping of special characters.
For normalization of punctuation encoding we used script provided by organizers of
WMT 2011 Translation Task,\footurl{http://www.statmt.org/wmt11/normalize-punctuation.perl}
while for tokenization we used \emph{tokenizer} shipped with
Moses.\footnote{Tokenization script may be found in \texttt{scripts/tokenizer/tokenizer.perl}}

The alignment tool, GIZA++, further requires that too long sentences are removed from parallel corpus.
No hard-coded limit exists, but as a rule of thumb the limit of 80 words is
recommended.\footnote{The 80 words limit is for example mentioned in "Corpus Preparation" section of Moses tutorial:
\url{http://www.statmt.org/moses/?n=moses.baseline}}
During sentence-length filtering both sides of parallel corpus must be processed simultaneously,
because the overlong sentence must be removed along with its counterpart from the other part of corpus
to keep the remaining sentences properly aligned.
Usually during this step one wants to remove also any empty lines as the same principle applies to them.

\section{Runtime benchmarking}

% What has been benchmarked?
We benchmarked phrase table construction by measuring CPU time, wall clock time
and maximum virtual memory requirements of \eppex{} and \emph{phrase-extract} tools.

% Environment.
For all experiments we used servers that are part of Sun Grid Engine cluster.
Although these machines were standard nodes in a cluster, we kept jobs of other
users away by ordering SGE to reserve all memory of the machine for our job only.
All the input and output files of benchmarked processes were read from and written
to a locally mounted hard disk, therefore network load had minimal impact on
benchmarking results.

% Software and hardware configuration.
All the servers had identical software configuration:
they were running 64-bit version of Ubuntu 10.04 server edition.
All experiments with a particular dataset were run on a random machine from
set of servers with identical hardware configuration:
\begin{itemize}
  \item "cu-bojar" configuration was run on machines with two Core4 Intel Xeon 3~GHz
  processors (4~cores in total) with 32~GB of RAM and approximately 429~GB of
  hard disk space. % tauri
  \item "cs-en" configuration was run on machines with two Core4 Intel Xeon 2.4~GHz
  processors (4~cores in total) with 48~GB of RAM and approximately 447~GB of
  hard disk space. % twister
  \item "fr-en" configuration was run on a single machine with four Core2 Intel Xeon
  2.93~GHz processors (8~cores in total) with 256~GB of RAM and approximately
  558~GB of hard disk space. % cosmos
\end{itemize}

% How the benchmarking was implemented?
% TODO: Make script available online.

% TODO: More info on complexity of training script (a lot of subprocesses).

The baseline phrase table creation managed by the training script involves
running multitude of working subprocesses to perform various intermediate
tasks like phrase pairs extraction, phrase pairs scoring, phrase table
consolidation, gzipping, wrapping the parallel execution etc.
As there is no single process to benchmark, we crafted a special Python
script to measure the runtime demands of this whole subtree of processes.
The script periodically gathers list of all subprocesses of observed process
(that is the main training script), captures the values of their CPU time
used so far and currently occupied virtual memory and updates the total CPU
time counter and virtual memory peak.
To retrieve a list of child processes a call to Unix \verb|ps| utility is
performed, CPU time and virtual memory usage of each process are read from
\verb|stat| and \verb|status| files from the respective \verb|/proc/[pid]/|
directory.
A period of one second gives a reasonable precision of benchmark, while
not impacting the performance of running processes.
The script provides a full log with benchmarking information of all detected
subprocesses, but we are mainly interested in the final summary that presents
the total wall clock time, aggregated CPU time consumed by all subprocesses
and their joint virtual memory peak.

\section{Phrase table quality benchmarking}

% BLEU score
