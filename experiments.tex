\chapter{Experiments}
\label{chap:experiments}

We conducted a series of experimental phrase table extractions in order to
assess \eppex{} memory demands, determine the ratio of speed up of the whole
process obtainable with various pruning levels and evaluate the impact of both
lossy counting and significance filtering on the quality of pruned phrase tables.
In this chapter we are going to describe the parameters of our experiments,
the employed data sets and the means of runtime benchmarking and quality
evaluation, while the actual results and comments on them are presented in
\Cref{chap:results}.

\section{Scenarios}

We use the term scenario to denote a particular toolkit (or phrase table construction process)
and the set of its configurations that we aim to evaluate.

\subsection{Baseline}

Our baseline is naturally determined by the phrase table creation process
that is implemented within 5th and 6th step of Moses training pipeline.
In the detailed description of this process in section \ref{sec:phrase-table-creation-in-moses}
we have mentioned that both steps can be partially run in parallel.
This option helps to speed up the whole process whenever training is
attempted on a machine with multiple processing units (cores or threads).
Besides the parallelization, the training script also supports a more subtle
runtime optimization: the sorting process, that is invoked to sort both halves
of phrase table in the phrase extraction step and later to sort the inverse
half before final consolidation of both halves, may be enabled to use a bigger
memory buffer, order to compress the output or to run in parallel mode.
\Tref{baseline-optimization-parameters} sums up all the parameters of Moses
training script that can be set in order to optimize the process of phrase
table creation and briefly explains their role.

\begin{table}[ht]
\centering
\begin{tabular}{ r p{86mm} }
parameter & description \\
\hline
\hline
\verb|--parallel|               & run scoring of both phrase table halves in parallel \\
\verb|--cores <num>|            & run extraction of phrases and scoring of each table half using \verb|num| threads (eg. 4) \\
\verb|--sort-buffer <memsize>|  & allow sorting process to use internal buffer of \verb|memsize| RAM (eg. 12G)\\
\verb|--sort-compress <cmd>|    & order sorting process to compress the output using \verb|cmd| program (eg. gzip) \\
\verb|--sort-parallel <num>|    & allow sorting process to run using \verb|num| threads (eg. 4, sort program must support it)\\
\hline
\hline
\end{tabular}
\caption{\label{baseline-optimization-parameters}
List of parameters to Moses training script that allows to speed up
the process of phrase table creation.}
\end{table}

To give a better picture of the performance of the baseline system with respect
to various optimization options and to get some insight into their importance,
we decided to run experiments with four different sets of optimization parameters
each being either proper subset or superset of the others (the short name in
parentheses is designated for use in result tables):
\begin{itemize}
  \item default (\emph{def-base}) -- no optimization parameters are set
  \item multi-core (\emph{multi-base}) -- multiple cores are used
  \item compressed (\emph{comp-base}) -- multiple cores are used and
    sorting process is ordered to compress its output
  \item optimized (\emph{opt-base}) -- multiple cores are used, sorting process
    is ordered to compress its output and to use more than default amount of memory
\end{itemize}

This selection by our own experience should represent the most popular ways of
invoking phrase table creation process via Moses training script and is designed to
reflect phrase table creation benchmarks that earlier popped out on Moses mailing
list.\footnote{Our selection is particularly inspired by the most recent benchmark
we are aware of: \tt{http://article.gmane.org/gmane.comp.nlp.moses.user/6749}.}
Also, by exploring the impact of available optimization options we aim to establish
as challenging baseline as possible given the computation resources of the machines
at our disposal.

% On number of cores...
In the non-default baselines we favored to turn on parallel processing using \verb|--cores|
option as this setting applies to both steps of phrase table construction, whereas
\verb|--parallel| is applicable only to scoring step.
Despite that all machines employed in our experiments have 8 or more cores (or threads),
in most of the experiments we decided to utilize 4 cores only as this is configuration
affordable not only on computational servers equipped with multiple CPUs, but also on most
of the modern laptops and we expect such results to be more apt for the Moses community.
All the time, however, we intentionally left at least one core idle to make sure our
benchmarking script or any administrative process of software managing the computational
cluster will not interfere with benchmarking.

% On sort options...
Regarding the optimization parameters for \emph{sort}, we use \emph{gzip} as compressing
program and set sort buffer size to either use the whole memory available on machine
(with some reasonable reserve for the remaining processes) or use approximately the same
amount of memory as consumed by \eppex{} when creating the full phrase table from
the same data (ie. without pruning -- the most "hungry" setting).
We did not set parallelization on for \emph{sort} -- simply because we were using older
version of \emph{sort} program that did not yet implement this feature.

% Moses 1.0 only.
For clarity and reproducibility of results we sticked to Moses release 1.0 in all our 
experiments.\footnote{Precisely the Github commit 1530ae4f5ff59cfd30f6f933e4dc05ea89cca8fc.}

\subsection{Significance filtering}

% Post-filtering baseline.
Post-filtering the phrase table with \emph{significance filter} can be considered as yet
another baseline, sensible mainly for quality benchmarking, because time requirements
will be by design above those of baseline phrase extraction.
Nevertheless, we attempted both performance and quality benchmarking of the \emph{sigfilter}
tool introduced in \Sref{sec:significance-pruning} to give the whole picture.

% Parameters of sigfiltering.
The degree of pruning done by \emph{sigfilter} is determined by the pruning
threshold for p-values (set via \verb|-l| option), by cutoff limit for forward
(direct) translation probabilities (set via \verb|-n| option) or by both.
A particularly interesting values for pruning threshold are \verb|a+e| and
\verb|a-e|: the former implies $\alpha + \epsilon$ threshold and results
in removal of all 1-1-1 phrase pairs, the latter implies $\alpha - \epsilon$
threshold and result in all 1-1-1 phrase pairs being preserved.
As for cutoff limit, in \citep{johnson:sigfilter} a limit of 30 is applied to
all experiments in order to control the width of beam search in decoding.
In Moses, the width of beam is controlled directly by the decoder, but
in the manual section on significance filtering the cutoff limit of 30
in combination with pruning threshold $\alpha + \epsilon$ is recommended as
"a good setting".

% TODO: A sum up table of various experiments and their parameters?

% Preprocessing steps.
Before \emph{sigfilter} can be run, some preprocessing is required.
The counts $C(\bar{s})$, $C(\bar{t})$ and $C(\bar{s},\bar{t})$ necessary for
population of contingency tables are established on the fly during the
pruning process by querying a suffix-array index built over the parallel
corpus used for phrase table creation.
This suffix-array index has to be produced by indexer that is part of SALM
toolkit,\footurl{http://projectile.sv.cmu.edu/research/public/tools/salm/salm.htm}
and because SALM indexer cannot process sentences longer than 254 words,\footnote{Although
SALM indexer complains about sentences being longer than 256 words, the actual limit
is 254 words.} such sentences have to be trimmed in advance.

\subsection{Eppex}

The runtime performance of \eppex{} and the degree of pruning heavily depends
on the parameters of Lossy Counting: more intensive pruning results in less phrase pairs
being extracted and then scored, thus with more pruning \eppex{} runs faster.

The earlier experiments with \eppex{} \citep{przywara:eppex} showed that the pruning
should not be too intensive: especially the removal of short phrase pairs can be
damaging to the quality of translation model.
This findings has been reflected in \eppex{} design in the possibility to instantiate
a separate Lossy Counter instance for each phrase pair length (or range of them).

% all-in setup
However, the simplest scenario to run is to use no pruning at all.
All phrase pairs will be output and the phrase table will be equal to the one
created by baseline extraction.\footnote{Equal does not necessarily mean identical:
some minor differences in scores with floating point values might occur due to
rounding errors.}
Obviously, this scenario is also the most memory demanding.

% hypothetical p=n+1 setup
Perhaps not as obviously, the scenario with \emph{positive and negative limits} set as
$p = n + 1$ will be as memory demanding as the one with no pruning.
The same cause applies in both cases, although here it might be not as striking:
when the Lossy Counting algorithm is given no space for error, it will make none,
but will consequently consume the most of space given a particular input size.
Therefore, despite this scenario is theoretically interesting, it is rather not very
usable in practice, unless one has a plenty of RAM available and wants to perform perfect
count-based pruning on the fly.

% sigfilter-like setups
A more practically interesting scenario to consider is the possibility to approximate
significance filtering with $\alpha \pm \epsilon$ thresholds with a proper setting of
\emph{limits}: by setting \emph{positive limit} to one, all of the 1-1-1 phrase pairs
will be preserved (like in $\alpha - \epsilon$), while setting \emph{negative limit} to
one will result in their complete removal (like in $\alpha + \epsilon$).
It is important to realize, that it is really just an approximation:
with the former setting, all phrase pairs will be actually preserved,
and with the latter all single occurring phrase pairs will be actually removed,
not only 1-1-1 phrase pairs.
On the other hand, the option to set a separate Lossy Counting allows us to come up with
a mix of both approaches: define a less intensive $\alpha - \epsilon$-like pruning for
shorter phrase pairs and a more intensive $\alpha + \epsilon$-like pruning for longer
phrase pairs.

% frequency-range rising limits setup
To sum up this reflection on various \eppex{} scenarios, the main reasoning that should
determine the settings of \emph{limits} is the observation that short phrases pairs are
more crucial to the overall translation quality of phrase-based models than longer ones
and therefore they have to be pruned with more caution.
This phenomenon has been observed also by \citet[p. 7]{zens:systcomp} when examining the
properties of output produced by the various pruning methods. In case of methods that
performed well in their evaluations, they found out that the more aggressive the pruning,
the larger the percentage of short phrases.

Finally, an important note to make is that for all experiments with \eppex{} we have used
the version 2.5.1\footnote{Precisely the Github commit 7052adc50c46f74d36175632977f6b93c2e4f931.}
compiled with \verb|--with-hashtables| option.\footnote{Compilation options are explained in
\Aref{chap:installation}.}

\section{Datasets}
% Data we used in experiments.

The thesis assignment specifically dictated that a massive parallel dataset has to be
employed in the evaluation of the epochal extraction performance.
For this reason, we picked up data available as part of Shared Machine Translation Task
for 2013 Workshop on Statistical Machine Translation (or WMT~13 in short) that presents
the de facto standard in the area of MT systems evaluation.\footurl{http://www.statmt.org/wmt13/translation-task.html}

\subsection{Cs-En setup}
% Czech-English

The Czech-English setup was based on data and models prepared as a part of a broader
effort to deliver a competitive system for WMT~13 Translation Task.\footnote{The author
took only minor part in this effort, but was kindly allowed to make use of the results
including the baseline system and all data not directly produced by or depending on
the tools evaluated in this work.}
%The description of this setup is therefore a rather brief as only part of the system
%has been built directly by the author.

As a training data for construction of the translation model we put together CzEng 1.0
parallel corpus\footurl{http://ufal.mff.cuni.cz/czeng/czeng10/} \citep{czeng10:lrec2012}
and Czech-English section of Europarl v7 corpus\footurl{http://www.statmt.org/europarl/}.
The joined data have 15,478,910 parallel sentences with 220,173,420 Czech and 253,283,160
English tokens.

The word-alignments were established on a lemmatized version of the combined corpus using
the recently published \emph{fast-align} tool \citep{dyer:fastalign}.

The translation model was created with two factors on both sides of the corpus:
\begin{enumerate}
  \item token with case determined by supervised truecaser
  \item tag assigned by state-of-art morphological tagger \emph{Morƒçe}\footurl{http://ufal.mff.cuni.cz/morce/}
\end{enumerate}

The translation model defined only single decoding step: two factors on the source side
were directly translated to two factors on the target side.\footnote{To say it in the language
of Moses command line: translation factors were set as \texttt{0,1-0,1}.}

Three language models were used:
\begin{enumerate}
  \item an interpolated language model of order 6 for the first factor built from News Crawl
    articles from years 2007-2011 treated with supervised truecaser and with perplexity tuned
    on target side of \emph{news-test2010} dataset
  \item an interpolated language model of order 8 for the second factor built from (and tuned
    on) the same data as model for the first factor
  \item a language model of order 6 for the first factor built over target side of CzEng 1.0
    treated with supervised truecaser
\end{enumerate}

We did not train lexicalized reordering model and instead relied only on the built-in
distance-based distortion model.

The weights of all the components were tuned using standard MERT implementation
that is part of Moses. % TODO: More info (or link or citation) on MERT?
As the development data for tuning, the \emph{news-test2011} dataset were used.

The BLEU score was evaluated using \texttt{evaluator} tool that is also part of Moses\footnote{The \texttt{evaluator}
tool is implemented in file \texttt{<moses>/mert/evaluator.cpp}} with a bootstrap value of 1000.
As the testing data, the \emph{news-test2012} and \emph{news-test2013} datasets were used.

\subsection{Fr-En setup}
% French-English

To confront \eppex{} with even more challenging input data (in terms of their size),
we built and evaluated French-English MT system, as significantly more French-English parallel
corpora were available as part of WMT~13 Translation Task. Concretely, the training data for
the French-English setup were gathered from the following parallel corpora:
\begin{itemize}
  \item Europarl v7 (Fr-En data)\footurl{http://www.statmt.org/europarl/}
  \item Common Crawl corpus
  \item United Nations corpus\footurl{http://www.euromatrixplus.net/multi-un/}
  \item News Commentary corpus
  \item $10^9$ French-English corpus
\end{itemize}
The size of our joint French-English training data is (after preprocessing described
at the end of the section) 39,143,703 sentences with 1,173,485,756 French and 1,000,538,764
English tokens.

GIZA++ \citep{och:giza}, the recommended aligner for Moses,\footurl{http://www.statmt.org/moses/?n=FactoredTraining.RunGIZA}
was used to establish the word alignments over the lowercased corpus.
The lowercased forms of tokens were also the single factor for both sides of
the translation model.
As a language model, only the target side of parallel corpora was used.
As in the "Cs-En" setup, no lexicalized reordering model was built and reordering was
determined only by built-in distortion model.

Weights tuning and evaluation of translation quality were done with the same methods
(Moses implementation of MERT and \texttt{evaluator} tool) and the same datasets
(\emph{news-test2011} for tuning, \emph{news-test2012} and \emph{news-test2013} for
evaluation) as in case of "Cs-En" setup. We did not, however, truecase the data before
evaluation in this setup, instead we used a lowercased version of test data.

\subsubsection*{Preprocessing}

Input data preprocessing is essential to almost any natural language processing task
and translation model training in phrase-based SMT is no exception.

We treated our "Fr-En" data with the following preprocessing steps:
\begin{enumerate}
  \item Cleaning of tokenization format:
  \begin{itemize}
    \item spaces at the beginning and end of the lines are removed
    \item all sequences of white space characters are replaced by a single space only
    \item all line endings are converted to Unix-style line endings
  \end{itemize}
  \item Normalization of punctuation encoding
  \item Tokenization
  \item Cleaning of any superfluous white space from tokenization -- ensures that the output contains only spaces and line-feed characters
  \item Special characters escaping (necessary in case of Moses):
  \begin{itemize}
    \item pipes ("\textbar") are replaced by "\&pipe;"
    \item less-than characters ("<") are replaced by "\&lt;"
    \item greater-than characters (">") are replaced by "\&gt;"
    \item ampersands ("\&") are replaced by "\&amp;"
  \end{itemize}
\end{enumerate}

We used our own tools for cleaning of tokenization format, post-tokenization cleaning
and escaping of special characters.
For the normalization of punctuation encoding we used script provided by organizers of
WMT~11 Translation Task,\footurl{http://www.statmt.org/wmt11/normalize-punctuation.perl}
while for tokenization we used \emph{tokenizer} shipped with
Moses.\footnote{Tokenization script may be found in \texttt{<moses>/scripts/tokenizer/tokenizer.perl}}

The alignment tool we employed, GIZA++, further requires that overly long sentences are
removed from parallel corpus.
No hard-coded limit exists, but as a rule of thumb the limit of 80 words is
recommended.\footnote{The 80 words limit is for example mentioned in "Corpus Preparation"
section of Moses tutorial: \url{http://www.statmt.org/moses/?n=moses.baseline}}
During sentence-length filtering both sides of parallel corpus must be
processed simultaneously, because each overlong sentence must be removed along
with its counterpart from the other part of corpus to keep the remaining sentences
properly aligned.
Usually during this step one wants to remove also any empty lines as the same principle
applies to them.

\subsection{The 2011 setup}
% Czech-English (cu-bojar)
Finally, we were curious to get some insight in the rate of performance improvement
of both \eppex{} and \emph{phrase-extract} tools since their older versions that have
been evaluated in a similar manner by \citet{przywara:eppex}.
Therefore, as a third dataset we picked up the same data that have been used in their
work -- see \citet{marecek:twostep} for the exact setup of the system \emph{cu-bojar}.
This parallel corpus is the smallest from our three configurations: it contains
approximately 8.4~M sentence pairs with 93.2~M Czech and 107.2~M English tokens.

Since we are mainly interested in comparison of the runtime benchmarking figures,
we did not attempt the quality benchmarking with this dataset.
Also, we omit the significance filtering setup, as \emph{sigfilter} did not change
since the version tested in 2011.

\section{Runtime benchmarking}

% What has been benchmarked?
We benchmarked phrase table construction by measuring CPU time, wall clock time,
virtual memory usage peak and disk usage peak both of \eppex{} and standard Moses
pipeline.

% Environment.
For all experiments we used servers that are part of Sun Grid Engine cluster.
Although these machines were standard nodes in a cluster, we kept jobs of other
users away by ordering SGE to reserve all memory of the machine for our job only.
All the input and output files of benchmarked processes were read from and written
to a locally mounted hard disk, therefore network load had no impact on
benchmarking results.

% Software and hardware configuration.
All the servers had identical software configuration:
they were running 64-bit version of Ubuntu 10.04 server edition.
For each dataset we use a server or set of servers with hardware configuration
that satisfies memory and disk demands incurred by the size of particular dataset:
\begin{itemize}
  \item "cu-bojar" configuration was run on a machine with two Quad-Core
  AMD Opteron\texttrademark{} 2387 processors (each with 4~cores capable of working
  with clock speed of 2.8~GHz; 8~threads in total),
  32~GB of RAM and approximately 429~GB of hard disk space. % andromeda7
  This is the same machine that was employed in 2011 experiments.
  \item "cs-en" configuration was run on several machines with identical hardware
  configuration: two Intel\textregistered{} Xeon\textregistered{} E5620
  processors\footurl{http://ark.intel.com/products/47925/Intel-Xeon-Processor-E5620}
  (each with 4 hyper-threaded cores with clock speed of 2.4~GHz; 16 threads in total),
  128~GB of RAM and approximately 558~GB of hard disk space. % lucifer[6-9]
  \item "fr-en" configuration was run on a machine with two Intel\textregistered{}
  Xeon\textregistered{} E7520
  processors\footurl{http://ark.intel.com/products/46490/Intel-Xeon-Processor-E7520}
  (each with 4 hyper-threaded cores with clock speed of 1.866~GHz; 16 threads in total),
  512~GB of RAM and approximately 1.7~TB of hard disk space. % iridium
\end{itemize}

% How the benchmarking was implemented?
% TODO: Make script available online.

% On wall clock, CPU time and VM peak benchmarking.
The baseline phrase table creation managed by the training script involves
running multitude of working subprocesses to perform various intermediate
tasks like phrase pairs extraction, phrase pairs scoring, phrase table
consolidation, gzipping, wrapping the parallel execution etc.
To measure the overall runtime demands of this dynamically changing tree
of processes, we crafted a special Python script:
it periodically gathers list consisting of the main observed
process\footnote{The main training script \texttt{train-model.perl} in our case.}
and all its subprocesses, captures the values of their CPU time used so far
and currently occupied virtual memory and updates the total CPU time counter
and virtual memory peak.
To retrieve a list of child processes a call to Unix \verb|ps| utility is
performed, CPU time and virtual memory usage of each process are read from
\verb|stat| and \verb|status| files from the respective \verb|/proc/[pid]/|
directory.
A period of one second gives a reasonable precision of benchmark, while
not damaging the performance of running processes.
The script provides a full log with benchmarking information of all detected
subprocesses, but we are mainly interested in the final summary that presents
the total wall clock time, aggregated CPU time consumed by all subprocesses
and their joint virtual memory peak.

% On disk usage peak measurement.
The disk usage have been measured by a separate bash script that invoked Unix
\texttt{du} command with summarizing option on the working directory with
a period of 1 second and updated relevant log file with the full output of
\texttt{du} whenever new peak was reached.
In case of \eppex{} experiments the peak disk usage is always equal to the
size of phrase table file as there are no temporary files produced and we
ignored the disk usage incurred by input files.
