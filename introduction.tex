\chapter{Introduction}
\label{chap:introduction}

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{This page is in Czech. Would you like to translate it?}{--- Google}

% A catchy intro: too informal for thesis?
It would make for a nice brain-teasing question, of which job interviewers
at Google are famous,\footnote{For the actual tricky questions and puzzles
asked by Google interviewers I recommend reading the book "Are You Smart
Enough to Work at Google?" by William Poundstone.} to ask:
"How many bilingual elves would Google had to employ in order to translate
all the words, sentences and web pages that are send to Google Translate
service each day?"\footurl{http://translate.google.com}
Obviously, it would be a hypothetical one. No bilingual elves are involved
in translation of the bunch of texts sent to Google Translate each day.
And no human translators either: Google Translate is fully implemented using
computer software only.

% What is MT?
The approach Google Translate uses to deliver translations is described as
\emph{machine translation} (MT) and more formally can be defined as a design
or development of computer software capable of fully automated translation
of text or speech from one natural language to another.
The words "fully automated" are important here as machine translation should
not be confused with approaches that only aims to design computer programs
helping human translators to work more efficiently: these are typically
referred to as computer-aided translation (CAT) systems.

% A brief look into history.
The first attempts to use computers for human language translation date
back to the early ages of computing. \citet{weaver:memorandum} in his famous
memorandum suggested:
\begin{quote}
(...) the possibility of contributing at least
something to the solution of the world-wide translation problem through the use
of electronic computers of great capacity, flexibility, and speed.
\end{quote}
The outset of the MT field has been marked with immense optimism, leading
to prognosis that machine translation would be a solved problem within
a decade. % TODO: Cite?
This quickly proved to be unrealistic assumption and even nowadays
there is still no system that provides the holy grail of MT:
\emph{fully automatic high quality translation of unrestricted text}.

% Introduce SMT. TODO: Slightly inspired by Preface of Koehn's book.
However, the last 25 years have been very prolific for the field.
A new paradigm arose, not only in MT but in natural language processing
field in general, that employs automatic discovery of principles that
rules human languages (and translation between them) by collecting
statistics over the data rather than explicit definition of such rules
by human experts.
In \emph{statistical machine translation} (SMT) these statistics are
collected by pairing the input and output side of the translation process.
Co-occurrences of \emph{atomic units of translation} are used to
evaluate properties of statistical model that is afterwards used to
search for the most probable translation given the input text.
The atomic units of translation are typically words or phrases and
the respective systems are referred to as \emph{word-based} or
\emph{phrase-based}.\footnote{In SMT context the term phrase is almost
exclusively used to refer to any short sequence of words and wears no
implicit linguistic notion.}

% Introduce phrase table.
Phrase-based systems internally utilize a table consisting of pairs of
phrases, one being the phrase from the source language and the other
being the phrase from the target language, and various scores assigned to
these pairs by the statistical model.
This table acts as a dictionary, listing all possible translations of phrases
in one language into phrases in another language along with indication of
quality of such translation expressed by the scores, and is usually referred
to as \emph{phrase translation table} or just \emph{phrase table}.
Any reasonable metric can be used as a phrase table score, but a de facto
standard is to use \emph{maximum likelihood probability} of the target language
phrase given the source language phrase and vice versa.

% Sketch the problem (partially copy-pasted from eppex paper).
To estimate maximum likelihood probabilities, \emph{frequency counts} of source
phrases, target phrases and all their co-occurrences must be collected from
the entire parallel corpus.
For substantial coverage of source and target languages, such corpus is often
very big and in consequence all phrase pairs and their counts cannot fit in
the physical memory of the computer.
To overcome this limitation, phrase table construction methods often simply
dump observed phrases to local disk and sort and count them on disk.
This approach allows to construct phrase tables of size limited only by
the capacity of the disk, with an obvious drawback that much more time is
needed to build the table.

% Underline the urgency of the problem.
In recent years the amount of parallel data available increased significantly,
but the more data are exploited in the process of MT system training,
the more computational resources are consumed by the phrase table component.
The growing phrase tables not only make translation models expensive to store and process,
but can even pose a challenging problem for further utilization of the system
(eg. in case of handheld devices or in otherwise constrained environments).

% Introduce the idea of phrase table pruning.
This arising problem attracted attention of MT community and several methods
aiming at \emph{pruning of phrase tables} have been demonstrated.
The design of such methods typically allows to control the degree of pruning by setting
a threshold value that either ensures removal of certain bottom-ratio of phrase pairs
sorted by values given by the internal pruning criterion or directly sets
a maximum/minimum limit for the positive values with respect to this criterion.
Moreover, to prove their usefulness, the method designers also provide theoretically
or empirically grounded means of how to set the threshold in order to remove
a significant portion of phrase table and in the same time retain the overall quality
of the underlying translation model.

% A systematic comparison of phrase table pruning techniques.
A recent work by \citet{zens:systcomp} presented a systematic description of existing
phrase table pruning techniques and also stated desiderata for a good phrase table pruning
criterion:
\begin{itemize}
  \item \emph{Soundness} -- the criterion should optimize some established
    information-theoretic measure of translation model quality.
  \item \emph{Efficiency} -- pruning should be fast (linear in the size of the phrase table).
  \item \emph{Self-containedness} -- pruning should use only information contained in the model
  (the phrase table) itself.
  \item \emph{Good empirical behavior} -- the criterion should be capable of pruning large parts
    of the phrase table without significant loss in translation quality.
\end{itemize}

These desiderata also served as a motivation for a proposal of a novel pruning method introduced
in their work: \emph{relative entropy pruning} reportedly outperforms all the existing methods and 
achieves consistently high savings between 85\% and 95\% of the phrase table with a negligible loss
of translation quality as measured by BLEU score \citep{papineni:bleu}.

From the perspective of this work it is worth to note that all the established pruning methods
mentioned by \citet{zens:systcomp} operates on complete phrase tables, i.e. no method attempts
to prune phrase pairs immediately in the process of their extraction from parallel corpus.

% TODO: Mention phrase table compacting?

\section{Aim of this work}

% Introduce our solution.
In this work, we examine capability of an algorithm that delivers approximate
frequency counts over stream of input items \citep{manku:lossycounting} to
work as on-the-fly filter applied to the phrase pairs extraction,
essentially speeding up the whole process and eliminating the need for
any post-filtering of created phrase table.
This approach has already been demonstrated as applicable by \citet{przywara:eppex}
and this thesis is a direct follow-up of that effort.

% What's our goal that should be confronted against in conclusions?
The ultimate goal of this work is to implement a software tool that performs
the filtrated phrase table construction using aforementioned algorithm.
A successful implementation should allow to process parallel corpora of
significant sizes (tens of millions of sentences) with memory demands manageable
by physical memories available on present computation servers (tens of GBs).
Beside the implementation and its detailed description, the crucial part of
this work also consists of a careful examination of the impact that various
settings of the algorithm imply on memory vs. time and translation quality
trade-offs when compared to current state-of-art methods of phrase table
construction and pruning.

As a state-of-art SMT system to set our baseline, we decided to choose \emph{Moses}
\citep{koehn:moses}: an open-source toolkit with rich documentation and active
community of researchers and developers.\footurl{http://www.statmt.org/moses/}

\section{Thesis outline}

We start with more detailed introduction to the phrase-based SMT,
carefully describe the process of phrase translation table construction
and mention some of the existing pruning methods and their available
implementations.

In Chapter 3 we introduce the algorithm that is the basis of our implementation
of on-the-fly filtration and show the properties of the output produced
by the algorithm that make it particularly applicable for phrase table pruning.

Chapter 4 is devoted to in-depth description of implementation details of our
phrase table extraction tool, \emph{epochal extractor} (or shortly \eppex{}).
Notably, various memory-management optimizations are mentioned.

To assess \eppex{} usability in real world applications, we carried out a set
of carefully crafted experiments aiming at comparison of resources usage as well as
the ultimate translation quality of \eppex{} and some of the methods described
in Chapter 2.
Detailed design of experiments is subject of Chapter 5,
while the results are discussed in Chapter 6.

In the final chapter we comment on our results and provide a conclusion of
what have been done and what can be done in the future work on this topic.

In Appendix A we give the instructions on how to get \eppex{} working on Linux machines.

In Appendix B all the program options of \eppex{} are explained with examples of their usage.
