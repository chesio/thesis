% In this chapter:
% - introduction of phrase based SMT
% - description of phrase table
% - description of phrase table creation in Moses
% -- with alternatives: memscore
% - phrase table post filtering
% -- sigfilter
% -- relent filter
% - alternatives to precomputed phrase table
% -- dynamic suffix arrays
% - mention phrase table compression tools

\chapter{Phrase tables in phrase-based Statistical Machine Translation}
\label{chap:phrase-based}

% Briefly introduce the concept of phrase-based SMT
% (SMT book, Chapter 5)

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{The currently best performing statistical machine translation systems are based on
phrase-based models: models that translate small word sequences at a time.}{--- Philipp Koehn, Statistical Machine Translation}

Phrase-based models are nowadays considered the most prominent approach
in the field of Statistical Machine Translation.
In this chapter, we are going to describe the concept of phrase-based models with a particular stress
on the description of phrase translation tables and methods for their construction,
but our introduction will be, inevitably, very condensed.
For a more detailed insight into phrase-based models and statistical machine
translation in general, we gently point our reader to \citet{koehn:smt}.

\section{Phrase-based Statistical Machine Translation}

Before phrase-based models were introduced, still simpler word-based models were
the center of interest of MT researchers.
\emph{Word-based models} use a lexicon of words with their translations to produce
a translation of the full text by proceeding one word at a time.
\emph{Phrase-based models} originate from this concept, but instead of using only
a single word as the unit of translation they allow for a sequence of words -- \emph{a phrase}
-- to be translated at once.
This seemingly trivial extension makes for a non-trivial improvement with some
imminent benefits:
\begin{enumerate}
  \item By using phrases rather than words, this model is now implicitly able to deal
    with non-compositional phrases
  \item A phrase-based model is able to handle the local reordering of words, for example:
    switching from \emph{the noun follows adjective} to \emph{the adjective follows noun} order.
  \item Word context on the source side now plays a more important role, because it can be kept
    within the phrase pair and lead to an implicit disambiguation during translation.
  \item With a lot of data, the model can be trained to cover long phrases or even
    whole sentences.
\end{enumerate}

\subsection{Mathematical background}

The phrase translation model is based on a noisy channel model \citep{koehn:spbt}.
By using Bayes rule, the translation probability for translating a foreign sentence $f$
into English as $e_{best}$ can be reformulated as:
\begin{equation}
  e_{best} = argmax_{e}~p(e|f) =  argmax_{e}~p(f|e)p(e)
\end{equation}

This allows for a separate translation model $p(f|e)$ and language model $p(e)$.
The translation model is further decomposed into:
\begin{equation} \label{eq:tm}
  p(\bar{e}_1^I|\bar{f}_1^I) = \prod_{i=1}^I \phi(\bar{f}_i|\bar{e}_i) d(start_i - end_{i-1} - 1)
\end{equation}

The foreign sentence $f$ is broken up into $I$ phrases $\bar{f}_i$, each of them
being translated into an English phrase $\bar{e}_i$. Since by the use of the noisy channel
model we have inverted the direction of translation, the phrase translation probability
$\phi(\bar{f}_i|\bar{e}_i)$ is modeled as a translation from English to the foreign language.

The reordering of English phrases is handled by a \emph{reordering model}, in \Eref{eq:tm}
denoted by $d(start_i - end_{i-1} - 1)$.
The most used reordering model is \emph{distance-based} and assigns a reordering cost based
on the distance to the position of the first word of the foreign input phrase that translates to
the $i$-th English phrase (noted as $start_i$) and the position of the last word of the foreign
phrase that translates to the preceding English phrase (noted as $end_{i-1}$).
In the case of two phrases translated in sequence, $start_i = end_{i-1} +1$, the
reordering model assigns a cost of $d(0)$.
The probability distribution of $d$ can either be computed from the data or set by applying
an exponentially decaying cost function $d(x) = \alpha^{|x|}$ with a proper value of $\alpha$
\citep{koehn:smt}.

By putting together a decomposed translation model and a language model (we assume standard $n$-gram
language model here), we get the mathematical definition of the
\emph{phrase-based statistical machine translation model}:
\begin{equation} \label{eq:pbsmt}
  e_{best} = argmax_e \prod_{i=1}^I \phi(\bar{f}_i|\bar{e}_i) d(start_i - end_{i-1} - 1) \prod_{i=1}^{|e|} p_{LM}(e_i|e_{i-n+1}...e_{i-1})
\end{equation}

The process of searching for the best translation $e_{best}$ according to the definition of
the translation model above is formally called \emph{decoding} and is a hard problem,
because there is an exponential number of choices given a particular input sentence.
The software that implements decoding, simply called \emph{decoder}, therefore has to employ
several strategies to limit the search space in order to speed up the decoding process.
One of such strategies is, when a particular source phrase $\bar{f}_i$ is examined,
to limit the search to only $K$ target phrases $\bar{e}_i$ that rank best
with respect to the values of $\phi(\bar{f}_i|\bar{e}_i)$.

Putting mathematical definitions aside, the role of all three components in the search for
the best translation (decoding) can be explained in a more intuitive way:
\begin{itemize}
  \item The phrase translation model provides likely translations for each phrase.
  \item The reordering model ensures the phrases are reordered appropriately.
  \item The language model ensures the output is fluent English.
\end{itemize}

\subsection{Log-linear model}

So far we have introduced three components of an SMT system -- a phrase translation model,
a reordering model and a language model -- and described them in terms of the probability
distributions that determine them.
Then, we laid out the definition of a phrase-based model as the multiplication of these
three distributions.
Despite the elegance of such a definition, it has its drawbacks and current SMT
systems tend to use a more flexible framework to combine all the various
components: the \emph{log-linear model}.
It has the general form:
\begin{equation}
  exp \Big( \sum_i^n \lambda_i h_i(x_i) \Big)
\end{equation}

The $h_i(x)$ represents quantities that are functions of variables $x_i$,
while $\lambda_i$ stands for the model parameters.

The original definition of a phrase-based SMT model (\Eref{eq:pbsmt}) can be reformulated to
fit a log-linear model by defining $h_1 = log(\phi)$, $h_2 = log(d)$ and $h_3 = log(p_{LM})$
and introducing the corresponding model parameters $\lambda_{\phi}$, $\lambda_{d}$ and $\lambda_{LM}$:
\begin{equation}
  \begin{aligned}
  p(e|f) = exp \Bigg[ &\lambda_{\phi} \sum_{i=1}^I log \, \phi(\bar{f}_i|\bar{e}_i) \\
       &+ \lambda_{d} \sum_{i=1}^{I} log \, d(start_i - end_{i-1} - 1) \\
       &+ \lambda_{LM} \sum_{i=1}^{|e|} log \, p_{LM}(e_i|e_{i-n+1}...e_{i-1}) \Bigg] \\
  \end{aligned}
\end{equation}

The log-linear model has some advantages over the probabilistic formulation.
First, as it is not build upon probabilistic reasoning, it does not require its
components to be based on valid probability distributions.
This way new features can be easily incorporated into the model, for example direct
phrase translation probability $\phi(e|f)$\footnote{Recall, that the phrase translation
probability model $\phi(\bar{f}|\bar{e})$ is inverted due to the use of noisy channel model.}.
Second, the $\lambda_i$ parameters can be used to put more weight on some of the components,
for example accenting the language model more if the translation is good, but the output
lacks fluency etc.

\section{Phrase translation table}
\label{sec:phrase-translation-table}
% What's the phrase table?

% It is hard to find better words to introduce section on phrase translation tables
% than \citet[p.130]{koehn:smt} did:
% \begin{quote}
%  Clearly, the power of phrase-based translation rests on a good phrase
%  translation table.
% \end{quote}

In \Cref{chap:introduction} we already gave a brief definition of a phrase
translation table. Let us repeat it here more formally, adhering
to the notation introduced earlier in this chapter.

\emph{Phrase translation tables} in SMT systems generally take the form of a list of
pairs of phrases $\bar{f}$ and $\bar{e}$, $\bar{f}$ being the phrase from the source
language and $\bar{e}$ being the phrase from the target language, along with scores
that should reflect the quality of translation $\bar{f}$ by $\bar{e}$.
The standard approach to obtaining such scores is to estimate the \emph{maximum likelihood
probability} of the phrase $\bar{e}$ given the phrase $\bar{f}$ and vice versa.
The probabilities $\phi(\bar{e}|\bar{f})$ and $\phi(\bar{f}|\bar{e})$ are often
referred to as \emph{direct} and \emph{inverse} (or \emph{forward} and \emph{backward})
\emph{phrase translation probabilities}.

To estimate maximum likelihood probabilities, the following frequency counts
of phrase pairs extracted from the parallel corpus must be collected:
\begin{itemize}
  \item $C(\bar{f},\bar{e})$ -- number of occurrences of the phrase pair $(\bar{f},\bar{e})$
  \item $C(\bar{f})$ -- number of occurrences of phrase pairs with phrase $\bar{f}$ on the source side:
    $C(\bar{f}) = \sum_{e}{C(\bar{f},e)}$
  \item $C(\bar{e})$ -- number of occurrences of phrase pairs with phrase $\bar{e}$ on the target side:
    $C(\bar{e}) = \sum_{f}{C(f,\bar{e})}$
\end{itemize}

The maximum likelihood probabilities $\phi(\bar{e}|\bar{f})$ and
$\phi(\bar{f}|\bar{e})$ are then calculated from these counts as follows:
\begin{equation}
  \phi(\bar{e}|\bar{f}) = \frac{C(\bar{f},\bar{e})}{C(\bar{f})}
  ~~~~~~~~~~ % Maybe not the cleanest, but definitely working solution :)
  \phi(\bar{f}|\bar{e}) = \frac{C(\bar{f},\bar{e})}{C(\bar{e})}
\end{equation}

\Tref{phrase-table-europarl} presents an extract from the actual phrase table
that was constructed from Europarl v6 Czech-English corpus.\footurl{http://www.statmt.org/europarl/archives.html}
The extract lists some of the translation options of the English phrase "Europe" and
indicates that there is more than 1000 such options in the full phrase table.

One of the reasons behind the high number of translation options of a particular
source phrase is the way phrase pairs extraction algorithm works:
in simple words, it basically loops over all possible target language phrases
and find the minimal source phrase that matches each of them with respect to
constraints imposed by word alignment points, while also accounting for
unaligned words.
Unaligned words are in fact often responsible for additional phrase pairs
being extracted: if the minimal source phrase borders unaligned words,
then it is extended to these words and the extended phrase is also extracted.
On the other hand, no phrase pair can be extracted if there are only unaligned
words, therefore the observation that more unaligned words lead to more
extracted phrase pairs is not valid in the extreme case.
In theory, the number of extracted phrase pairs is roughly quadratic to the number
of words in a sentence, therefore in practice often a maximum phrase length limit
is enforced \citep[Chapter 5.2]{koehn:smt}.

\Fref{fig:phrase-pair-alignment} presents a snippet picked from a French-English
phrase table with two phrase pairs and their alignment information.
In the first example, the French words "de" and "liberté" are unaligned,
the punctuation mark ";" in the French phrase is aligned to the same mark in the English phrase
and the French "et" is aligned to the English "and".
In the second example, all the words in both phrases are aligned:
the French word "de" is aligned to the English word "of", the word "liberté" to "liberty"
and both words "a" and "été" are aligned to the single word "was".

\begin{figure}[ht]
\begin{verbatim}
  de liberté ; et ||| ; and ||| 2-0 3-1
  ...
  de liberté a été ||| of liberty was ||| 0-0 1-1 2-2 3-2
\end{verbatim}
\caption{\label{fig:phrase-pair-alignment}
An example of phrase pairs with word alignment information.}
\end{figure}

% An example of actual phrase table.
\begin{table}[ht]
\centering
\begin{tabular}{ l l l l }
No. & English ($\bar{f}$) & Czech ($\bar{e}$) & $\phi(\bar{e}|\bar{f})$ \\
\hline
\hline
1. & europe & evropě & 0.325794 \\
2. & europe & evropa & 0.244393 \\
3. & europe & evropu & 0.121596 \\
4. & europe & evropy & 0.114969 \\
5. & europe & evropou & 0.0204101 \\
6. & europe & evropě , & 0.0174326 \\
7. & europe & evropské & 0.0144552 \\
8. & europe & evropských & 0.0139269 \\
9. & europe & evropu , & 0.012054 \\
10. & europe & evropy , & 0.00840417 \\
... & ... & ... & ... \\
52. & europe & na evropu & 0.000288143 \\
53. & europe & ) evropa & 0.000240119 \\
... & ... & ... & ... \\
56. & europe & evropě v & 0.000240119 \\
57. & europe & , evropu & 0.000192095 \\
... & ... & ... & ... \\
75. & europe & unie & 0.000192095 \\
76. & europe & , která evropa & 0.000144071 \\
... & ... & ... & ... \\
94. & europe & v evropě , & 0.000144071 \\
95. & europe & " evropa & 9.60476e-05 \\
... & ... & ... & ... \\
160. & europe & že by evropa & 9.60476e-05 \\
161. & europe & " evropa ( & 4.80238e-05 \\
... & ... & ... & ... \\
1010. & europe & životě v evropě & 4.80238e-05 \\
\hline
\hline
\end{tabular}
\caption{\label{phrase-table-europarl}
An extract from an actual phrase table listing some of the possible translations of English
word "Europe" into Czech sorted by their maximum likelihood probability.
The full table contained more than 26 millions phrase pairs and has been extracted
from the Europarl v6 Czech-English corpus.}
\end{table}

\subsection{Phrase table creation in Moses}
\label{sec:phrase-table-creation-in-moses}
% What's the state-of-art implementation of phrase table extraction in Moses?
% Introduce train-model.perl and steps of training pipeline.

It has already been said in the introduction that phrase table construction methods
usually store all the temporary data on disk, because the amount of extracted
phrase pairs is often too big to be fit in the physical memory of the computer.
The phrase table construction process in \emph{Moses} also adheres to this concept.
In the following text, we describe this process in great detail to give our reader
a clear picture of its quite complex layout and all the steps and substeps involved.

Moses release 1.0\footurl{http://www.statmt.org/moses/?n=Moses.Releases} comes
with a training script that incorporates all the steps involved in the creation of
ready-to-go translation systems from the parallel corpus, including phrase table creation.
In the \emph{train-model.perl} script, this whole training process is covered in
9 consecutive steps\footurl{http://www.statmt.org/moses/?n=FactoredTraining.HomePage},
including word alignment, lexical table construction, phrase table construction and more.

The modular step-by-step design of the training process makes it very open to
alternating implementations of particular steps.
The training script interface explicitly supports use of third party tools by
allowing only a subset of the nine steps to be executed.
The only (obvious) requirement is that such tools have to be capable of reading and
writing data in a format that is compatible with the preceding and subsequent steps
of the pipeline.

In this work we are exclusively concerned with the fifth and sixth step of
the training pipeline:
phrase pair extraction\footurl{http://www.statmt.org/moses/?n=FactoredTraining.ExtractPhrases}
and phrase pair scoring\footurl{http://www.statmt.org/moses/?n=FactoredTraining.ScorePhrases}.
Conceptually, these two steps can be considered a single step only, as their ultimate
goal is to construct a phrase table given a parallel corpus and word alignment.
The reason why phrase table construction is split into these two steps is that
for large input data the table simply does not fit into computer memory.
To overcome this limitation the core implementation uses disk space as a temporary storage
for extracted phrases and one of the effects of such a design is the separation of
phrase extraction and phrase scoring phases into individual steps.
Another reason for this break-up is the extraction of reordering information
of individual phrase pairs -- this information is later used in step 7 in the creation
of the lexicalized reordering model.

The whole training pipeline aims to utilize the multi-core processor architecture that
prevails in the latest computers as much as possible. To turn on parallel processing,
two parameters exist and they both apply also to the steps we are interested in:
\begin{itemize}
  \item \verb|--parallel|: A boolean switch that enables concurrent runnning of such parts of
  the pipeline that perform the same task twice: once for the source and once for the target language.
  With this switch on, both parts are processed in parallel.
  \item \verb|--cores N|: An unsigned integer that simply sets the number of CPU threads
  that may be utilized during training. In Moses 1.0 only phrase extraction and phrase
  scoring steps make use of this setting.
\end{itemize}
Despite doing very similar jobs, both options apply to different parts
of the implementation, so they can be combined.
However, even if none of these options is specified, some parts of the pipeline
are still implicitly parallelized.

Let us now describe the phrase extraction and phrase scoring steps in greater detail.

The \emph{phrase extraction phase} starts with optional splitting of all input files
(alignment information, both parts of the corpus and an optional weighting file) into $N$
parts, where $N$ is the number of threads set by \verb|--cores|.
Then, the actual extraction of phrases is done by running $N$ extractors in parallel,
each processing its own split.
Each extractor produces the following set of files: a direct phrase pairs table, an inverse
phrase pairs table and an optional reordering information table.
In the final stage, all output files of the same type from all $N$ extractors are
merged together, sorted and dumped into one direct and one inverse phrase table
and one reordering information table.

In \emph{phrase scoring phase} first both direct and inverse phrase tables are
populated with scores and then both are consolidated into the final phrase table.
The scoring for both parts can be processed in parallel if the \verb|--parallel|
switch is specified.
Further parallelization is done within the scoring process itself.
First, similar to extraction, the input is split into $N$ chunks.
However, this time it is important that all phrase pairs with the same source
phrase are kept together in the same chunk (and vice versa for target phrases
in the case of inverse phrase table).\footnote{N.B., this is the reason behind
the sorting done in the final stage of phrase extraction.}
Then, $N$ scorers are run in parallel: each of them is provided with its set
of chunks to process.
For each chunk the scorer outputs the phrase pairs populated either with direct
or inverse scores.
After all scorers finish their job, all chunks are merged together.
The inverse phrase table is sorted once more, but this time differently:
now the order of phrase pairs has to match the direct phrase table.
This makes the consolidation of both halves simple; they are just merged
in a linear fashion to produce the final \emph{phrase translation table}.

Before we finish this section, it is important to mention that all
the temporary data stored to disk during processing is compressed
(and then uncompressed when read later on).
This helps to lessen the total disk usage and also reduces disk I/O in exchange for
increased CPU load, which in a typical case is a worthwhile exchange as parallel reads/writes
to disk tend to slow down the overall process much more than the additional
CPU processing required to perform compression and decompression.

Also, we should mention that Moses allows to train SMT model with multiple input
factors, a so-called \emph{factored model}.\footurl{http://www.statmt.org/moses/?n=FactoredTraining.FactoredTraining}
However, from the perspective of phrase table extraction and scoring algorithm,
the factorized input data are handled transparently and no change to the algorithm
is required.

\section{Phrase table pruning}
\label{sec:phrase-table-pruning}

% The problem.
Despite that fact that \Tref{phrase-table-europarl} contains only excerpts from the list
of all translations of "Europe" from English to Czech, it is easy to see
that the quality of translations rapidly diminishes towards the bottom of the table
and that the total number of possible translations (more than 1000) suggests
that many of them are just wrong and/or will be never used in any translation.
This observation likewise holds true to the whole phrase table: we could pick
any other phrase to illustrate this phenomenon, however, short and frequent
phrases tend to accumulate more of such "noise".

% The solution.
Such a significant presence of poor translation options in translation tables led to
attempts to design methods for their removal that could effectively produce smaller and
easier to handle phrase tables and at the same time retain the overall quality of
translation, or possibly even improve it, by discarding the misleading options.

A systematic overview of the existing phrase table pruning methods is presented in
\citep{zens:systcomp}.
Notably, three main approaches are mentioned:
\begin{enumerate}
  \item Simple statistics pruning -- pruning based either on the frequency counts or
    translation probabilities of phrase pairs
  \item Significance pruning -- pruning based on significance testing of
    cooccurrences of phrases aiming at the filtration of the poor translation options
    caused by input data noise
  \item Relative entropy pruning -- pruning based on the relative entropy model
    aimed at the reduction of the internal redundancy of translation models
\end{enumerate}

\subsection{Simple statistics pruning}
\label{sec:simple-statistics-pruning}

The simplest pruning methods base their pruning criteria on the statistics directly
available in the pruned phrase table. \citet{zens:systcomp} specify 4 different criteria
categorized either as absolute or a relative.

\subsubsection*{Absolute pruning}

Absolute pruning methods rely on the statistics of a single phrase pair only and therefore
they prune each phrase pair independently of the others.
This way, it is possible to remove all translation options of a particular source
phrase.\footnote{However, whether the quality of the translation model can be hurt by this is
still an open question.}

\emph{Count-based pruning} prunes a phrase pair $(\bar{f},\bar{e})$ if its frequency count
$C(\bar{f},\bar{e})$ is below a threshold $\theta_{c}$:
\begin{equation} \label{eq:count-based-pruning}
  C(\bar{f},\bar{e}) < \theta_{c}
\end{equation}

\emph{Probability-based pruning} prunes a phrase pair $(\bar{f},\bar{e})$ if its forward
translation probability $\phi(\bar{e}|\bar{f})$ is below a threshold $\theta_{p}$:
\begin{equation}
  \phi(\bar{e}|\bar{f}) < \theta_{p}
\end{equation}

\subsubsection*{Relative pruning}

Relative pruning methods process a whole set of phrase pairs with the same source phrase
$\bar{f}$ at once and always retain at least one phrase pair from each set (one translation
option for each source phrase).

\emph{Threshold pruning} prunes phrase pairs $(\bar{f},\bar{e})$ that have a forward translation
probability far worse than the phrase pair representing the best translation option for the source
phrase $\bar{f}$ with respect to a given minimum ratio threshold $\theta_{t} \in [0,1]$:
\begin{equation}
  \phi(\bar{e}|\bar{f}) < \theta_{t} \max_{\bar{e}}{\phi(\bar{e}|\bar{f})}
\end{equation}

\emph{Histogram pruning} prunes all but the top $K$ phrase pairs $(\bar{f},\bar{e})$ from the set
of all phrase pairs with the same source phrase $\bar{f}$ based on their forward translation
probability.

It is easy to note the similarity of both criteria and as a matter of the fact with $\theta_{t} = 1$
and $K = 1$ both methods give the same result: only the best translation option(s) for each
source phrase are kept in the pruned phrase table.

\subsection{Significance pruning}
\label{sec:significance-pruning}
% Introduce Johnson's significance filtering.

% Introduce the idea of significance filtering
A phrase table pruning method popular within the Moses community is \emph{significance filtering}
introduced by \citet{johnson:sigfilter} and implemented by the \emph{sigfilter} tool
that is part of the third party contributions to the Moses toolkit.\footnote{More information
about \emph{sigfilter} is available in the section "Pruning the Translation Table" of
\url{http://www.statmt.org/moses/?n=Moses.AdvancedFeatures}.}
It has proved to be capable of pruning out a substantial amount of a phrase table with no
significant harm to its quality: authors reported savings up to 90\% and in some cases
even an improvement in the quality after pruning as measured by the BLEU score.
The idea behind significance filtering is that phrase pairs that are weakly supported
by the data or, in other words, more statistically insignificant than others, can be
removed, because it is more likely that their occurrence is either chance or an artifact
incurred by the models used to prepare the parallel data (such as the model creating word
alignments).

% Describe counts necessary for contingency table
Internally, a significance filter performs significance testing using two by two
contingency tables that are populated with the following counts (see
\Tref{two-by-two-contingency-table}):
\begin{itemize}
  \item $C(\bar{f},\bar{e})$ as the number of parallel sentences that contain one or more
    occurrences of phrase $\bar{f}$ on the source side and phrase $\bar{e}$ on the target side
  \item $C(\bar{f})$ as the number of parallel sentences that contain one or more
    occurrences of phrase $\bar{f}$ on the source side
  \item $C(\bar{e})$ as the number of parallel sentences that contain one or more
    occurrences of phrase $\bar{e}$ on the target side
  \item $N$ as the total number of parallel sentences
\end{itemize}

% An example of 2x2 contingency table
\begin{table}[ht]
\centering
\def\arraystretch{1.5}
\begin{tabular}{ | c c | c | }
\hline
$C(\bar{f},\bar{e})$  &  $C(\bar{f}) - C(\bar{f},\bar{e})$  &  $C(\bar{f})$ \\
$C(\bar{e}) - C(\bar{f},\bar{e})$  &  $N - C(\bar{f}) - C(\bar{e}) + C(\bar{f},\bar{e})$  & $N - C(\bar{f})$ \\
\hline
$C(\bar{e})$  &  $N - C(\bar{e})$  &  $N$ \\
\hline
\end{tabular}
\caption{\label{two-by-two-contingency-table}
Two by two contingency table for $\bar{f}$ and $\bar{e}$.}
\end{table}

% Define p-value, mention Fisher's exact test
Use of a \emph{significance test} allows for the statistical assessment of the importance of an association
represented by the contingency table by calculating the probability (formally denoted
by its \emph{p-value}) that the observed contingency table or any more extreme example could
occur by chance assuming the model of independence: thus, the lower the p-value of
a particular table the more significant the represented association.
Several significance tests exist, but many depend on asymptotic assumptions
that are not valid for small counts, therefore significance filtering employs
\emph{Fisher's exact test} that calculates the \emph{p-value} using a hypergeometric
distribution with the result that it is valid for all sample sizes.

% Put phrase table filtering in perspective to significance testing
In the case of significance filtering, the association being tested is
the cooccurrence of phrases $\bar{f}$ and $\bar{e}$ forming a particular phrase pair $(\bar{f},\bar{e})$
and the model of independence simply presumes that there is no explicit dependence
between phrases from the source and target sides.

To prune the phrase table a threshold marking the maximum p-value must be given.
Because the probabilities are very small numbers, their negative logarithm is used
instead, and as a consequence the threshold has to be a positive real number with
bigger values meaning more pruning.

% 1-1-1 phrase pairs and related a-e and a+e thresholds
The significance filter has no predefined thresholds with the exception of a special
(but frequent) case of phrase pairs with $C(\bar{f},\bar{e}) = C(\bar{f}) = C(\bar{e}) = 1$.
The p-value of the contingency table of these \emph{1-1-1 phrase pairs} under Fisher's
exact test is $1/N$ and by setting $\alpha = log(N)$ and picking $\epsilon$ such that
$0 < \epsilon \ll 1$, we can define two useful thresholds:
\begin{itemize}
  \item $\alpha + \epsilon$ is the smallest threshold that results in complete removal
    of 1-1-1 phrase pairs
  \item $\alpha - \epsilon$ is the largest threshold that results in complete preservation
    of 1-1-1 phrase pairs
\end{itemize}

Moreover, since the contingency table for 1-1-1 phrase pairs has the lowest p-value
of all contingency tables with $C(\bar{f},\bar{e}) = 1$, setting the pruning threshold
to $\alpha + \epsilon$ results in the removal of all phrase pairs occurring exactly once.

% The cutoff limit
In addition to setting the pruning threshold for p-values, the \emph{sigfilter} tool
also allows the pruning of all but the top $K$ translation options of each source phrase,
$K$ being referred to as \emph{cutoff} limit.\footnote{In other words \emph{sigfilter}
also implements \emph{histogram pruning} method.}
The combination of both is also possible: first, the cutoff limit is applied leaving
only the top $K$ options; then, the remaining phrase pairs are pruned with respect to the
p-value threshold.

\subsection{Relative entropy pruning}
\label{sec:relent-pruning}
% Introduce phrase table filtering by Ling.

% Sketch the problem.
One of the motivations behind the move from word-based to phrase-based models
is that phrase pairs allow the capture of contextual information that contributes
to more precise translation.
However, a typical phrase table will contain many phrase pairs that do not
encode any context relevant for phrase translation: often, using a particular
phrase pair in the translation process will have a similar effect to using
several shorter phrase pairs independent of each other.
As a consequence, such longer phrase pairs can be deemed superfluous and thus
viable candidates for pruning.

% Introduce the relent filter.
Based on this observation, a phrase table filtering method called \emph{relative
entropy pruning} has been recently proposed by \citet{ling:relentfilter} and
\citet{zens:systcomp}.
This method first establishes a relative entropy model that measures how likely
a phrase pair encodes a translation event that might be derived from translation
events represented by shorter phrase pairs with a similar overall translation
probability. Then, it applies this model to phrase table pruning, removing
the most redundant translation options.
The reported results show that this method has little negative impact on
translation quality and in some situations can even lead to small improvements,
because the pruning of redundant translation options effectively compresses
the search space during decoding and a wider spectrum of options is explored.

% Briefly describe the relative entropy metric.
The goal of relative entropy pruning is to deliver a pruned model $\phi'(\bar{e}|\bar{f})$
as similar as possible to the original model $\phi(\bar{e}|\bar{f})$.
To measure models' similarity the Kullback-Leibler divergence is used:
\begin{equation}
  D(\phi||\phi') = \sum_{\bar{f},\bar{e}}{p(\bar{f},\bar{e}) \Big[log\big(\phi(\bar{e}|\bar{f})\big) - log\big(\phi'(\bar{e}|\bar{f})\big)\Big]}
\end{equation}

For each phrase pair $(\bar{f},\bar{e})$ the deviation between the probability emission
from the pruned model and the original model expressed as
$log\big(\phi(\bar{e}|\bar{f})\big) - log\big(\phi'(\bar{e}|\bar{f})\big)$ is weighted
by the frequency of observation of the pair given by $p(\bar{f}, \bar{e})$.

% On computation of p(f,e)
There is no obvious optimal distribution to the weight component $p(\bar{f}, \bar{e})$,
so a simple solution is to just use a uniform distribution.
However, \citet{ling:relentfilter} suggests that as the input data size grows, this
approach proves to be inferior to the more complex approach of using a multinomial distribution
modeled by the fraction:
\begin{equation}
  p(\bar{f}, \bar{e}) = \frac{N(\bar{f},\bar{e})}{N}
\end{equation}
where $N(\bar{f},\bar{e})$ is the number of sentence pairs with phrase $f$ observed on the
source side and $e$ on the target side and $N$ is the total number of sentence pairs
in the parallel data.

% On computation of p'(e|f)
The probability $\phi'(\bar{e}|\bar{f})$ in the pruned model is estimated by using a
decoder to select the best translation options with the phrase pair $(\bar{f},\bar{e})$
being no longer available.
In such a case, the decoder is forced to use shorter phrase pairs to produce the same
translation.
Therefore, the pruned model probability has to account for all possible segmentations
$S_I^K$ of the phrase pair $(\bar{f},\bar{e})$ into $K$ "sub-phrase-pairs"
$\bar{e}_k$ and $\bar{f}_{\pi_k}$ with all possible reorderings $\pi_I^K$ determining
the alignment of such "sub-phrase-pairs".
The ultimate approximation of this probability is following:\footnote{See
\citet{zens:systcomp} for a more detailed derivation of this approximation.}
\begin{equation}
  \phi'(\bar{e}|\bar{f}) \approx \max_{S_I^K,\pi_I^K}{\prod_{k=1}^{K} \phi(\bar{e}_k|\bar{f}_{\pi_k})}
\end{equation}

% Note about the "independence of phrase pairs" assumption.
The probability assigned to a phrase pair $(\bar{f},\bar{e})$ by the translation model
may, in general, depend on the ordering of the phrase pairs within the sentence pair, but from
a computational perspective a search for the best model adhering to such a consideration
would be unfeasible, because all possible subsets of a given size would have to be evaluated.
Because of this, an assumption that all phrase pairs affect the relative entropy roughly
independently of their context is applied and with this assumption the problem can be
reduced to a local optimization problem that offers a simple criterion for pruning.
Namely, a phrase pair $(\bar{f},\bar{e})$ is pruned if its contribution to the overall
divergence of the pruned and original model is below a threshold $\theta_{re}$:
\begin{equation}
  p(\bar{f},\bar{e}) \Big[log\big(\phi(\bar{e}|\bar{f})\big) - log\big(\phi'(\bar{e}|\bar{f})\big)\Big] < \theta_{re}
\end{equation}

% No parametric thresholds here.
The concept of relative entropy pruning does not provide any option to set
the pruning threshold parametrically as in the case of the $\alpha \pm \epsilon$ option
available with \emph{significance filtering}.
The typical usage potential of the method lies rather in providing the possibility to pruning off
a given portion of a phrase table (eg. 50\% of it), ensuring that the translation model
powered by the pruned table will perform as closely to the original model as possible.

% Mention the relent filter in Moses 0.91
An implementation of relative entropy pruning by \citet{ling:relentfilter}, called a
\emph{relent-filter}, is also part of the third-party contributions to the Moses toolkit,
but works only with an older version of Moses (release 0.91).\footnote{More information
about the \emph{relent-filter} tool is available in the section "Pruning the Phrase Table based
on Relative Entropy" in the Advanced Features chapter of the Moses online manual:
\url{http://www.statmt.org/moses/?n=Moses.AdvancedFeatures}.}
% Compare relent filter to sigfilter.
Despite the fact that technically the \emph{relent-filter} acts as a post-filtering tool,
the same way \emph{sigfilter} does, it is more complex to apply and also far more
computationally demanding.
The increased complexity stems from the fact that weights for all the components of the
translation system have to be tuned in advance to ensure the soundness of the relative entropy metric.
The increased computation demands are mainly caused by the forced decoding of the whole set of
phrase pairs in the phrase table that is required for the calculation of divergence scores.

\section{Phrase table compacting}

\emph{Phrase table compacting} is an approach that also attempts to deal with the performance
problems incurred by phrase tables of enormous sizes, but does so by compressing the data
stored in phrase tables, rather than by filtering them.
Usually, this compression not only saves disk space, but also results in faster loading times
during decoding and therefore positively impacts the decoding speed as well.

Although both phrase table compacting and pruning aims at tackling a similar problem, phrase
table compacting is independent of the way pruning is done, so both approaches are complementary:
any pruned phrase table can be further compacted.

As this work is mainly related to phrase table pruning, it would be excessive to provide more
details on phrase table compacting here, but we would like to point any reader interested in this
topic to a recent paper by \citet{junczys:compact} that presents the theoretical background behind
the phrase table compression tool, that is now the recommended option in Moses,\footnote{More
information about this compression tool is available in the section "Compact Phrase Table" of:
\url{http://www.statmt.org/moses/?n=Moses.AdvancedFeatures}} and also links to many relevant
sources of information on this topic.
