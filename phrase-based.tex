% In this chapter:
% - introduction of phrase based SMT
% - description of phrase table
% - description of phrase table creation in Moses
% -- with alternatives: memscore
% - phrase table post filtering
% -- sigfilter
% - alternatives to precomputed phrase table
% -- dynamic suffix arrays
% - mention phrase table compression tools

\chapter{Phrase tables in phrase-based Statistical Machine Translation}
\label{chap:phrase-based}

% Briefly introduce the concept of phrase-based SMT
% (SMT book, Chapter 5)

Phrase-based models are nowadays considered the most eminent approach in
the field of Statistical Machine Translation and the best performing
SMT systems are based on phrase-based models \citep{koehn:smt},
Google Translate\footurl{http://translate.google.com} being a prominent example.
Their mathematical motivation is very simple, yet they prove to be powerful enough
to compete with linguistically motivated rule-based models.
In addition, they conceptually benefit from steadily increasing amount of both
monolingual and multilingual corpora available as a positive side-effect of rapidly
growing usage of information technologies in our daily lives.

\section{Phrase-based Statistical Machine Translation}

Before phrase-based model were introduced, a yet simpler word-based models were in
the center of interest of MT researchers. \emph{Word-based models} use a lexicon
of words with their translations to produce translation of the full text by proceeding
with one word at a time.
This simple approach works well, but has obvious drawbacks,
for example the inability to catch word context.
Also it does not account for situations, where foreign word has to be translated
with two or more words or vice versa.

\emph{Phrase-based models} originates from the concept of word-based models,
but instead of using only single word as translation unit they
allow for sequence of words -- \emph{a phrase} -- to be translated at once.
This seemingly trivial extension makes for a non-trivial improvement
with following imminent benefits:
\begin{enumerate}
  \item By using phrases rather than words the model is now implicitly able to deal with
    \emph{one-word-translates-to-many} situations.
  \item A phrase-based model is able to handle local reordering of words for example switching
    from \emph{noun follows adjective} to \emph{adjective follows noun} order etc.
  \item Word context on source side plays now more important role, because it might be kept
    within the phrase pair and lead to implicit disambiguation during translation.
  \item With a lot of data and storage space available the model can be trained to cover
    (and later easily translate) the whole sentences.
\end{enumerate}


\subsection{Mathematical Definition}
% TODO.


\section{Phrase translation table}
% What's the phrase table?

\emph{Phrase translation table} in SMT systems captures the mapping
between phrases in source language and phrases in target language
along with scores that should reflect the quality of translating a particular
source language phrase as a particular target language phrase.
The concept of phrase table is not related to any linguistic notion,
neither the particular phrase pair nor the scores have to be meaningful
in terms of syntax or semantics. The current phrase-based systems construct
phrase tables using \emph{maximum likelihood} estimates calculated from (almost)
plain text input data (hence statistical). The utility of translating phrase
$s$ into phrase $t$ is then represented by maximum likelihood probability of
the phrase $t$ given the phrase $s$.

An example of a simple phrase table is presented by \Tref{phrase-table-example}.
The table contains various options of how to translate Czech phrase
"pes" (dog) into English.

% TODO: Include real phrase table example.
% TODO: Include a phrase, not a single word only.
\begin{table}[h]
\centering
\begin{tabular}{ l l l}
Czech (s) & English (t) & Probability p(t|s) \\
\hline
\hline
pes & dog & 0.8 \\
pes & cat & 0.1 \\
pes & wolf & 0.07 \\
pes & hamster & 0.03 \\
\hline
\hline
\end{tabular}
\caption{\label{phrase-table-example}Phrase translation table extract.}
\end{table}

We may note that the example phrase table contains some erroneous items,
as dog is unlikely to become cat just by switching to a different language.
The fact is that standard methods of phrase table construction may produce such
unlikely phrase translations, one reason being that their input data usually
contains a lot of noise.
We will discuss this problem further in the following sections.

% TODO: Ctrl+C Ctrl+V from the eppex paper.

Phrase tables in Statistical Machine Translation (SMT) systems generally take
the form of a list of pairs of phrases $s$ and $t$, $s$ being the phrase from
the source language and $t$ being the phrase from the target language, along
with scores that should reflect the goodness of translating $s$ as $t$.
The standard approach to obtain such scores is to use \emph{maximum likelihood
probability} of the phrase $t$ given the phrase $s$ and vice versa.
The probabilities $p(s|t)$ and $p(t|s)$ are often referred to as
\emph{forward} and \emph{reverse} \emph{translation probabilities}.


\subsection{Phrase table creation in Moses}

% TODO: Ctrl+C Ctrl+V from the eppex paper.

To estimate $p(s|t)$ and $p(t|s)$, frequency counts $C(t,s)$, $C(s)$ and
$C(t)$ are usually collected from the entire training corpus.
For substantial coverage of source and target languages, such corpora are
often very big so all phrase pairs and their counts cannot fit in the
physical memory of the computer.
To overcome this limitation, phrase table construction methods often simply
dump observed phrases to local disk and sort and count them on disk.
This approach allows to construct phrase tables of size limited only by the
capacity of the disk.
The obvious drawback of this solution is that much more time is needed
to build the table.

% What's the state-of-art implementation of phrase table extraction in Moses?
% Introduce train-model.perl and steps of training pipeline.

Moses release 1.0 comes with a training script that incorporates all the steps
involved in creation of ready-to-go translation system from the parallel corpus,
including phrase table creation.
In the \emph{train-model.perl} script this whole training process is covered by
9 subsequent steps\footurl{http://www.statmt.org/moses/?n=FactoredTraining.HomePage},
including word alignment, lexical table construction, phrase table construction and more.

The modular step-by-step design of the training process makes it very open to
alternate implementations of particular steps.
The training script interface explicitly supports use of third party tools by
allowing only subrange of the nine steps to be executed.
The only (obvious) requirement is that such tools have to be capable of reading and
writing data in format that is compatible with the preceding and subsequent steps
of the pipeline.

In this work we are exclusively concerned with the fifth and sixth step of
the training pipeline:
the phrase pairs extraction\footurl{http://www.statmt.org/moses/?n=FactoredTraining.ExtractPhrases}
and phrase pairs scoring\footurl{http://www.statmt.org/moses/?n=FactoredTraining.ScorePhrases}.
Conceptually, these two steps can be considered a single step only, as their ultimate
goal is to construct phrase table given parallel corpus and word alignment.
The reason why the phrase table construction is split in these two steps is that
for large input data the table simply does not fit into computer memory.
To overcome this limitation the core implementation uses disk space as a temporary storage
for extracted phrases and one of the effects of such design is the separation of
phrase extraction and phrase scoring phases into individual steps.
Another reason for this break-up is the extraction of reordering information
of individual phrase pairs -- this information is later used in step 7 in creation
of lexicalized reordering model.

Let us now describe the phrase extraction and phrase scoring steps in greater detail.

The whole training pipeline aims to utilize multi-core processors architecture that
prevails in nowadays computers as much as possible. To turn on parallel processing,
two parameters exists and they both apply also to the steps of our interest:
\begin{itemize}
  \item \verb|--parallel|: A boolean switch that enables concurrent run of such parts of
  pipeline that performs the same task twice: once for source and once for target language.
  With this switch on, both parts are processed in parallel.
  \item \verb|--cores N|: An unsigned integer that simply sets the number of CPU threads
  that may be utilized during training. In Moses 1.0 only phrase extraction and phrase
  scoring steps make use of this setting.
\end{itemize}
Despite they do very similar job, both options apply to different parts
of implementation, so they can be combined.
However, even if none of these options is specified, some parts of pipeline
are still implicitly parallelized.

The \emph{phrase extraction phase} starts with optional split of all input files
(alignment information, both parts of corpus and optional weights file) into $N$
parts, where $N$ is the number of threads set by \verb|--cores|.
Then, the actual extraction of phrases is done by running $N$ extractors in parallel,
each processing its own split.
Each extractor produces following set of files: direct phrase pairs table, inverse
phrase pairs table and (optional) reordering information table.
In the final stage, all output files of the same type from all $N$ extractors are
merged together, sorted and dumped into one direct and one inverse phrase tables
and one reordering information table.

In \emph{phrase scoring phase} first both direct and inverse phrase tables are
populated with scores and then both are consolidated into the final phrase table.
The scoring for both parts can be processed in parallel if the \verb|--parallel|
switch is specified.
Further parallelization is done within the scoring process itself.
First, similarly as with extraction, the input is split into $N$ chunks.
However, this time it is important that all phrase pairs with the same source
phrase are kept together in the same chunk (and vice versa for target phrases
in case of inverse phrase table).\footnote{N.B., this is the reason behind the sorting done in the final stage of phrase extraction.}
Then, $N$ scorers are run in parallel: each of them is provided with its set
of chunks to process.
For each chunk the scorer outputs the phrase pairs populated either with direct
or inverse scores.
After all scorers finish their job, all chunks are merged together.
The inverse phrase table is sorted once more, but this time differently:
now the phrase pairs order has to match the direct phrase table.
This make the consolidation of both halves simple; they are just merged
in a linear fashion to produce the final \emph{phrase translation table}.

Before we finish this section, it is important to mention that all
the temporary data stored to disk during processing is compressed
(and then uncompressed when read later on).
This helps to lessen the total disk usage and also reduces disk I/O in cost of
more CPU time, which in typical case is a worthy deal as parallel reads/writes
to disk tends to slow down the overall process much more than additional
CPU processing required to perform compression and decompression.

\subsection{Phrase table pruning}

% Introduce Johnson's significance filtering.

\subsection{Phrase table compression}

% Introduce phrase table compacting tool by Marcin.
