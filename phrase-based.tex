% In this chapter:
% - introduction of phrase based SMT
% - description of phrase table
% - description of phrase table creation in Moses
% -- with alternatives: memscore
% - phrase table post filtering
% -- sigfilter
% -- relent filter
% - alternatives to precomputed phrase table
% -- dynamic suffix arrays
% - mention phrase table compression tools

\chapter{Phrase tables in phrase-based Statistical Machine Translation}
\label{chap:phrase-based}

% Briefly introduce the concept of phrase-based SMT
% (SMT book, Chapter 5)

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{The currently best performing statistical machine translation systems are based on
phrase-based models: models that translate small word sequences at a time.}{--- Philipp Koehn, Statistical Machine Translation}

Phrase-based models are nowadays considered the most eminent approach
in the field of Statistical Machine Translation.
In this chapter we are going to describe their concept with a particular stress
on description of phrase translation tables and methods of their construction,
but our introduction will be, inevitably, very condensed.
For a more detailed insight into phrase-based models and statistical machine
translation in general, we gently point our reader to \citet{koehn:smt}.

\section{Phrase-based Statistical Machine Translation}

Before phrase-based models were introduced, a yet simpler word-based models were in
the center of interest of MT researchers.
\emph{Word-based models} use a lexicon of words with their translations to produce
translation of the full text by proceeding with one word at a time.
\emph{Phrase-based models} originates from this concept, but instead of using only
single word as translation unit they allow for sequence of words -- \emph{a phrase}
-- to be translated at once.
This seemingly trivial extension makes for a non-trivial improvement with some
imminent benefits:
\begin{enumerate}
  \item By using phrases rather than words this model is now implicitly able to deal
    with non-compositional phrases
  \item A phrase-based model is able to handle local reordering of words, for example:
    switching from \emph{noun follows adjective} to \emph{adjective follows noun} order.
  \item Word context on source side plays now more important role, because it might be kept
    within the phrase pair and lead to implicit disambiguation during translation.
  \item With a lot of data the model can be trained to cover long phrases or even
    whole sentences.
\end{enumerate}

\subsection{Mathematical background}

The phrase translation model is based on noisy channel model \citep{koehn:spbt}.
By using Bayes rule, the translation probability for translating foreign sentence $f$
into English as $e_{best}$ can be reformulated as:
\begin{equation}
  e_{best} = argmax_{e}~p(e|f) =  argmax_{e}~p(f|e)p(e)
\end{equation}

This allows for a separate translation model $p(f|e)$ and language model $p(e)$.
The translation model is further decomposed into:
\begin{equation} \label{eq:tm}
  p(\bar{e}_1^I|\bar{f}_1^I) = \prod_{i=1}^I \phi(\bar{f}_i|\bar{e}_i) d(start_i - end_{i-1} - 1)
\end{equation}

The foreign sentence $f$ is broken up into $I$ phrases $\bar{f}_i$, each of them
being translated into an English phrase $\bar{e}_i$. Since by the use of noisy channel
model we inverted the translation direction, the phrase translation probability
$\phi(\bar{f}_i|\bar{e}_i)$ is modeled as a translation from English to foreign language.

The reordering of English phrases is handled by \emph{reordering model}, in \Eref{eq:tm}
denoted by $d(start_i - end_{i-1} - 1)$.
The most used reordering model is \emph{distance-based} and assigns reordering cost based
on distance of the position of the first word of foreign input phrase that translates to
the $i$-th English phrase (noted as $start_i$) and the position of the last word of foreign
phrase that translates to preceding English phrase (noted as $end_{i-1}$).
In case of two phrases translated in sequence, $start_i = end_{i-1} +1$, and the
reordering model assigns cost of $d(0)$.
The probability distribution of $d$ can be either computed from data or set by applying
exponentially decaying cost function $d(x) = \alpha^{|x|}$ with a proper value of $\alpha$
\citep{koehn:smt}.

By putting together decomposed translation model and language model (we assume standard $n$-gram
language model here), we get the mathematical definition of
\emph{phrase-based statistical machine translation model}:
\begin{equation} \label{eq:pbsmt}
  e_{best} = argmax_e \prod_{i=1}^I \phi(\bar{f}_i|\bar{e}_i) d(start_i - end_{i-1} - 1) \prod_{i=1}^{|e|} p_{LM}(e_i|e_{i-n+1}...e_{i-1})
\end{equation}

Putting mathematical definitions aside, the role of all three components in search of
the best translation can be explained in more intuitive way:
\begin{itemize}
  \item The phrase translation model ensures that best translation for each phrase is selected.
  \item The reordering model ensures the phrases are reordered appropriately.
  \item The language model ensures the output is fluent English.
\end{itemize}

\subsection{Log-linear model}

So far we have introduced three components of SMT system -- phrase translation model,
reordering model and language model -- and described them in terms of probability
distributions that determine them.
Then, we laid out the definition of phrase-based model as multiplication of all
three distributions.
Despite the elegance of such definition, it has its drawbacks and current SMT
systems tend to use a more flexible framework to combine all the various
components that is well-known especially in machine learning community:
a \emph{log-linear model}.
It has general form:
\begin{equation}
  exp \Big( \sum_i^n \lambda_i h_i(x_i) \Big)
\end{equation}

The $h_i(x)$ are quantities that are functions of variables $x_i$,
while $\lambda_i$ stand for the model parameters.

The original definition of phrase-based SMT model (\Eref{eq:pbsmt}) can be reformulated to
fit log-linear model by defining $h_1 = log(\phi)$, $h_2 = log(d)$ and $h_3 = log(p_{LM})$
and introducing corresponding model parameters $\lambda_{\phi}$, $\lambda_{d}$ and $\lambda_{LM}$:
\begin{equation}
  \begin{aligned}
  p(e|f) = exp \Bigg[ &\lambda_{\phi} \sum_{i=1}^I log \, \phi(\bar{f}_i|\bar{e}_i) \\
       &+ \lambda_{d} \sum_{i=1}^{I} log \, d(start_i - end_{i-1} - 1) \\
       &+ \lambda_{LM} \sum_{i=1}^{|e|} log \, p_{LM}(e_i|e_{i-n+1}...e_{i-1}) \Bigg] \\
  \end{aligned}
\end{equation}

The log-linear model has some advantages over the probabilistic formulation.
First, as it is not build upon probabilistic reasoning, it does not require its
components to be based on valid probability distributions.
This way new features can be easily incorporated into the model, for example the direct
phrase translation probability $\phi(e|f)$\footnote{Recall, that the phrase translation
probability model $\phi(\bar{f}|\bar{e})$ is inverted due to the use of noisy channel model.}.
Second, the $\lambda_i$ parameters can be used to put more weight on some of the components,
for example accenting the language model more if the translation is good, but the output
lacks fluency etc.

\section{Phrase translation table}
% What's the phrase table?

% It is hard to find better words to introduce section on phrase translation tables
% than \citet[p.130]{koehn:smt} did:
% \begin{quote}
%  Clearly, the power of phrase-based translation rests on a good phrase
%  translation table.
% \end{quote}

In \Cref{chap:introduction} we already came with a brief definition of phrase
translation table. Let us repeat it here using more formal notion that adheres
to the notation introduced earlier in this chapter.

\emph{Phrase translation tables} in SMT systems generally take the form of a list of
pairs of phrases $\bar{f}$ and $\bar{e}$, $\bar{f}$ being the phrase from the source
language and $\bar{e}$ being the phrase from the target language, along with scores
that should reflect the goodness of translating $\bar{f}$ as $\bar{e}$.
The standard approach to obtain such scores is to estimate \emph{maximum likelihood
probability} of the phrase $\bar{e}$ given the phrase $\bar{f}$ and vice versa.
The probabilities $\phi(\bar{e}|\bar{f})$ and $\phi(\bar{f}|\bar{e})$ are often
referred to as \emph{direct} and \emph{inverse} (or \emph{forward} and \emph{backward})
\emph{phrase translation probabilities}.

To estimate maximum likelihood probabilities, the following frequency counts
of phrase pairs extracted from parallel corpus must be collected:
\begin{itemize}
  \item $C(\bar{f},\bar{e})$ -- number of occurrences of phrase pair $(\bar{f},\bar{e})$
  \item $C(\bar{f})$ -- number of occurrences of phrase pairs with phrase $\bar{f}$ on source side:
    $C(\bar{f}) = \sum_{e}{C(\bar{f},e)}$
  \item $C(\bar{e})$ -- number of occurrences of phrase pairs with phrase $\bar{e}$ on target side:
    $C(\bar{e}) = \sum_{f}{C(f,\bar{e})}$
\end{itemize}

The maximum likelihood probabilities $\phi(\bar{e}|\bar{f})$ and
$\phi(\bar{f}|\bar{e})$ are then calculated from these counts as follows:
\begin{equation}
  \phi(\bar{e}|\bar{f}) = \frac{C(\bar{f},\bar{e})}{C(\bar{f})}
  ~~~~~~~~~~ % Maybe not the cleanest, but definitely working solution :)
  \phi(\bar{f}|\bar{e}) = \frac{C(\bar{f},\bar{e})}{C(\bar{e})}
\end{equation}

\Tref{phrase-table-europarl} presents an extract from the actual phrase table
(with only the direct phrase translation probabilities).
The full table has been constructed from Europarl v6 Czech-English
corpus.\footurl{http://www.statmt.org/europarl/archives.html}

% An example of actual phrase table.
\begin{table}[h]
\centering
\begin{tabular}{ l l l l}
No. & English ($\bar{f}$) & Czech ($\bar{e}$) & $\phi(\bar{e}|\bar{f})$ \\
\hline
\hline
1. & europe & evropě & 0.325794 \\
2. & europe & evropa & 0.244393 \\
3. & europe & evropu & 0.121596 \\
4. & europe & evropy & 0.114969 \\
5. & europe & evropou & 0.0204101 \\
6. & europe & evropě , & 0.0174326 \\
7. & europe & evropské & 0.0144552 \\
8. & europe & evropských & 0.0139269 \\
9. & europe & evropu , & 0.012054 \\
10. & europe & evropy , & 0.00840417 \\
... & ... & ... & ... \\
52. & europe & na evropu & 0.000288143 \\
53. & europe & ) evropa & 0.000240119 \\
... & ... & ... & ... \\
56. & europe & evropě v & 0.000240119 \\
57. & europe & , evropu & 0.000192095 \\
... & ... & ... & ... \\
75. & europe & unie & 0.000192095 \\
76. & europe & , která evropa & 0.000144071 \\
... & ... & ... & ... \\
94. & europe & v evropě , & 0.000144071 \\
95. & europe & " evropa & 9.60476e-05 \\
... & ... & ... & ... \\
160. & europe & že by evropa & 9.60476e-05 \\
161. & europe & " evropa ( & 4.80238e-05 \\
... & ... & ... & ... \\
1010. & europe & životě v evropě & 4.80238e-05 \\
\hline
\hline
\end{tabular}
\caption{\label{phrase-table-europarl}
An extract from phrase table listing some of the possible translations of English word
"Europe" into Czech sorted by their maximum likelihood probability.
The full table contained more than 26 millions of phrase pairs and has been extracted
from Europarl v6 Czech-English corpus.}
\end{table}

\subsection{Phrase table creation in Moses}
\label{sec:phrase-table-creation-in-moses}
% What's the state-of-art implementation of phrase table extraction in Moses?
% Introduce train-model.perl and steps of training pipeline.

It has been already said in introduction that the phrase table construction methods
usually store all the temporary data on the disk, because the amount of extracted
phrase pairs is often very big and so they cannot fit in the physical memory of
the computer.
The phrase table construction process in \emph{Moses} also adheres to this concept.
In the following text, we describe this process in great detail to give our reader
a clear picture of its quite complex layout and all the steps and substeps involved.

Moses release 1.0\footurl{http://www.statmt.org/moses/?n=Moses.Releases} comes
with a training script that incorporates all the steps involved in creation of
ready-to-go translation system from the parallel corpus, including phrase table creation.
In the \emph{train-model.perl} script this whole training process is covered by
9 subsequent steps\footurl{http://www.statmt.org/moses/?n=FactoredTraining.HomePage},
including word alignment, lexical table construction, phrase table construction and more.

The modular step-by-step design of the training process makes it very open to
alternate implementations of particular steps.
The training script interface explicitly supports use of third party tools by
allowing only subrange of the nine steps to be executed.
The only (obvious) requirement is that such tools have to be capable of reading and
writing data in format that is compatible with the preceding and subsequent steps
of the pipeline.

In this work we are exclusively concerned with the fifth and sixth step of
the training pipeline:
the phrase pairs extraction\footurl{http://www.statmt.org/moses/?n=FactoredTraining.ExtractPhrases}
and phrase pairs scoring\footurl{http://www.statmt.org/moses/?n=FactoredTraining.ScorePhrases}.
Conceptually, these two steps can be considered a single step only, as their ultimate
goal is to construct phrase table given parallel corpus and word alignment.
The reason why the phrase table construction is split in these two steps is that
for large input data the table simply does not fit into computer memory.
To overcome this limitation the core implementation uses disk space as a temporary storage
for extracted phrases and one of the effects of such design is the separation of
phrase extraction and phrase scoring phases into individual steps.
Another reason for this break-up is the extraction of reordering information
of individual phrase pairs -- this information is later used in step 7 in creation
of lexicalized reordering model.

The whole training pipeline aims to utilize multi-core processors architecture that
prevails in nowadays computers as much as possible. To turn on parallel processing,
two parameters exists and they both apply also to the steps of our interest:
\begin{itemize}
  \item \verb|--parallel|: A boolean switch that enables concurrent run of such parts of
  pipeline that performs the same task twice: once for source and once for target language.
  With this switch on, both parts are processed in parallel.
  \item \verb|--cores N|: An unsigned integer that simply sets the number of CPU threads
  that may be utilized during training. In Moses 1.0 only phrase extraction and phrase
  scoring steps make use of this setting.
\end{itemize}
Despite they do very similar job, both options apply to different parts
of implementation, so they can be combined.
However, even if none of these options is specified, some parts of pipeline
are still implicitly parallelized.

Let us now describe the phrase extraction and phrase scoring steps in greater detail.

The \emph{phrase extraction phase} starts with optional split of all input files
(alignment information, both parts of corpus and optional weights file) into $N$
parts, where $N$ is the number of threads set by \verb|--cores|.
Then, the actual extraction of phrases is done by running $N$ extractors in parallel,
each processing its own split.
Each extractor produces following set of files: direct phrase pairs table, inverse
phrase pairs table and (optional) reordering information table.
In the final stage, all output files of the same type from all $N$ extractors are
merged together, sorted and dumped into one direct and one inverse phrase tables
and one reordering information table.

In \emph{phrase scoring phase} first both direct and inverse phrase tables are
populated with scores and then both are consolidated into the final phrase table.
The scoring for both parts can be processed in parallel if the \verb|--parallel|
switch is specified.
Further parallelization is done within the scoring process itself.
First, similarly as with extraction, the input is split into $N$ chunks.
However, this time it is important that all phrase pairs with the same source
phrase are kept together in the same chunk (and vice versa for target phrases
in case of inverse phrase table).\footnote{N.B., this is the reason behind
the sorting done in the final stage of phrase extraction.}
Then, $N$ scorers are run in parallel: each of them is provided with its set
of chunks to process.
For each chunk the scorer outputs the phrase pairs populated either with direct
or inverse scores.
After all scorers finish their job, all chunks are merged together.
The inverse phrase table is sorted once more, but this time differently:
now the phrase pairs order has to match the direct phrase table.
This make the consolidation of both halves simple; they are just merged
in a linear fashion to produce the final \emph{phrase translation table}.

Before we finish this section, it is important to mention that all
the temporary data stored to disk during processing is compressed
(and then uncompressed when read later on).
This helps to lessen the total disk usage and also reduces disk I/O in cost of
more CPU time, which in typical case is a worthy deal as parallel reads/writes
to disk tends to slow down the overall process much more than additional
CPU processing required to perform compression and decompression.

\section{Phrase table pruning}
\label{sec:phrase-table-pruning}

% The problem.
Despite that \Tref{phrase-table-europarl} contains only excerpts from the list
of all translations of "Europe" from English to Czech, it is easy to notice
that the quality of translations rapidly diminishes to the bottom of the table
and that the total number of possible translations (more than 1000) suggests
that many of them are just wrong and/or will be never used in any translation.
This observation likewise holds true to the whole phrase table: we could pick up
any other phrase to illustrate this phenomenon, however, short and frequent
phrases tend to accumulate more of such "noise".

% The solution.
Such significant presence of poor translation options in translation tables led to
attempts to design methods of their removal that could effectively produce smaller,
easier to manipulate phrase tables and in the same time retain the overall quality of
translation process or possibly even improve it by discarding the misleading options.

A systematic overview of existing phrase table pruning methods is presented in \citep{zens:systcomp}.
Notably, three main approaches are mentioned:
\begin{enumerate}
  \item Simple statistics pruning -- pruning based either on frequency counts or
    translation probabilities of phrase pairs
  \item Significance pruning -- pruning based on significance testing of
    cooccurrences of phrases aiming at filtration of poor translation options caused
    by input data noise
  \item Relative entropy pruning -- pruning based on relative entropy model established
    on top of the translation model aiming at reduction of its internal redundancy
\end{enumerate}

\subsection{Simple statistics pruning}
\label{sec:simple-statistics-pruning}

The simplest pruning methods define their pruning criteria on the statistics immediately
available in the pruned phrase table. \citet{zens:systcomp} specify 4 different criteria
categorized to be either an absolute or a relative pruning method.

\subsubsection*{Absolute pruning}

Absolute pruning methods rely on statistics of a single phrase pair only and therefore
they prune each phrase pair independently of others.
This way, it is possible to remove all translation options of a particular source
phrase.\footnote{However, whether the quality of translation model can be hurt by this is
still an open question.}

\emph{Count-based pruning} prunes a phrase pair $(\bar{f},\bar{e})$ if its frequency count
$C(\bar{f},\bar{e})$ is below a threshold $\theta_{c}$:
\begin{equation} \label{eq:count-based-pruning}
  C(\bar{f},\bar{e}) < \theta_{c}
\end{equation}

\emph{Probability-based pruning} prunes a phrase pair $(\bar{f},\bar{e})$ if its forward
translation probability $\phi(\bar{e}|\bar{f})$ is below a threshold $\theta_{p}$:
\begin{equation}
  \phi(\bar{e}|\bar{f}) < \theta_{p}
\end{equation}

\subsubsection*{Relative pruning}

Relative pruning methods process a whole set of phrase pairs with the same source phrase
$\bar{f}$ at once and always retain at least one phrase pair from each set, therefore they
ensure there will remain at least one translation option for each source phrase.

\emph{Threshold pruning} prunes phrase pairs $(\bar{f},\bar{e})$ that have forward translation
probability far worse than the phrase pair representing the best translation option for source
phrase $\bar{f}$ with respect to given minimum ratio threshold $\theta_{t} \in [0,1]$:
\begin{equation}
  \phi(\bar{e}|\bar{f}) < \theta_{t} \max_{\bar{e}}{\phi(\bar{e}|\bar{f})}
\end{equation}

\emph{Histogram pruning} prunes all but top $K$ phrase pairs $(\bar{f},\bar{e})$ from the set
of all phrase pairs with the same source phrase $\bar{f}$ based on their forward translation
probability.

It is easy to note the similarity of both criteria and as a matter of fact with $\theta_{t} = 1$
and $K = 1$ both methods give the same result: only the best translation option(s) for each
source phrase are kept in pruned phrase table.

\subsection{Significance pruning}
\label{sec:significance-pruning}
% Introduce Johnson's significance filtering.

% Introduce the idea of significance filtering
A phrase table pruning method popular within Moses community is \emph{significance filter}
introduced by \citet{johnson:sigfilter} and implemented by the \emph{sigfilter} tool
that is part of third party contributions to Moses toolkit.\footnote{More information
about \emph{sigfilter} is available in the section "Pruning the Translation Table" of
\tt{http://www.statmt.org/moses/?n=Moses.AdvancedFeatures}.}
It proved to be capable of pruning out a substantial amount of phrase table with no
significant harm to its quality: authors reported savings up to 90\% and in some cases
even improvement of quality after pruning as measured by BLEU score.
The idea behind significance filtering is that phrase pairs that are weakly supported
by the data or, in other words, more statistically insignificant than others, can be
removed, because it is more likely that they occurrence is either a chance or an artifact
incurred by models used to prepare the parallel data (such as the model creating word
alignments).

% Describe counts necessary for contingency table
Internally, significance filter performs significance testing using two by two
contingency tables that are populated with the following counts (see
\Tref{two-by-two-contingency-table}):
\begin{itemize}
  \item $C(\bar{f},\bar{e})$ as the number of parallel sentences that contain one or more
    occurrences of phrase $\bar{f}$ on source side and phrase $\bar{e}$ on target side
  \item $C(\bar{f})$ as the number of parallel sentences that contain one or more
    occurrences of phrase $\bar{f}$ on the source side
  \item $C(\bar{e})$ as the number of parallel sentences that contain one or more
    occurrences of phrase $\bar{e}$ on the target side
  \item $N$ as the total number of parallel sentences
\end{itemize}

% An example of 2x2 contingency table
\begin{table}[ht]
\centering
\def\arraystretch{1.5}
\begin{tabular}{ | c c | c | }
\hline
$C(\bar{f},\bar{e})$  &  $C(\bar{f}) - C(\bar{f},\bar{e})$  &  $C(\bar{f})$ \\
$C(\bar{e}) - C(\bar{f},\bar{e})$  &  $N - C(\bar{f}) - C(\bar{e}) + C(\bar{f},\bar{e})$  & $N - C(\bar{f})$ \\
\hline
$C(\bar{e})$  &  $N - C(\bar{e})$  &  $N$ \\
\hline
\end{tabular}
\caption{\label{two-by-two-contingency-table}
Two by two contingency table for $\bar{f}$ and $\bar{e}$.}
\end{table}

% Define p-value, mention Fisher's exact test
\emph{Significance test} allows to statistically assess the importance of association
represented by the contingency table by calculating the probability (formally denoted
as \emph{p-value}) that the observed contingency table or any more extreme one could
occur by chance assuming the model of independence: thus, the lower the p-value of
particular table the more significant the represented association.
Several significance tests exist, but many depend on asymptotic assumptions
that are not valid for small counts, therefore significance filtering employs
\emph{Fisher's exact test} that calculates \emph{p-value} using hypergeometric
distribution and is valid for all sample sizes.

% Put phrase table filtering in perspective to significance testing
In case of significance filtering the co-occurrence of phrases $\bar{f}$ and $\bar{e}$
forming a particular phrase pair $(\bar{f},\bar{e})$ is the association being tested
and the independence of model simply presumes that there is no explicit dependence
between phrases from source and target side.
% TODO: Does the above make sense? :)

To prune the phrase table a threshold marking the maximum p-value must be given.
Because the probabilities are very small numbers, their negative logarithm is used
instead, and as a consequence the threshold has to be a positive real number with
bigger values meaning more pruning.

% 1-1-1 phrase pairs and related a-e and a+e thresholds
The significance filter has no predefined thresholds with an exception for a special
(but frequent) case of phrase pairs with $C(\bar{f},\bar{e}) = C(\bar{f}) = C(\bar{e}) = 1$.
The p-value of contingency table of these \emph{1-1-1 phrase pairs} under Fisher's
exact test is $1/N$ and by setting $\alpha = log(N)$ and picking $\epsilon$ such that
$0 < \epsilon \ll 1$, we can define two useful thresholds:
\begin{itemize}
  \item $\alpha + \epsilon$ is the smallest threshold that results in complete removal
    of 1-1-1 phrase pairs
  \item $\alpha - \epsilon$ is the largest threshold that results in complete preservation
    of 1-1-1 phrase pairs
\end{itemize}

Moreover, since the contingency table for 1-1-1 phrase pairs has the lowest p-value
from all contingency tables with $C(\bar{f},\bar{e}) = 1$, setting pruning threshold
to $\alpha + \epsilon$ results in removal of all phrase pairs occurring exactly once.

% The cutoff limit
In addition to setting the pruning threshold for p-values, the \emph{sigfilter} tool
allows also to prune all but top $K$ translation options of each source phrase,
$K$ being referred to as \emph{cutoff} limit.\footnote{In other words \emph{sigfilter}
also implements \emph{histogram pruning} method.}
The combination of both is also possible: first, the cutoff limit is applied leaving
only top $K$ options; then, remaining phrase pairs are pruned with respect to the
p-value threshold.

% TODO: Already said in the introduction:
Finally, the important aspect for comparison of significance filtering and the
filtering method described in this work is that due its design, significance
filtering can be only used as post-processing filter and therefore, in contrast to
our method, it does not interfere with the process of phrase table creation in any way.

\subsection{Relative entropy pruning}
\label{sec:relent-pruning}
% Introduce phrase table filtering by Ling.

% Sketch the problem.
One of the motivations behind the move from word-based to phrase-based models
is that phrase pairs allow to capture contextual information that contributes
to more precise translation.
However, a typical phrase table will contain many phrase pairs that does not
encode any context relevant for phrase translation: often, using a particular
phrase pair in translation process will have a similar effect to instead using
several shorter phrase pairs independent of each other.
As a consequence, such longer phrase pairs can be deemed superfluous and thus
a viable candidates for pruning.

% Introduce the relent filter.
Based on this observation, a phrase table filtering method called \emph{relative
entropy pruning} has been recently proposed by \citet{ling:relentfilter} and
\citet{zens:systcomp}.
This method first establish relative entropy model that measures how likely
a phrase pair encodes a translation event that might be derived from translation
events represented by shorter phrase pairs with the similar overall translation
probability. Then, it applies this model to phrase table pruning, removing
the most redundant translation options.
The reported results show that this method has little negative impact on
translation quality and in some situations can even lead to small improvements,
because the pruning of redundant translation options effectively compress
the search space during decoding and wider spectrum of options is explored.

% Briefly describe the relative entropy metric.
The goal of relative entropy pruning is to deliver the pruned model $\phi'(\bar{e}|\bar{f})$
as similar as possible to the original model $\phi(\bar{e}|\bar{f})$.
To measure models similarity Kullback-Leibler divergence is used: % TODO: Cite?
\begin{equation}
  D(\phi||\phi') = \sum_{\bar{f},\bar{e}}{p(\bar{f},\bar{e}) \Big[log\big(\phi(\bar{e}|\bar{f})\big) - log\big(\phi'(\bar{e}|\bar{f})\big)\Big]}
\end{equation}

For each phrase pair $(\bar{f},\bar{e})$ the deviation between probability emission
from the pruned model and the original model expressed as
$log\big(\phi(\bar{e}|\bar{f})\big) - log\big(\phi'(\bar{e}|\bar{f})\big)$ is weighted
by the frequency that the pair is observed given by $p(\bar{f}, \bar{e})$.

% On computation of p(f,e)
There is no obvious optimal distribution to the weight component $p(\bar{f}, \bar{e})$,
so a simple solution is to just use uniform distribution.
However, \citet{ling:relentfilter} suggest that as the input data size grows, this
approach proves to be inferior to more complex approach of using multinomial distribution
modeled by the fraction:
\begin{equation}
  p(\bar{f}, \bar{e}) = \frac{N(\bar{f},\bar{e})}{N}
\end{equation}
where $N(\bar{f},\bar{e})$ is the number of sentence pairs with phrase $f$ observed on the
source side and $e$ on the target side and $N$ is the total number of sentence pairs
in the parallel data.

% On computation of p'(e|f)
The probability $\phi'(\bar{e}|\bar{f})$ under the pruned model is estimated by using the
decoder to select the best translation options with the phrase pair $(\bar{f},\bar{e})$
being no longer available.
In such a case, the decoder is forced to use shorter phrase pairs to produce the same
translation.
Therefore, the pruned model probability has to account for all possible segmentations
$S_I^K$ of phrase pair $(\bar{f},\bar{e})$ into $K$ "sub-phrase-pairs"
$\bar{e}_k$ and $\bar{f}_{\pi_k}$ with all possible reorderings $\pi_I^K$ determining
the alignment of such "sub-phrase-pairs".
The ultimate approximation of this probability is following:\footnote{See
\citet{zens:systcomp} for a more detailed derivation of this approximation.}
\begin{equation}
  \phi'(\bar{e}|\bar{f}) \approx \max_{S_I^K,\pi_I^K}{\prod_{k=1}^{K} \phi(\bar{e}_k|\bar{f}_{\pi_k})}
\end{equation}

% Note about the "independence of phrase pairs" assumption.
The probability assigned to a phrase pair $(\bar{f},\bar{e})$ by the translation model
may, in general, depend on ordering of phrase pairs within the sentence pair, but from
a computational perspective a search for the best model adhering to such consideration
would be infeasible, because all possible subsets of a given size would have to be evaluated.
Because of this, an assumption that all phrase pairs affect the relative entropy roughly
independently on their context is applied and with this assumption the problem can be
reduced to a local optimization problem that offers a simple criterion for pruning.
Namely, a phrase pair $(\bar{f},\bar{e})$ is pruned if its contribution to the overall
divergence of pruned and original model is below a threshold $\theta_{re}$:
\begin{equation}
  p(\bar{f},\bar{e}) \Big[log\big(\phi(\bar{e}|\bar{f})\big) - log\big(\phi'(\bar{e}|\bar{f})\big)\Big] < \theta_{re}
\end{equation}

% No parametric thresholds here.
The conception of relative entropy pruning does not provide any option to set
the pruning threshold parametrically as in the case of $\alpha \pm \epsilon$ option
available with \emph{significance filtering}.
The typical usage potential of the method lies rather in possibility to prune off
a given portion of a phrase table (eg. 50\% of it), ensuring that the translation model
powered by the pruned table will perform as closely to the original model as possible.

% Mention the relent filter in Moses 0.91
An implementation of relative entropy pruning by \citet{ling:relentfilter} named
\emph{relent-filter} is also part of third-party contributions to Moses toolkit,
but works only with an older version of Moses (release 0.91).\footnote{More information
about \emph{relent-filter} is available in the section "Pruning the Phrase Table based
on Relative Entropy" of Advanced Features chapter of Moses manual:
\tt{http://www.statmt.org/moses/?n=Moses.AdvancedFeatures}.}
% Compare relent filter to sigfilter.
Despite that technically the \emph{relent-filter} acts as a post-filtering tool,
the same way \emph{sigfilter} does, it is more complex to apply, because the weights
of translation model has to be tuned in advance to ensure the soundness of relative
entropy metric.
Because of this prerequisite, time requirements of relative entropy pruning run with
Moses toolkit will be most often far above those of significance pruning as the state of art
methods of translation model tuning are, in general, very time demanding.
