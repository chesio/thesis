\chapter{Results}
\label{chap:results}

\setlength{\epigraphwidth}{1.0\textwidth}
\epigraph{The closer a machine translation is to a professional human translation, the better it is.}{--- Papineni et al., BLEU: a Method for Automatic Evaluation of Machine Translation}

% TODO: A short intro?

\section{The Cs-En dataset}
\label{sec:cs-en-results}

We performed 14 experimental phrase table extractions for the Cs-En dataset,
the comprehensive list is presented by \Tref{cs-en-wmt13-scenarios}.
We included one more optimized baseline with 8 cores, because the difference in
wall clock time necessary to construct the phrase table between runs with 4 and 8 cores
was considerable.
Sorting processes in both optimized baselines were given 18~GB of memory, so the virtual
peak of the entire pipeline reached approximately the same value as in the case of the \eppex{}
run with no pruning.

% Cs-En: description and parameters of experiments
\begin{table}[ht]
\centering
\begin{tabular}{ r p{10cm} }
name & description and parameters \\
\hline
\hline
def-base        & Standard Moses pipeline with no special parameters \\
multi-base      & Standard Moses pipeline with \verb|--cores 4| \\
comp-base       & Standard Moses pipeline with \verb|--sort-compress gzip| and \verb|--cores 4| \\
opt-base        & Standard Moses pipeline with \verb|--sort-buffer 18G|, \verb|--sort-compress gzip| and \verb|--cores 4| \\
opt-c8-base     & Standard Moses pipeline with \verb|--sort-buffer 18G|, \verb|--sort-compress gzip| and \verb|--cores 8| \\
\hline
eppex zero      & \eppex{} set to no pruning and \verb|--GZOutput| option \\
eppex def.      & \eppex{} with \verb|--limits| set to \verb|1-3:0:1,4-5:1:4,6-7:4:8| and \verb|--GZOutput| option \\
eppex 0:n       & \eppex{} with \verb|--limits| set to \verb|1:0:1,2:0:2,...,7:0:7| and \verb|--GZOutput| option \\
eppex 0:n+1     & \eppex{} with \verb|--limits| set to \verb|1:0:2,2:0:3,...,7:0:8| and \verb|--GZOutput| option \\
eppex 1:n+1     & \eppex{} with \verb|--limits| set to \verb|1:1:2,2:1:3,...,7:1:8| and \verb|--GZOutput| option \\
\hline
sigfilter a-e   & baseline followed by significance filtering with pruning threshold $\alpha - \epsilon$ \\
sigfilter a+e   & baseline followed by significance filtering with pruning threshold $\alpha + \epsilon$ \\
sigfilter 30 a+e  & baseline followed by significance filtering with cutoff limit of 30 and pruning threshold $\alpha + \epsilon$ \\
sigfilter 30    & baseline followed by significance filtering with cutoff limit of 30 \\
\hline
\hline
\end{tabular}
\caption{\label{cs-en-wmt13-scenarios}
List of various experiments and their settings for Cs-En setup, "eppex~def." is shortcut for \emph{eppex defensive}.}
\end{table}

The \emph{eppex zero} experiment is performed to establish the maximum memory demands of \eppex{}
for the input data of a given size.
The \emph{eppex defensive} experiment has the positive and negative limits set to achieve the same
level of pruning as the \emph{eppex 1-in} experiment that performed very well in 2011 experiments.
The remaining \eppex{} experiments have a separate Lossy Counter set for every phrase length
from one (single-word phrases) to seven (the same maximum phrase length as extracted by the baseline
system by default).
We set the negative limit to be the same for all instances, but the positive limit is set in correspondence
to the phrase length to allow for a bigger estimation errors for longer phrase pairs.
Full details of \eppex{} configuration options are given in \Cref{chap:usage}.
For example, the \verb|--limits| of the \emph{eppex 1:n+1} configuration allow to drop singleton phrases
of all lengths and ensure that all phrases of length $n$ occurring at least $n+1$ times will be preserved.

\subsection{Translation phrase table size and quality}

\Tref{cs-en-wmt13-pt-size-and-bleu} presents the phrase table sizes and BLEU scores for all
distinct phrase tables created in our experiments.
Despite the fact that the phrase table sizes are very different, with the most pruned phrase table being
only $1/13$ size of the baseline, the achieved BLEU scores are very tight and the difference
between the best and the worst score is only 0.66~point in the case of \emph{news-test2012} test data
and 0.61~point in the case of \emph{news-test2013} test data.

% Cs-En: phrase tables sizes and BLEU scores
\begin{table}[ht]
\centering
\begin{tabular}{ | c | c c | c c | }
\hline
 & \multicolumn{2}{|c|}{final phrase table size} & \multicolumn{2}{|c|}{BLEU score} \\
experiment & phrase pairs & .gz file size & wmt-12 & wmt-13 \\
\hline
\hline
baseline          & 336.0~M & 8.8~GB & 23.27 & 25.83 \\
sigfilter 30      & 301.9~M & 8.2~GB & 23.12 & 25.62 \\
sigfilter a-e     & 203.1~M & 5.9~GB & 23.01 & 25.60 \\
eppex def.        & 109.8~M & 2.7~GB & \textbf{23.38} & \textbf{25.98} \\
eppex 0:n         &  86.2~M & 2.3~GB & 23.24 & 25.86 \\
sigfilter a+e     &  70.0~M & 1.9~GB & 23.21 & 25.59 \\
eppex 0:n+1       &  67.9~M & 1.8~GB & 23.04 & 25.61 \\
sigfilter 30 a+e  &  60.2~M & 1.7~GB & 23.05 & 25.63 \\
eppex 1:n+1       &  25.7~M & 0.7~GB & 22.72 & 25.37 \\
\hline
\end{tabular}
\caption{\label{cs-en-wmt13-pt-size-and-bleu}
Phrase table sizes and BLEU scores for the various experiments of the Cs-En setup.}
\end{table}

The \emph{eppex defensive} experiment again proved to be very competitive:
the phrase table has only $1/3$ size of the baseline,
but the obtained BLEU score was higher for both the test sets (although not significantly).
On the other hand, the pruning of all singletons in the \emph{eppex 1:n+1} experiment
resulted in the worst scores on both sets and therefore seems already too harsh for
the Cs-En setup.

Interestingly, there is no clear differentiation of what was the best \emph{sigfilter}
configuration: especially, in the case of \emph{news-test2013} test data all the scores
happen to occur within a tiny range of $[25.59, 25.63]$, despite the phrase table sizes
differ significantly, from 60.2~M to 301.9~M phrase pairs.

\subsection{Memory and time requirements}

\Tref{cs-en-wmt13-time-benchmarks} presents the amount of time necessary to finish
phrase table extraction with various systems and their configurations and also
the measured CPU consumption.

% Cs-En: baseline and eppex wall clock and CPU time values
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r r | r r | r r | }
\hline
 & \multicolumn{2}{|c|}{total time} & \multicolumn{2}{|c|}{extraction} & \multicolumn{2}{|c|}{scoring} \\
experiment & wall & CPU & wall & CPU & wall & CPU \\
\hline
\hline
def-base      & 15.6 & 15.5 & 7.1 & 6.5 & 8.5 & 9.0 \\
multi-base    & 10.4 & 19.4 & 4.9 & 6.7 & 5.5 & 12.7 \\
comp-base     & 10.6 & 25.4 & 4.1 & 11.3 & 6.4 & 14.1 \\
opt-base      & 8.3 & 19.9 & 2.6 & 7.0 & 5.6 & 13.0 \\
opt-c8-base   & 7.0 & 20.7 & 2.3 & 7.2 & 4.7 & 13.6 \\
eppex zero    & 2.9 & 2.9 & -- & -- & -- & -- \\
\hline
eppex def.    & 1.6 & 1.6 & -- & -- & -- & -- \\
eppex 0:n     & 1.5 & 1.5 & -- & -- & -- & -- \\
eppex 0:n+1   & 1.4 & 1.4 & -- & -- & -- & -- \\
eppex 1:n+1   & 1.2 & 1.2 & -- & -- & -- & -- \\
\hline
\end{tabular}
\caption{\label{cs-en-wmt13-time-benchmarks}
Wallclock times and CPU usage values (in hours) of phrase table
construction for the various experiments of the Cs-En setup.}
\end{table}

Even with no pruning \eppex{} is capable of doing the phrase table construction more
than twice as fast as the most challenging baseline (and using 7 times less CPU time).
When compared to the default baseline the difference is even more pronounced with \eppex{}
being 5~times as fast.

From the comparison of the default and multi-core baseline it seems clear that whenever there
is a possibility of employing multiple cores it should be taken: just by running with 4 cores
the baseline execution time has been cut to $2/3$.
Doubling the number of cores from 4 to 8 in optimized experiments resulted in a further reduction
of execution time, although not as significant (by 15\%).

An important observation is that adding only the option to make the \texttt{sort} program gzip
its temporary data, did not help decrease the total execution time.
More precisely, it did help in the phrase extraction phase (ca. $-0.8~h$), but did the very opposite
in the scoring phase (ca. $+0.9~h$), so the overall impact on the total wall clock time was eventually
marginally negative (ca. $+0.2~h$).

A reasonable explanation for such, perhaps unexpected, behavior stems from the fact that in the phrase
extraction phase both phrase table halves are sorted simultaneously, whereas in the scoring phase
only the indirect half is sorted.
Apparently, in our environment forcing \texttt{sort} to gzip its temporary data makes it run
slower when there are no other processes accessing the disk (as in the scoring phase), but reducing
disk access by the same means when two similar sort tasks run simultaneously can benefit
in both of them finishing faster (as in the phrase extraction phase).
The striking increase of CPU consumption in the \emph{comp-base} experiment, that can only be
attributed to the extra gzipping, suggests that the sorting processes had to gzip a lot of
temporary data:
phrase extraction required almost 4.5 CPU hours more, scoring required almost 1.5 CPU hours
more.\footnote{Given that there are two sorting processes in the phrase extraction phase and
only one in the phrase scoring phase, one could expect the ratio
4.5:1.5 of additional CPU time to be closer to 2:1, but after phrase extraction, phrase table halves
contain every phrase pair occurrence, whereas after scoring they contain only unique phrase pairs,
so the size of data to sort is rather closer to 3:1 than 2:1 ratio.}
Similar conclusion might be made also by an examination of partial wall clock times of the \emph{multi-base}
and \emph{opt-base} experiments: while the phrase extraction time fell from 4.9~hours to 2.6~hours
(i.e. almost to $1/2$), the scoring time jumped up a marginally from 5.5~hours to 5.6~hours.
Thus, even the significantly increased sorting buffer managed only to neutralize the negative impact of
temporary data compression on the wall clock time of the scoring phase, but pronounced even more
the positive impact of the same settings on the phrase extraction phase.

However, to put things in a wider perspective, we believe that the option to compress temporary data of
the \texttt{sort} routine was introduced in order to limit the peak usage of disk space, which may
otherwise pose a limiting factor in some environments, as the figures presented in the following text
demonstrate.

\Tref{cs-en-wmt13-vm-and-disk-usage-peaks} illustrates both memory and disk space demands of the baseline
and \eppex{} experiments.

% Cs-En: virtual memory and disk usage peaks
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r l | r | }
\hline
 & \multicolumn{2}{|c|}{VM peak} & \\
experiment & size & step & du peak \\
\hline
\hline
def-base       &  2.0~GB &    scoring & 206.7~GB \\
multi-base     &  6.2~GB &    scoring & 213.5~GB \\
comp-base      &  6.2~GB &    scoring &  43.4~GB \\
opt-base       & 36.2~GB & extraction &  42.8~GB \\
opt-c8-base    & 36.2~GB & extraction &  42.8~GB \\
eppex zero     & 36.8~GB &      eppex &   8.8~GB \\
\hline
eppex def.     & 18.1~GB &      eppex &   2.7~GB \\
eppex 0:n      & 10.5~GB &      eppex &   2.2~GB \\
eppex 0:n+1    &  8.4~GB &      eppex &   1.8~GB \\
eppex 1:n+1    &  9.6~GB &      eppex &   0.7~GB \\
\hline
\end{tabular}
\caption{\label{cs-en-wmt13-vm-and-disk-usage-peaks}
Virtual memory and disk usage peaks of the phrase table construction for various experiments of Cs-En setup.}
\end{table}

The default baseline experiment had the lowest memory demand of all experiments, it needed only 2~GB
of memory to finish.
With non-default settings the memory consumption tripled when multiple cores were utilized (in \emph{multi-base}
and \emph{comp-base} experiments), except for cases when it was dominated by the size of buffer dedicated
to the \texttt{sort} routine (in optimized experiments).\footnote{Because of the parallel run of sort processes
in the phrase extraction step, the virtual memory peak is approximately twice the size of the buffer.
It is quite important to keep that in mind, when training the baseline system.}

\Eppex{} memory demands are largely dependent on the amount of pruning requested: with \emph{zero}
pruning almost 37~GB of memory was consumed, mild \emph{defensive} pruning required only half as much
memory and harsh \emph{1:n+1} pruning only $1/4$ (but still almost 10~GB).

The comparison of VM peaks and phrase table sizes of \emph{eppex 0:n+1} and \emph{eppex 1:n+1}
experiments confirms the findings from \Sref{sec:lossy-counting-applicability}:
because of the harsher limits, \emph{eppex 1:n+1} produced a half as big phrase table as \emph{eppex 0:n+1},
but since it had a smaller estimation error, it required 15\% more memory.

The disk usage peak exceeded 200~GB in the baseline experiments without compression of
\texttt{sort} temporary data, with compression it fell to approximately 40~GB
(notably still above the memory peak of \eppex{} with no pruning).
As noted before, \eppex{} produces no temporary data, therefore in all experiments the
disk usage peaks strictly copied the size of the produced phrase tables.

Finally, \Tref{cs-en-wmt13-sigfilter-runtime-benchmarks} presents benchmarking
figures for all configurations of the \texttt{sigfilter} tool.
Even a simple histogram pruning required more time than any pruning with
\eppex{}.\footnote{This is partially due a design flaw in the \texttt{sigfilter} tool:
a SALM-indexed corpus is always required to proceed, even though it is not necessary in
the case of histogram pruning. The unnecessary SALM indexing accounts for a large part
of the reported wall clock time.}
On the other hand, the significance pruning consumed considerably less memory than any
of the epochal extractions performed.

% Cs-En: sigfilter runtime requirements
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r r | r l | }
\hline
 & \multicolumn{2}{|c|}{time} & \multicolumn{2}{|c|}{VM peak} \\
experiment & wall & CPU & size & step \\
\hline
\hline
sigfilter a-e     & 10.2~h & 11.3~h & 6.7~GB & filtering \\
sigfilter a+e     & 9.9~h & 10.6~h & 6.8~GB & filtering \\
sigfilter 30 a+e  & 8.1~h & 8.7~h & 6.7~GB & filtering \\
sigfilter 30      & 2.5~h & 3.4~h & 2.7~GB & indexing \\
\hline
\end{tabular}
\caption{\label{cs-en-wmt13-sigfilter-runtime-benchmarks}
Benchmarking figures for various settings of \emph{sigfilter} in the Cs-En setup:
wallclock time, CPU usage value and virtual memory peak with the phase in which it occurred.}
\end{table}

%Also an interesting observation is that the optimized baseline phrase table construction
%(\emph{opt-c8-base}) took one hour less then the fastest significance filtering
%(\emph{sigfilter 30 a+e}).
% Not that much interesting ;)

\section{The Fr-En dataset}
\label{sec:fr-en-results}

\Tref{fr-en-80-scenarios} presents the list of the basic experimental phrase table extractions
performed on the Fr-En dataset.
In addition to these basic experiments, we ran discounting variants with either
\emph{Good-Turing} or \emph{Kneser-Ney} discounting \citep[Chapter 6]{manning:stat-nlp},
the full list is shown in \Tref{fr-en-80-smoothed-scenarios}.

% Fr-En: description and parameters of experiments
\begin{table}[ht]
\centering
\begin{tabular}{ r p{10cm} }
name & description and parameters \\
\hline
\hline
opt-base      & Standard Moses pipeline with \verb|--sort-buffer 100G|, \verb|--sort-compress gzip| and \verb|--cores 4| \\
opt-c8-base   & Standard Moses pipeline with \verb|--sort-buffer 100G|, \verb|--sort-compress gzip| and \verb|--cores 8| \\
eppex zero    & \eppex{} set to no pruning and \verb|--GZOutput| option \\
eppex def.    & \eppex{} with \verb|--limits| set to \verb|1-3:0:1,4-5:1:4,6-7:4:8| and \verb|--GZOutput| option \\
eppex 0:n     & \eppex{} with \verb|--limits| set to \verb|1:0:1,2:0:2,...,7:0:7| and \verb|--GZOutput| option \\
eppex 1:n+1   & \eppex{} with \verb|--limits| set to \verb|1:1:2,2:1:3,...,7:1:8| and \verb|--GZOutput| option \\
eppex 1:n+2   & \eppex{} with \verb|--limits| set to \verb|1:1:3,2:1:4,...,7:1:9| and \verb|--GZOutput| option \\
eppex 2:n+2   & \eppex{} with \verb|--limits| set to \verb|1:2:3,2:2:4,...,7:2:9| and \verb|--GZOutput| option \\
\hline
\hline
\end{tabular}
\caption{\label{fr-en-80-scenarios}
List of the basic experiments and their settings for the Fr-En setup.}
\end{table}

In the case of baseline experiments, we had attempted all the configurations mentioned
in \Sref{sec:baseline-experiments}, but managed to only finish the optimized ones
that allowed \texttt{sort} to use 100~G of memory.
The non-optimized attempts usually died in the middle of sorting with \texttt{sort} complaining
about missing temporary files.
We did not investigate the exact reason, but our raw guess is that the number of temporary
files reached some system-imposed limit, as there were thousands of them left in
the working directory after the sorting process died.

The evaluated \eppex{} configurations are basically the same as in the case of the Cs-En setup,
only that we included an additional, harsher configuration \emph{eppex 2:n+2}, in which all phrase
pairs with true frequency below~3 were pruned.

% Fr-En: smoothed experiments
\begin{table}[ht]
\centering
\begin{tabular}{ r p{10cm} }
name & description and parameters \\
\hline
\hline
eppex zero GT & \emph{eppex zero} with \verb|--GoodTuring| option \\
eppex zero KN & \emph{eppex zero} with \verb|--KneserNey| option \\
eppex def. GT & \emph{eppex defensive} with \verb|--GoodTuring| option \\
eppex def. KN & \emph{eppex defensive} with \verb|--KneserNey| option \\
eppex 0:n GT  & \emph{eppex 0:n} with \verb|--GoodTuring| option \\
eppex 0:n KN  & \emph{eppex 0:n} with \verb|--KneserNey| option \\
eppex 1:n+1 GT & \emph{eppex 1:n+1} with \verb|--GoodTuring| option \\
eppex 1:n+1 KN & \emph{eppex 1:n+1} with \verb|--KneserNey| option \\
\hline
\hline
\end{tabular}
\caption{\label{fr-en-80-smoothed-scenarios}
List of the Fr-En experiments run with discounting.}
\end{table}

\subsection{Translation phrase table size and quality}

\Tref{fr-en-pt-size-and-bleu} presents phrase table sizes and BLEU scores for all
distinct phrase tables created in the basic experiments.
The baseline phrase table contained an incredible 1.7~billion phrase pairs,
while the smallest phrase table produced by the harshest \eppex{} configuration
was less than 5\% of that size.
Despite this huge contrast in phrase table sizes, the obtained BLEU scores
were even tighter than in the case of the Cs-En setup;
the differences between the best and the worst experiments were only 0.28~points
and 0.37~points in the case of the \emph{news-test2012} and \emph{news-test2013} sets
respectively.
In fact, such small differences could be also attributed to the randomness of
MERT.\footnote{We did not invest the computing resources necessary to estimate
the confidence bounds covering optimizer instability \citep{clark:optimizerinstability}.}

% Fr-En: phrase tables sizes and BLEU scores
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r r | c c | }
\hline
 & \multicolumn{2}{|c|}{final phrase table size} & \multicolumn{2}{|c|}{BLEU score} \\
experiment & phrase pairs & .gz file size & wmt-12 & wmt-13 \\
\hline
\hline
baseline          & 1782.0~M & 42.5~GB & 28.60 & 29.58 \\
sigfilter 30      & 1470.8~M & 35.9~GB & 28.49 & 29.64 \\
sigfilter a-e     &  811.4~M & 21.0~GB & 28.45 & 29.53 \\
eppex def.        &  463.4~M & 10.6~GB & \textbf{28.67} & 29.50 \\
sigfilter a+e     &  329.5~M &  8.4~GB & 28.66 & \textbf{29.77} \\
eppex 0:n         &  283.2~M &  7.0~GB & 28.41 & 29.50 \\
sigfilter 30 a+e  &  270.0~M &  7.0~GB & 28.57 & 29.76 \\
eppex 1:n+1       &  127.9~M &  3.2~GB & 28.59 & 29.53 \\
eppex 1:n+2       &  105.8~M &  2.7~GB & 28.56 & 29.40 \\
eppex 2:n+2       &   77.0~M &  2.0~GB & 28.39 & 29.43 \\
\hline
\end{tabular}
\caption{\label{fr-en-pt-size-and-bleu}
Phrase table sizes and BLEU scores for the basic experiments on the Fr-En setup.
The \emph{eppex zero} configuration is not included, because the produced phrase table
was the same as that of the \emph{baseline}.}
\end{table}

The \emph{eppex defensive} configuration scored best on the \emph{news-test2012} set,
but was closely followed by the \emph{sigfilter a+e} configuration that scored best on
the \emph{news-test2013} set.
The slightly harsher variation \emph{sigfilter 30 a+e}, that not only pruned out
all of the single-occurring phrase pairs (singletons), but also performed histogram pruning prior to
significance pruning, performed nearly as well, indicating that the removal of
singletons did not hurt the quality of the baseline Fr-En translation model.
This observation is further supported by the fact, that amongst the \eppex{} configurations,
the \emph{1:n+1} configuration that removed all singletons too, ranked as good as
the \emph{defensive} configuration, although it produced a more than 3~times smaller phrase table.
This suggests that in a gigaword parallel corpus all reasonable words already appear twice or more.

The BLEU scores obtained with translation models that had phrase translation probabilities
adjusted by either Good-Turing or Kneser-Ney discounting are shown in \Tref{fr-en-smoothed-bleu}.
The phrase tables created with the \emph{eppex zero} configuration are basically the
baseline ones.

% Fr-En: BLEU scores of smoothed experiments
\begin{table}[ht]
\centering
\begin{tabular}{ | c | c c c | c c c | }
\hline
 & \multicolumn{3}{|c|}{wmt12} & \multicolumn{3}{|c|}{wmt13} \\
experiment & none & GT & KN & none & GT & KN \\
\hline
\hline
eppex zero    & 28.60 & 28.67 & \textbf{28.85} & 29.58 & 29.73 & \textbf{29.95} \\
eppex def.    & 28.67 & 28.71 & \textbf{28.85} & 29.50 & 29.87 & 29.67 \\
eppex 0:n     & 28.41 & 28.71 & 28.63 & 29.50 & 29.55 & 29.54 \\
eppex 1:n+1   & 28.59 & 28.65 & \emph{28.53} & 29.53 & 29.56 & 29.78 \\
\hline
\end{tabular}
\caption{\label{fr-en-smoothed-bleu}
BLEU scores for the discounting experiments on the Fr-En setup.
The "none" columns show the scores for runs without discounting, "GT" and "KN" columns
present scores obtained with Good-Turing and Kneser-Ney discounting respectively.}
\end{table}

The discounting clearly improved the performance of the translation models,
higher BLEU scores were observed in all the evaluations except the \emph{eppex 1:n+1 KN}
configuration with the \emph{news-test2012} set.
The highest improvement rates were observed in the baseline experiments (i.e. with unfiltered
phrase tables), in a way confirming observations of \citet{johnson:sigfilter}:
they had noticed that significance filtering of phrase tables can sometimes actually raise
the BLEU score, but the effect is less prominent, if discounting is employed.
The obtained results suggest that this observation holds for lossy-counted pruning as well.

\subsection{Memory and time requirements}

The wall clock times and CPU consumption of all the basic experiments are shown
in \Tref{fr-en-time-benchmarks}.

% Fr-En: baseline and eppex wall clock and CPU time values
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r r | r r | r r | }
\hline
 & \multicolumn{2}{|c|}{total time} & \multicolumn{2}{|c|}{extraction} & \multicolumn{2}{|c|}{scoring} \\
experiment & wall & CPU & wall & CPU & wall & CPU \\
\hline
\hline
opt-base      & 56.8 & 146.1 & 15.0 & 39.2 & 41.8 & 106.9 \\
opt-c8-base   & 45.1 & 149.8 & 12.7 & 39.7 & 32.4 & 110.1 \\
eppex zero    & 25.5 &  25.4 & -- & -- & -- & -- \\
\hline
eppex def.    & 12.2 & 12.2 & -- & -- & -- & -- \\
eppex 0:n     & 10.7 & 10.7 & -- & -- & -- & -- \\
eppex 1:n+1   & 9.5 & 9.5 & -- & -- & -- & -- \\
eppex 1:n+2   & n/a & n/a & -- & -- & -- & -- \\
eppex 2:n+2   & 9.3 & 9.2 & -- & -- & -- & -- \\
\hline
\end{tabular}
\caption{\label{fr-en-time-benchmarks}
Wallclock times and CPU usage values (in hours) of the phrase table
construction for various experiments on the Fr-En setup.
The values of the \emph{eppex 1:n+2} experiment were not properly measured.}
\end{table}

Again, \eppex{} was able to build the unpruned phrase table in almost twice
as short a time than the optimized baseline that run in parallel on 8~cores
and used approximately the same amount of memory, as shown in \Tref{fr-en-vm-and-disk-usage-peaks}.

Since we only managed to accomplish the baseline extraction with the optimized
configuration, the virtual memory peaks of baseline experiments were dominated by
the size of memory buffer dedicated to the \texttt{sort} program.
The epochal extraction without pruning consumed more than 200~GB of the memory,
which was less than a half of the RAM available on the machine used,
but is a very considerable amount otherwise.
However, even mild \emph{defensive} pruning cut this requirement to $1/2$.

% Fr-En: virtual memory and disk usage peaks.
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r l | r | }
\hline
 & \multicolumn{2}{|c|}{VM peak} & \\
experiment & size & step & du peak \\
\hline
\hline
opt-base       & 200.1~GB & extraction & 160.5~GB \\
opt-c8-base    & 200.1~GB & extraction & 160.5~GB \\
eppex zero     & 203.0~GB &      eppex &  48.0~GB \\
\hline
eppex def.     &  89.9~GB &      eppex &  16.0~GB \\
eppex 0:n      &  48.7~GB &      eppex &   8.0~GB \\
eppex 1:n+1    &  46.4~GB &      eppex &   4.0~GB \\
eppex 1:n+2    &  37.7~GB &      eppex &   4.0~GB \\
eppex 2:n+2    &  45.6~GB &      eppex &   2.0~GB \\
\hline
\end{tabular}
\caption{\label{fr-en-vm-and-disk-usage-peaks}
Virtual memory and disk usage peaks of the phrase table construction for various experiments on the Fr-En setup.}
\end{table}

The monitored disk usage peaks of \eppex{} experiments were rounded up
to multiples of two for some unknown reason, but this does not affect
the overall picture.
Despite being given a substantial memory buffer, the baseline still
required considerable temporary disk space, almost four times the
actual phrase table size (42.5~GB).

Finally, \Tref{fr-en-sigfilter-runtime-benchmarks} presents the runtime benchmarking
values for different configurations of the \texttt{sigfilter} tool.\footnote{Due a tight
experiment schedule we run the \emph{sigfilter} experiments on the Fr-En setup on a set of
machines designated for the Cs-En setup.}
The observations are very similar to the corresponding ones in the Cs-En setup.
Again, no significance filtering took less time than the optimized baseline phrase table
construction (\emph{opt-c8-base}) and despite the fact that the memory demands increased
significantly, they were again below any of those of \eppex{}.

% Fr-En: sigfilter runtime requirements
\begin{table}[ht]
\centering
\begin{tabular}{ | c | r r | r l | }
\hline
 & \multicolumn{2}{|c|}{time} & \multicolumn{2}{|c|}{VM peak} \\
experiment & wall & CPU & size & step \\
\hline
\hline
sigfilter a-e     & 82.7~h & 87.9~h & 28.2~GB & filtering \\
sigfilter a+e     & 83.9~h & 87.2~h & 28.5~GB & filtering \\
sigfilter 30 a+e  & 46.9~h & 50.3~h & 28.4~GB & filtering \\
sigfilter 30      & 10.3~h & 13.8~h & 11.5~GB & indexing \\
\hline
\end{tabular}
\caption{\label{fr-en-sigfilter-runtime-benchmarks}
Benchmarking figures for various settings of \emph{sigfilter} in the Fr-En setup:
wallclock time, CPU usage value and virtual memory peak with the phase in which it occurred.}
\end{table}

\section{Comparison to the 2011 version}
\label{sec:cu-bojar-results}
% Comparison of current eppex and phrase-extract performance vs. mid-2011 state

\citet{przywara:eppex} presented an evaluation of an early version of \eppex{}
that was only able to extract phrases. It performed a faster phrase
table creation than the legacy \emph{phrase-extract} in such situations,
where lossy counting resulted in significant filtration of extracted phrase
pairs and the consequent sub-steps (sorting, scoring and consolidation of both
phrase table halves) had to process a reduced amount of data;
with the result that the whole process of phrase table creation finished faster.

As we had all the parallel data used in their experiments available and
we performed our runtime benchmarking in an almost identical manner,
we decided to carry out a comparison between the early and current versions of
both versions of \eppex{} as well as the \emph{phrase-extract} toolkit.

\subsection{Implementation differences}

The current version of \eppex{} differs from the early version mainly in its
capability of performing not only phrase pair extraction, but a complete
phrase table construction (that is including phrase pair scoring) and even
Good-Turing and Kneser-Ney discounting.
The early version, however, could be used to extract orientation info
the same way \emph{extract} can, while this functionality has been dropped
from the current version as not related to the core task of phrase table creation.
Following the upgrade of the \emph{phrase-extract} suite, an option to read from
gzipped input files and dump gzipped output files has been added to \eppex{}.
Internally, there have been a multitude of performance tweaks,
as performance optimization is the most important aim in \eppex{} development,
but a full listing of implementation changes would be purposeless here.

On the contrary, the \emph{phrase-extract} suite has been updated mainly in order to
provide a richer functionality, but three changes since the mid-2011 version
had a significant impact on its runtime performance: the optimization of target phrase
scanning in \emph{scorer} implementation,\footnote{Commit 677378774aca30c8f0d4ca57267f7ac5ef7d7cb6.}
adding an option to gzip the output directly within the main three binaries
(\emph{extract}, \emph{scorer} and \emph{consolidate})
and the parallelization of both phrase extraction and phrase scoring
phases.\footnote{See \texttt{extract-parallel.perl} and \texttt{score-parallel.perl}
in \texttt{<moses>/scripts/generic/}.}

\subsection{Parameters of experiments}

\Tref{cu-bojar-scenarios} presents all the experiments and their settings.
Again, we have included one more optimized baseline with 7 cores, because
the acceleration achieved is considerable.
The \emph{default baseline} is invoked with the same parameters as the \emph{2011
baseline}, except for an option that turns on gzipping of temporary data and
the phrase table that is now implicitly activated from within the training script.
Because of this, we also turn \verb|--GZOutput| on for all \eppex{} runs.
We experimented with different pruning parameters: \emph{eppex-1-in} (milder
pruning) and \emph{eppex-1-out} (harsher pruning) use the same pruning
parameters as the 2011 experiments and we also performed an \emph{eppex zero}
experiment (an \eppex{} run with no pruning).

% Detailed info of cu-bojar experiments
\begin{table}[ht]
\centering
\begin{tabular}{ r p{10cm} }
name & description and parameters \\
\hline
\hline
def-base        & Standard Moses pipeline with no special parameters \\
multi-base      & Standard Moses pipeline with \verb|--cores 4| \\
comp-base       & Standard Moses pipeline with \verb|--sort-compress gzip|
  and \verb|--cores 4| \\
opt-base        & Standard Moses pipeline with \verb|--sort-buffer 12G|,
  \verb|--sort-compress gzip| and \verb|--cores 4| \\
opt-c7-base     & Standard Moses pipeline with \verb|--sort-buffer 12G|,
  \verb|--sort-compress gzip| and \verb|--cores 7| \\
eppex zero      & \eppex{} set to no pruning and \verb|--GZOutput| option \\
eppex 1-in      & \eppex{} with pruning thresholds set to keep in
  all phrase pairs of length 1--3 and prune longer phrase pairs
  with max. positive threshold of 8 and \verb|--GZOutput| option \\
eppex 1-out     & \eppex{} with pruning thresholds set to remove
  all single-occurring phrase pairs and prune the rest with
  max. positive threshold of 8 and \verb|--GZOutput| option \\
\hline
\hline
\end{tabular}
\caption{\label{cu-bojar-scenarios}List of various experiments and their
settings for the "cu-bojar" setup. The parameters of "eppex 1-in" and "eppex 1-out"
conform to the corresponding 2011 experiments, "def-base" corresponds to the 2011
"baseline" experiment except for (now activated implicitly) gzipping of
temporary data.}
\end{table}

\subsection{Memory and time requirements}

% NOTE:
% Steps and substeps of phrase table construction via train-model.perl back in 2011:
% (1) Phrase extraction
%  a) extract
%  b) gzip f2e
%  c) gzip e2f
% (2) Phrase scoring
%  d) sort f2e (now done in (1) and so reported as part of phrase extraction in table below)
%  e) score e2f
%  f) sort e2f (now done in (1) and so reported as part of phrase extraction in table below)
%  g) score e2f
%  h) sort inv
%  i) cons
%  j) gzip pt

\Tref{cu-bojar-time-benchmarks} compares wallclock times and CPU usage of all the
experiments and in the case of the baseline and 2011 experiments also separately for phrase
extraction and phrase scoring phases.

The comparison between the old and current baselines reveals that the optimization of
\emph{scorer} mentioned above resulted in an increase in speed of more than a factor of two.
On the other hand default phrase extraction became slightly more time consuming,
but this may be explained by the fact that parallelization incurred some overhead
that does not pay back when running only with a single core, but significantly cuts
down the running time when using multiple cores: in our setup using 7~cores (along with
more memory for sorting) lowered the time of phrase extraction to 40\% and of phrase scoring
to almost 50\% of respective default measures.

The comparison between 2011 and the current version of \eppex{} also displays a significant
increase in speed: with harsh pruning the phrase table construction is done in half an hour (took
almost two hours in 2011) and without any pruning the full phrase table was built in
less than hour and half.

\begin{table}[!htb]
\centering
\begin{tabular}{ | c | r r | r r | r r | }
\hline
 & \multicolumn{2}{|c|}{total time} & \multicolumn{2}{|c|}{extraction} & \multicolumn{2}{|c|}{scoring} \\
experiment & wall & CPU & wall & CPU & wall & CPU \\
\hline
\hline
baseline*     & 8.8 & 7.1 & 2.1 & 1.0 & 6.7 & 6.1 \\
def-base      & 5.6 & 4.8 & 2.5 & 1.5 & 3.1 & 3.2 \\
multi-base    & 4.3 & 5.7 & 2.2 & 1.5 & 2.1 & 4.1 \\
comp-base     & 3.3 & 7.1 & 1.1 & 2.5 & 2.3 & 4.6 \\
opt-base      & 3.2 & 6.0 & 1.1 & 1.7 & 2.1 & 4.3 \\
opt-c7-base   & 2.8 & 6.0 & 1.1 & 1.6 & 1.7 & 4.3 \\
eppex zero    & 1.4 & 1.4 & -- & -- & -- & -- \\
\hline
eppex 1-in*   & 4.1 & 3.7 & 1.8 & 1.5 & 2.2 & 2.2 \\
eppex 1-in    & 0.8 & 0.8 & -- & -- & -- & -- \\
\hline
eppex 1-out*  & 1.8 & 1.6 & 1.6 & 1.4 & 0.2 & 0.2 \\
eppex 1-out   & 0.5 & 0.5 & -- & -- & -- & -- \\
\hline
\end{tabular}
\caption{\label{cu-bojar-time-benchmarks}
Wallclock times and CPU usage values (in hours) of phrase table
construction for various experiments of the "cu-bojar" setup.
An asterisk denotes measurements from the 2011 experiments.}
\end{table}

\Tref{cu-bojar-vm-and-du-peaks} presents memory and disk usage peaks of all the experiments
(except for disk usage peaks that are not available for the 2011 experiments).

\begin{table}[ht]
\centering
\begin{tabular}{ | c | r | r | }
\hline
experiment & VM peak & du peak \\
\hline
\hline
baseline*     &  1.1~GB &      -- \\
def-base      &  1.2~GB & 51.8~GB \\
multi-base    &  3.9~GB & 53.6~GB \\
comp-base     &  3.9~GB & 14.3~GB \\
opt-base      & 24.1~GB & 43.7~GB \\
%opt-c7-base   & 24.1~GB & 43.7~GB \\
eppex zero    & 16.8~GB &  3.7~GB \\
\hline
eppex 1-in*   & 19.2~GB &      -- \\
eppex 1-in    & 13.4~GB &  1.3~GB \\
\hline
eppex 1-out*  & 16.7~GB &      -- \\
eppex 1-out   & 11.3~GB &  0.3~GB \\
\hline
\end{tabular}
\caption{\label{cu-bojar-vm-and-du-peaks}
Virtual memory and disk usage peaks in phrase table construction for various experiments
of the "cu-bojar" setup. An asterisk denotes measurements from the 2011 experiments.}
\end{table}

\newpage

Memory demands of the default baseline remain low, the 0.1~GB difference between 2011 and now
stems from the fact that our benchmarking script includes the main process in the set of
measured processes.

Memory demands of the current version of \eppex{} are significantly lower than of the 2011 version:
in 2011, the harsh pruning setup required as much memory as the current version run without
any pruning at all.

The disk usage peak is significantly reduced in the \emph{compressed} baseline, but rises almost to
default levels in optimized baselines, despite the fact that the \verb|--sort-compress| option has been applied
to them as well.
We do not poses sufficient knowledge of the inner workings of \emph{GNU sort}, therefore we do not
attempt to give an explanation of this peculiar behavior.

\section{The memory demands of eppex}
\label{sec:eppex-memory-demands}

We assessed the memory demands of \eppex{} by a procedure described in \Sref{sec:memory-benchmarking}.
The comprehensive tables with virtual memory peaks and phrase table sizes for the benchmarked
configurations of \eppex{} are part of \Aref{chap:benchmarking-tables}; here, we are going
to comment on some of the more interesting figures.

\subsection{The Cs-En dataset}

The Czech-English translation model was constructed with two factors: a truecased token and
a morphological tag.
In the whole data, there were on average 14~Czech and 16.5~English words per sentence,
and each of the batches had average sentence length close to the overall average.
The phrase-pairs-per-sentence ratio was close to 22.\footnote{This ratio simply expresses
how many unique phrase pairs were on average contributed by each parallel sentence in
the corpus.}

\Fref{fig:cs-en-vm-peaks} presents the virtual memory peaks measured on the three most varying
configurations of the \eppex{} scenario (the complete figures are part of \Tref{cs-en-memory-benchmarking}):
\emph{eppex zero} with no Lossy Counter instantiated,
\emph{eppex def.} with three Lossy Counters instantiated and \emph{eppex 0:n} with
a separate Lossy Counter instantiated for each phrase length.
Judging by the shapes of curves connecting the points with measurements, there seems to be
a close-to-linear correlation between the training data size and the virtual memory peak in all
three cases.

\begin{figure}[!htb]
  \centering
  \input{benchmark-cs-en-vm-peaks}
  \caption{
    A plot of virtual memory peaks of various \eppex{} configurations invoked on the subsets of Cs-En training data.
  }
  \label{fig:cs-en-vm-peaks}
\end{figure}

This observation inspired us to take a look onto the possibility of predicting the memory
demands of the epochal extraction.
Specifically, we were curious, if a virtual memory peak quickly determined by running
\eppex{} on some subset of the training data made it possible to predict the amount of
memory required to process the whole data.

For each batch of size $K$ with a virtual memory peak $vm_{K}$ we evaluated the predicted virtual
memory peak as: $pred_{K} = vm_{K} / K \times N$, where $N$ is the size of the whole
data.\footnote{Assuming a linear correlation, the ratio $vm_{K} / K$ is an estimation of the linear coefficient.}
We then calculated the error of prediction $pred_{K}$ as the normalized difference between
the prediction and true virtual memory peak $vm_{real}$: $err_{K} = (pred_{K} - vm_{real}) / vm_{real}$.
\Fref{fig:cs-en-vm-peak-prediction} presents the error values for subsets of increasing size
expressed as percentages.

\begin{figure}[!htb]
  \centering
  \input{predict-cs-en-vm-peaks}
  \caption{
    Error of virtual memory peak predictions based on the partial runs of various \eppex{} configurations
    invoked on the subsets of Cs-En training data.
  }
  \label{fig:cs-en-vm-peak-prediction}
\end{figure}

With all three configurations, the prediction errors had a very similar course: predictions on small
subsets were strongly overestimated, but the error continually declined as the ratio
of the training data used increased.
A decent prediction, overestimating the total VM peak by no more than 20\%, was possible based on $1/4$
of the training data.

\subsection{The Fr-En dataset}

The French-English training data contained only a single factor: an input token in lowercase.
There were on average 30~French and 25.5~English tokens per sentence in the whole data,
but in most of the batches the averages were below these values (down to 25.5~tokens
per sentence in the case of French and 24~tokens per sentence in the case of English).
Compared to the Cs-En dataset, the sentences were on average almost twice as long and
likewise the phrase-pairs-per-sentence ratio was twice as large, almost~46 in the case of
unpruned phrase table.

In the case of the Fr-En data, we decided to point out the differences between the memory demands
and the output sizes of three \eppex{} configurations instantiated with separate Lossy Counting
instances for each phrase length, as two of them have the same error margin and two of them
prune with the same negative limit.

\Fref{fig:fr-en-vm-peaks} illustrates the influence of error margin on \eppex{} memory demands
(the complete figures are part of \Tref{fr-en-memory-benchmarking}).
Both \emph{eppex 0:n} and \emph{eppex 1:n+1} configurations have the same error margins and
their memory demands closely follow each other, with the latter having bit lower memory
peaks, because of the harsher pruning.

\begin{figure}[!htb]
  \centering
  \input{benchmark-fr-en-vm-peaks}
  \caption{
    A plot of virtual memory peaks of three \eppex{} configurations invoked on subsets of Fr-En training data.
  }
  \label{fig:fr-en-vm-peaks}
\end{figure}

Despite the fact that \emph{eppex 0:n} and \emph{eppex 1:n+1} have similar memory demands, their outputs
differ significantly, as \Fref{fig:fr-en-phrase-table-sizes} shows (the complete figures are part
of \Tref{fr-en-output-size-benchmarking}).
The \emph{eppex 1:n+1} configuration pruned off all the singletons, so it consistently
produced phrase tables substantially smaller than the \emph{eppex 0:n} configuration.

In fact, the degree of pruning of \emph{eppex 1:n+1} is very close to the \emph{eppex 1:n+2} configuration,
because in both cases it is determined by the common negative limit.

\begin{figure}[!htb]
  \centering
  \input{benchmark-fr-en-phrase-table-sizes}
  \caption{
    A plot of phrase table sizes obtained with three \eppex{} configurations invoked on subsets of Fr-En training data.
  }
  \label{fig:fr-en-phrase-table-sizes}
\end{figure}

One more observable phenomenon is the clearly visible plateau in the plot of phrase table sizes
obtained by the \emph{eppex 0:n} configuration on the range of input sizes from 25~M to 35~M parallel sentences.
A quick examination of the figures in \Tref{fr-en-output-size-benchmarking} confirms that, indeed, with
the \emph{eppex 0:n} configuration, 268~million phrase pairs is extracted from 25~million parallel sentences,
but only 1~million more is extracted from the additional 5~million sentences and 2~million more from the additional
10~million sentences.

The origin of this peculiarity lies in the great amount of phrase pairs with counts in the error
margin zone, i.e. phrase pairs that are neither guaranteed to be removed nor retained.
If such a phrase pair happened to be extracted from a batch of a certain size, but has no more
additional occurrences in the batch of increased size, then the likelihood of its extraction
goes down.

Since our corpus was created from several smaller corpora and we did not shuffle the sentences,
it is likely that the distribution of extracted phrase pairs rapidly changed as a result of data
coming from a different parallel corpus and, consequently, the amount of phrase pairs in the error zone,
that happened to be pruned, massively increased.

Nevertheless, the definition of Lossy Counting limits guarantees that
whenever the limits are fixed adding more items to the input stream results in at least
as many phrase pairs being extracted as from the non-extended stream.
